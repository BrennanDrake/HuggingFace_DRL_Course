[2026-01-17 16:01:29,178][12243] Saving configuration to /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/config.json...
[2026-01-17 16:01:30,143][12243] Rollout worker 0 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 1 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 2 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 3 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 4 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 5 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 6 uses device cpu
[2026-01-17 16:01:30,143][12243] Rollout worker 7 uses device cpu
[2026-01-17 16:01:30,183][12243] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:01:30,183][12243] InferenceWorker_p0-w0: min num requests: 2
[2026-01-17 16:01:30,212][12243] Starting all processes...
[2026-01-17 16:01:30,212][12243] Starting process learner_proc0
[2026-01-17 16:01:30,262][12243] Starting all processes...
[2026-01-17 16:01:30,267][12243] Starting process inference_proc0-0
[2026-01-17 16:01:30,267][12243] Starting process rollout_proc0
[2026-01-17 16:01:30,267][12243] Starting process rollout_proc1
[2026-01-17 16:01:30,267][12243] Starting process rollout_proc2
[2026-01-17 16:01:30,270][12243] Starting process rollout_proc3
[2026-01-17 16:01:30,270][12243] Starting process rollout_proc4
[2026-01-17 16:01:30,271][12243] Starting process rollout_proc5
[2026-01-17 16:01:30,272][12243] Starting process rollout_proc6
[2026-01-17 16:01:30,273][12243] Starting process rollout_proc7
[2026-01-17 16:01:31,599][12372] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:01:31,599][12372] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2026-01-17 16:01:31,622][12372] Num visible devices: 1
[2026-01-17 16:01:31,623][12372] Starting seed is not provided
[2026-01-17 16:01:31,623][12372] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:01:31,623][12372] Initializing actor-critic model on device cuda:0
[2026-01-17 16:01:31,623][12372] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:01:31,624][12372] RunningMeanStd input shape: (1,)
[2026-01-17 16:01:31,631][12372] ConvEncoder: input_channels=3
[2026-01-17 16:01:31,663][12388] Worker 2 uses CPU cores [6, 7, 8]
[2026-01-17 16:01:31,673][12387] Worker 1 uses CPU cores [3, 4, 5]
[2026-01-17 16:01:31,697][12372] Conv encoder output size: 512
[2026-01-17 16:01:31,698][12372] Policy head output size: 512
[2026-01-17 16:01:31,698][12389] Worker 3 uses CPU cores [9, 10, 11]
[2026-01-17 16:01:31,702][12386] Worker 0 uses CPU cores [0, 1, 2]
[2026-01-17 16:01:31,707][12372] Created Actor Critic model with architecture:
[2026-01-17 16:01:31,707][12372] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2026-01-17 16:01:31,714][12385] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:01:31,714][12385] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2026-01-17 16:01:31,716][12392] Worker 6 uses CPU cores [18, 19, 20]
[2026-01-17 16:01:31,731][12393] Worker 7 uses CPU cores [21, 22, 23]
[2026-01-17 16:01:31,736][12385] Num visible devices: 1
[2026-01-17 16:01:31,757][12390] Worker 4 uses CPU cores [12, 13, 14]
[2026-01-17 16:01:31,800][12391] Worker 5 uses CPU cores [15, 16, 17]
[2026-01-17 16:01:32,081][12372] Using optimizer <class 'torch.optim.adam.Adam'>
[2026-01-17 16:01:32,782][12372] No checkpoints found
[2026-01-17 16:01:32,782][12372] Did not load from checkpoint, starting from scratch!
[2026-01-17 16:01:32,782][12372] Initialized policy 0 weights for model version 0
[2026-01-17 16:01:32,783][12372] LearnerWorker_p0 finished initialization!
[2026-01-17 16:01:32,784][12372] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:01:32,926][12385] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:01:32,927][12385] RunningMeanStd input shape: (1,)
[2026-01-17 16:01:32,934][12385] ConvEncoder: input_channels=3
[2026-01-17 16:01:32,997][12385] Conv encoder output size: 512
[2026-01-17 16:01:32,997][12385] Policy head output size: 512
[2026-01-17 16:01:33,030][12243] Inference worker 0-0 is ready!
[2026-01-17 16:01:33,030][12243] All inference workers are ready! Signal rollout workers to start!
[2026-01-17 16:01:33,057][12388] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,058][12392] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,058][12387] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,058][12391] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,058][12393] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,058][12389] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,059][12386] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,059][12390] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:01:33,306][12389] Decorrelating experience for 0 frames...
[2026-01-17 16:01:33,306][12386] Decorrelating experience for 0 frames...
[2026-01-17 16:01:33,306][12387] Decorrelating experience for 0 frames...
[2026-01-17 16:01:33,306][12388] Decorrelating experience for 0 frames...
[2026-01-17 16:01:33,520][12389] Decorrelating experience for 32 frames...
[2026-01-17 16:01:33,521][12386] Decorrelating experience for 32 frames...
[2026-01-17 16:01:33,522][12387] Decorrelating experience for 32 frames...
[2026-01-17 16:01:33,550][12391] Decorrelating experience for 0 frames...
[2026-01-17 16:01:33,768][12391] Decorrelating experience for 32 frames...
[2026-01-17 16:01:33,771][12388] Decorrelating experience for 32 frames...
[2026-01-17 16:01:33,784][12389] Decorrelating experience for 64 frames...
[2026-01-17 16:01:33,788][12387] Decorrelating experience for 64 frames...
[2026-01-17 16:01:33,804][12390] Decorrelating experience for 0 frames...
[2026-01-17 16:01:34,017][12390] Decorrelating experience for 32 frames...
[2026-01-17 16:01:34,034][12388] Decorrelating experience for 64 frames...
[2026-01-17 16:01:34,036][12386] Decorrelating experience for 64 frames...
[2026-01-17 16:01:34,230][12391] Decorrelating experience for 64 frames...
[2026-01-17 16:01:34,269][12388] Decorrelating experience for 96 frames...
[2026-01-17 16:01:34,273][12386] Decorrelating experience for 96 frames...
[2026-01-17 16:01:34,280][12390] Decorrelating experience for 64 frames...
[2026-01-17 16:01:34,464][12391] Decorrelating experience for 96 frames...
[2026-01-17 16:01:34,477][12387] Decorrelating experience for 96 frames...
[2026-01-17 16:01:35,210][12372] Signal inference workers to stop experience collection...
[2026-01-17 16:01:35,214][12385] InferenceWorker_p0-w0: stopping experience collection
[2026-01-17 16:01:35,877][12372] Signal inference workers to resume experience collection...
[2026-01-17 16:01:35,877][12385] InferenceWorker_p0-w0: resuming experience collection
[2026-01-17 16:01:36,132][12243] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 8192. Throughput: 0: nan. Samples: 2614. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[2026-01-17 16:01:36,133][12243] Avg episode reward: [(0, '3.279')]
[2026-01-17 16:01:37,470][12385] Updated weights for policy 0, policy_version 10 (0.0056)
[2026-01-17 16:01:39,450][12385] Updated weights for policy 0, policy_version 20 (0.0006)
[2026-01-17 16:01:41,132][12243] Fps is (10 sec: 21299.4, 60 sec: 21299.4, 300 sec: 21299.4). Total num frames: 114688. Throughput: 0: 3225.6. Samples: 18742. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)
[2026-01-17 16:01:41,133][12243] Avg episode reward: [(0, '4.385')]
[2026-01-17 16:01:41,133][12372] Saving new best policy, reward=4.385!
[2026-01-17 16:01:41,475][12385] Updated weights for policy 0, policy_version 30 (0.0006)
[2026-01-17 16:01:43,431][12385] Updated weights for policy 0, policy_version 40 (0.0006)
[2026-01-17 16:01:45,465][12385] Updated weights for policy 0, policy_version 50 (0.0006)
[2026-01-17 16:01:46,132][12243] Fps is (10 sec: 20889.6, 60 sec: 20889.6, 300 sec: 20889.6). Total num frames: 217088. Throughput: 0: 4664.6. Samples: 49260. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)
[2026-01-17 16:01:46,133][12243] Avg episode reward: [(0, '4.590')]
[2026-01-17 16:01:46,135][12372] Saving new best policy, reward=4.590!
[2026-01-17 16:01:47,578][12385] Updated weights for policy 0, policy_version 60 (0.0006)
[2026-01-17 16:01:48,612][12243] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 12243], exiting...
[2026-01-17 16:01:48,613][12243] Runner profile tree view:
main_loop: 18.4009
[2026-01-17 16:01:48,613][12243] Collected {0: 262144}, FPS: 14246.3
[2026-01-17 16:01:48,613][12372] Stopping Batcher_0...
[2026-01-17 16:01:48,614][12372] Loop batcher_evt_loop terminating...
[2026-01-17 16:01:48,614][12372] Saving /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000064_262144.pth...
[2026-01-17 16:01:48,616][12388] EvtLoop [rollout_proc2_evt_loop, process=rollout_proc2] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance2'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 117, in step
    obs, info["reset_info"] = self.env.reset()
                              ^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 30, in reset
    return self.env.reset(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 515, in reset
    obs, info = self.env.reset(seed=seed, options=options)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/envs/env_wrappers.py", line 82, in reset
    obs, info = self.env.reset(**kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 467, in reset
    return self.env.reset(seed=seed, options=options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 51, in reset
    return self.env.reset(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 346, in reset
    self.game.new_episode()
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 16:01:48,618][12388] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc2_evt_loop
[2026-01-17 16:01:48,619][12391] EvtLoop [rollout_proc5_evt_loop, process=rollout_proc5] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance5'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 16:01:48,621][12391] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc5_evt_loop
[2026-01-17 16:01:48,620][12387] EvtLoop [rollout_proc1_evt_loop, process=rollout_proc1] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance1'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 16:01:48,622][12387] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc1_evt_loop
[2026-01-17 16:01:48,621][12386] EvtLoop [rollout_proc0_evt_loop, process=rollout_proc0] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance0'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 16:01:48,622][12386] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc0_evt_loop
[2026-01-17 16:01:48,660][12372] Stopping LearnerWorker_p0...
[2026-01-17 16:01:48,660][12385] Weights refcount: 2 0
[2026-01-17 16:01:48,660][12372] Loop learner_proc0_evt_loop terminating...
[2026-01-17 16:01:48,661][12385] Stopping InferenceWorker_p0-w0...
[2026-01-17 16:01:48,661][12385] Loop inference_proc0-0_evt_loop terminating...
[2026-01-17 16:01:53,097][12393] Another process currently holds the lock /tmp/sf2_brennan/doom_007.lockfile, attempt: 1
[2026-01-17 16:02:06,394][12393] Decorrelating experience for 0 frames...
[2026-01-17 16:02:06,602][12393] Decorrelating experience for 32 frames...
[2026-01-17 16:02:06,857][12393] Decorrelating experience for 64 frames...
[2026-01-17 16:02:07,086][12393] Decorrelating experience for 96 frames...
[2026-01-17 16:02:07,155][12393] Stopping RolloutWorker_w7...
[2026-01-17 16:02:07,155][12393] Loop rollout_proc7_evt_loop terminating...
[2026-01-17 16:08:11,024][05312] Saving configuration to /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/config.json...
[2026-01-17 16:08:11,996][05312] Rollout worker 0 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 1 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 2 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 3 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 4 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 5 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 6 uses device cpu
[2026-01-17 16:08:11,996][05312] Rollout worker 7 uses device cpu
[2026-01-17 16:08:12,044][05312] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:08:12,044][05312] InferenceWorker_p0-w0: min num requests: 2
[2026-01-17 16:08:12,072][05312] Starting all processes...
[2026-01-17 16:08:12,073][05312] Starting process learner_proc0
[2026-01-17 16:08:12,122][05312] Starting all processes...
[2026-01-17 16:08:12,127][05312] Starting process inference_proc0-0
[2026-01-17 16:08:12,127][05312] Starting process rollout_proc0
[2026-01-17 16:08:12,128][05312] Starting process rollout_proc1
[2026-01-17 16:08:12,128][05312] Starting process rollout_proc2
[2026-01-17 16:08:12,128][05312] Starting process rollout_proc3
[2026-01-17 16:08:12,128][05312] Starting process rollout_proc4
[2026-01-17 16:08:12,130][05312] Starting process rollout_proc5
[2026-01-17 16:08:12,132][05312] Starting process rollout_proc6
[2026-01-17 16:08:12,132][05312] Starting process rollout_proc7
[2026-01-17 16:08:13,437][05451] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:08:13,437][05451] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2026-01-17 16:08:13,466][05465] Worker 0 uses CPU cores [0, 1, 2]
[2026-01-17 16:08:13,470][05468] Worker 1 uses CPU cores [3, 4, 5]
[2026-01-17 16:08:13,475][05451] Num visible devices: 1
[2026-01-17 16:08:13,475][05451] Starting seed is not provided
[2026-01-17 16:08:13,476][05451] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:08:13,476][05451] Initializing actor-critic model on device cuda:0
[2026-01-17 16:08:13,476][05451] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:08:13,478][05451] RunningMeanStd input shape: (1,)
[2026-01-17 16:08:13,484][05472] Worker 7 uses CPU cores [21, 22, 23]
[2026-01-17 16:08:13,487][05451] ConvEncoder: input_channels=3
[2026-01-17 16:08:13,498][05466] Worker 2 uses CPU cores [6, 7, 8]
[2026-01-17 16:08:13,506][05467] Worker 3 uses CPU cores [9, 10, 11]
[2026-01-17 16:08:13,535][05470] Worker 5 uses CPU cores [15, 16, 17]
[2026-01-17 16:08:13,559][05469] Worker 4 uses CPU cores [12, 13, 14]
[2026-01-17 16:08:13,576][05471] Worker 6 uses CPU cores [18, 19, 20]
[2026-01-17 16:08:13,589][05451] Conv encoder output size: 512
[2026-01-17 16:08:13,589][05451] Policy head output size: 512
[2026-01-17 16:08:13,607][05451] Created Actor Critic model with architecture:
[2026-01-17 16:08:13,607][05451] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2026-01-17 16:08:13,623][05464] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:08:13,623][05464] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2026-01-17 16:08:13,652][05464] Num visible devices: 1
[2026-01-17 16:08:14,056][05451] Using optimizer <class 'torch.optim.adam.Adam'>
[2026-01-17 16:08:14,959][05451] Loading state from checkpoint /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000064_262144.pth...
[2026-01-17 16:08:14,961][05451] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:08:14,962][05451] Loading state from checkpoint /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000064_262144.pth...
[2026-01-17 16:08:14,963][05451] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:08:14,963][05451] Loading state from checkpoint /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000064_262144.pth...
[2026-01-17 16:08:14,963][05451] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:08:14,964][05451] Did not load from checkpoint, starting from scratch!
[2026-01-17 16:08:14,964][05451] Initialized policy 0 weights for model version 0
[2026-01-17 16:08:14,966][05451] LearnerWorker_p0 finished initialization!
[2026-01-17 16:08:14,966][05451] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:08:15,097][05464] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:08:15,098][05464] RunningMeanStd input shape: (1,)
[2026-01-17 16:08:15,105][05464] ConvEncoder: input_channels=3
[2026-01-17 16:08:15,168][05464] Conv encoder output size: 512
[2026-01-17 16:08:15,169][05464] Policy head output size: 512
[2026-01-17 16:08:15,201][05312] Inference worker 0-0 is ready!
[2026-01-17 16:08:15,202][05312] All inference workers are ready! Signal rollout workers to start!
[2026-01-17 16:08:15,228][05467] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,228][05465] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,228][05471] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,228][05466] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,229][05469] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,229][05470] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,229][05472] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,229][05468] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:08:15,531][05467] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,531][05465] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,531][05466] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,531][05468] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,746][05467] Decorrelating experience for 32 frames...
[2026-01-17 16:08:15,746][05465] Decorrelating experience for 32 frames...
[2026-01-17 16:08:15,747][05466] Decorrelating experience for 32 frames...
[2026-01-17 16:08:15,775][05469] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,974][05472] Decorrelating experience for 0 frames...
[2026-01-17 16:08:15,989][05469] Decorrelating experience for 32 frames...
[2026-01-17 16:08:16,008][05467] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,008][05466] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,184][05472] Decorrelating experience for 32 frames...
[2026-01-17 16:08:16,194][05468] Decorrelating experience for 32 frames...
[2026-01-17 16:08:16,224][05470] Decorrelating experience for 0 frames...
[2026-01-17 16:08:16,244][05467] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,259][05465] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,423][05471] Decorrelating experience for 0 frames...
[2026-01-17 16:08:16,440][05470] Decorrelating experience for 32 frames...
[2026-01-17 16:08:16,444][05466] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,447][05472] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,494][05465] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,641][05471] Decorrelating experience for 32 frames...
[2026-01-17 16:08:16,655][05469] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,661][05468] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,681][05472] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,704][05470] Decorrelating experience for 64 frames...
[2026-01-17 16:08:16,898][05469] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,901][05468] Decorrelating experience for 96 frames...
[2026-01-17 16:08:16,902][05471] Decorrelating experience for 64 frames...
[2026-01-17 16:08:17,147][05470] Decorrelating experience for 96 frames...
[2026-01-17 16:08:17,148][05471] Decorrelating experience for 96 frames...
[2026-01-17 16:08:17,272][05451] Signal inference workers to stop experience collection...
[2026-01-17 16:08:17,276][05464] InferenceWorker_p0-w0: stopping experience collection
[2026-01-17 16:08:17,577][05312] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 16:08:17,577][05312] Avg episode reward: [(0, '2.440')]
[2026-01-17 16:08:18,199][05451] Signal inference workers to resume experience collection...
[2026-01-17 16:08:18,200][05464] InferenceWorker_p0-w0: resuming experience collection
[2026-01-17 16:08:19,202][05464] Updated weights for policy 0, policy_version 10 (0.0060)
[2026-01-17 16:08:20,344][05464] Updated weights for policy 0, policy_version 20 (0.0006)
[2026-01-17 16:08:21,380][05464] Updated weights for policy 0, policy_version 30 (0.0006)
[2026-01-17 16:08:22,436][05464] Updated weights for policy 0, policy_version 40 (0.0006)
[2026-01-17 16:08:22,577][05312] Fps is (10 sec: 33587.4, 60 sec: 33587.4, 300 sec: 33587.4). Total num frames: 167936. Throughput: 0: 5974.0. Samples: 29870. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2026-01-17 16:08:22,577][05312] Avg episode reward: [(0, '4.601')]
[2026-01-17 16:08:22,577][05451] Saving new best policy, reward=4.601!
[2026-01-17 16:08:23,478][05464] Updated weights for policy 0, policy_version 50 (0.0006)
[2026-01-17 16:08:24,548][05464] Updated weights for policy 0, policy_version 60 (0.0006)
[2026-01-17 16:08:25,626][05464] Updated weights for policy 0, policy_version 70 (0.0006)
[2026-01-17 16:08:26,699][05464] Updated weights for policy 0, policy_version 80 (0.0006)
[2026-01-17 16:08:27,577][05312] Fps is (10 sec: 36044.8, 60 sec: 36044.8, 300 sec: 36044.8). Total num frames: 360448. Throughput: 0: 8759.2. Samples: 87592. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:08:27,577][05312] Avg episode reward: [(0, '4.473')]
[2026-01-17 16:08:27,743][05464] Updated weights for policy 0, policy_version 90 (0.0006)
[2026-01-17 16:08:28,796][05464] Updated weights for policy 0, policy_version 100 (0.0006)
[2026-01-17 16:08:29,870][05464] Updated weights for policy 0, policy_version 110 (0.0006)
[2026-01-17 16:08:30,947][05464] Updated weights for policy 0, policy_version 120 (0.0006)
[2026-01-17 16:08:32,024][05464] Updated weights for policy 0, policy_version 130 (0.0006)
[2026-01-17 16:08:32,036][05312] Heartbeat connected on Batcher_0
[2026-01-17 16:08:32,040][05312] Heartbeat connected on LearnerWorker_p0
[2026-01-17 16:08:32,048][05312] Heartbeat connected on RolloutWorker_w0
[2026-01-17 16:08:32,048][05312] Heartbeat connected on InferenceWorker_p0-w0
[2026-01-17 16:08:32,052][05312] Heartbeat connected on RolloutWorker_w1
[2026-01-17 16:08:32,055][05312] Heartbeat connected on RolloutWorker_w2
[2026-01-17 16:08:32,059][05312] Heartbeat connected on RolloutWorker_w3
[2026-01-17 16:08:32,063][05312] Heartbeat connected on RolloutWorker_w4
[2026-01-17 16:08:32,065][05312] Heartbeat connected on RolloutWorker_w5
[2026-01-17 16:08:32,069][05312] Heartbeat connected on RolloutWorker_w6
[2026-01-17 16:08:32,072][05312] Heartbeat connected on RolloutWorker_w7
[2026-01-17 16:08:32,577][05312] Fps is (10 sec: 38502.2, 60 sec: 36863.9, 300 sec: 36863.9). Total num frames: 552960. Throughput: 0: 7759.1. Samples: 116386. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)
[2026-01-17 16:08:32,577][05312] Avg episode reward: [(0, '4.724')]
[2026-01-17 16:08:32,592][05451] Saving new best policy, reward=4.724!
[2026-01-17 16:08:33,113][05464] Updated weights for policy 0, policy_version 140 (0.0007)
[2026-01-17 16:08:34,169][05464] Updated weights for policy 0, policy_version 150 (0.0006)
[2026-01-17 16:08:35,250][05464] Updated weights for policy 0, policy_version 160 (0.0006)
[2026-01-17 16:08:36,339][05464] Updated weights for policy 0, policy_version 170 (0.0006)
[2026-01-17 16:08:37,420][05464] Updated weights for policy 0, policy_version 180 (0.0006)
[2026-01-17 16:08:37,577][05312] Fps is (10 sec: 38092.7, 60 sec: 37068.7, 300 sec: 37068.7). Total num frames: 741376. Throughput: 0: 8683.5. Samples: 173670. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:08:37,577][05312] Avg episode reward: [(0, '4.783')]
[2026-01-17 16:08:37,579][05451] Saving new best policy, reward=4.783!
[2026-01-17 16:08:38,499][05464] Updated weights for policy 0, policy_version 190 (0.0006)
[2026-01-17 16:08:39,659][05464] Updated weights for policy 0, policy_version 200 (0.0007)
[2026-01-17 16:08:40,813][05464] Updated weights for policy 0, policy_version 210 (0.0006)
[2026-01-17 16:08:42,038][05464] Updated weights for policy 0, policy_version 220 (0.0007)
[2026-01-17 16:08:42,577][05312] Fps is (10 sec: 36454.5, 60 sec: 36700.2, 300 sec: 36700.2). Total num frames: 917504. Throughput: 0: 9106.9. Samples: 227672. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:08:42,577][05312] Avg episode reward: [(0, '5.318')]
[2026-01-17 16:08:42,577][05451] Saving new best policy, reward=5.318!
[2026-01-17 16:08:43,156][05464] Updated weights for policy 0, policy_version 230 (0.0007)
[2026-01-17 16:08:44,302][05464] Updated weights for policy 0, policy_version 240 (0.0007)
[2026-01-17 16:08:45,427][05464] Updated weights for policy 0, policy_version 250 (0.0006)
[2026-01-17 16:08:46,545][05464] Updated weights for policy 0, policy_version 260 (0.0006)
[2026-01-17 16:08:47,577][05312] Fps is (10 sec: 36044.7, 60 sec: 36727.4, 300 sec: 36727.4). Total num frames: 1101824. Throughput: 0: 8492.6. Samples: 254778. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:08:47,577][05312] Avg episode reward: [(0, '5.967')]
[2026-01-17 16:08:47,580][05451] Saving new best policy, reward=5.967!
[2026-01-17 16:08:47,663][05464] Updated weights for policy 0, policy_version 270 (0.0006)
[2026-01-17 16:08:48,816][05464] Updated weights for policy 0, policy_version 280 (0.0006)
[2026-01-17 16:08:49,931][05464] Updated weights for policy 0, policy_version 290 (0.0006)
[2026-01-17 16:08:51,024][05464] Updated weights for policy 0, policy_version 300 (0.0006)
[2026-01-17 16:08:52,068][05464] Updated weights for policy 0, policy_version 310 (0.0006)
[2026-01-17 16:08:52,577][05312] Fps is (10 sec: 36864.1, 60 sec: 36747.0, 300 sec: 36747.0). Total num frames: 1286144. Throughput: 0: 8856.5. Samples: 309976. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2026-01-17 16:08:52,577][05312] Avg episode reward: [(0, '7.073')]
[2026-01-17 16:08:52,578][05451] Saving new best policy, reward=7.073!
[2026-01-17 16:08:53,198][05464] Updated weights for policy 0, policy_version 320 (0.0006)
[2026-01-17 16:08:54,304][05464] Updated weights for policy 0, policy_version 330 (0.0006)
[2026-01-17 16:08:55,370][05464] Updated weights for policy 0, policy_version 340 (0.0006)
[2026-01-17 16:08:56,447][05464] Updated weights for policy 0, policy_version 350 (0.0006)
[2026-01-17 16:08:57,494][05464] Updated weights for policy 0, policy_version 360 (0.0006)
[2026-01-17 16:08:57,577][05312] Fps is (10 sec: 37273.8, 60 sec: 36864.0, 300 sec: 36864.0). Total num frames: 1474560. Throughput: 0: 9166.0. Samples: 366642. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:08:57,577][05312] Avg episode reward: [(0, '10.022')]
[2026-01-17 16:08:57,579][05451] Saving new best policy, reward=10.022!
[2026-01-17 16:08:58,570][05464] Updated weights for policy 0, policy_version 370 (0.0006)
[2026-01-17 16:08:59,618][05464] Updated weights for policy 0, policy_version 380 (0.0006)
[2026-01-17 16:09:00,673][05464] Updated weights for policy 0, policy_version 390 (0.0006)
[2026-01-17 16:09:01,731][05464] Updated weights for policy 0, policy_version 400 (0.0006)
[2026-01-17 16:09:02,577][05312] Fps is (10 sec: 38502.4, 60 sec: 37137.1, 300 sec: 37137.1). Total num frames: 1671168. Throughput: 0: 8789.0. Samples: 395506. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:02,577][05312] Avg episode reward: [(0, '13.318')]
[2026-01-17 16:09:02,577][05451] Saving new best policy, reward=13.318!
[2026-01-17 16:09:02,791][05464] Updated weights for policy 0, policy_version 410 (0.0006)
[2026-01-17 16:09:03,855][05464] Updated weights for policy 0, policy_version 420 (0.0006)
[2026-01-17 16:09:04,927][05464] Updated weights for policy 0, policy_version 430 (0.0006)
[2026-01-17 16:09:05,983][05464] Updated weights for policy 0, policy_version 440 (0.0006)
[2026-01-17 16:09:07,051][05464] Updated weights for policy 0, policy_version 450 (0.0006)
[2026-01-17 16:09:07,577][05312] Fps is (10 sec: 38502.5, 60 sec: 37191.7, 300 sec: 37191.7). Total num frames: 1859584. Throughput: 0: 9412.7. Samples: 453442. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:07,577][05312] Avg episode reward: [(0, '12.080')]
[2026-01-17 16:09:08,105][05464] Updated weights for policy 0, policy_version 460 (0.0006)
[2026-01-17 16:09:09,176][05464] Updated weights for policy 0, policy_version 470 (0.0006)
[2026-01-17 16:09:10,236][05464] Updated weights for policy 0, policy_version 480 (0.0006)
[2026-01-17 16:09:11,313][05464] Updated weights for policy 0, policy_version 490 (0.0007)
[2026-01-17 16:09:12,386][05464] Updated weights for policy 0, policy_version 500 (0.0006)
[2026-01-17 16:09:12,577][05312] Fps is (10 sec: 38092.5, 60 sec: 37310.8, 300 sec: 37310.8). Total num frames: 2052096. Throughput: 0: 9410.1. Samples: 511048. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:12,577][05312] Avg episode reward: [(0, '17.536')]
[2026-01-17 16:09:12,578][05451] Saving new best policy, reward=17.536!
[2026-01-17 16:09:13,452][05464] Updated weights for policy 0, policy_version 510 (0.0006)
[2026-01-17 16:09:14,488][05464] Updated weights for policy 0, policy_version 520 (0.0006)
[2026-01-17 16:09:15,543][05464] Updated weights for policy 0, policy_version 530 (0.0006)
[2026-01-17 16:09:16,622][05464] Updated weights for policy 0, policy_version 540 (0.0006)
[2026-01-17 16:09:17,577][05312] Fps is (10 sec: 38911.8, 60 sec: 37478.4, 300 sec: 37478.4). Total num frames: 2248704. Throughput: 0: 9417.2. Samples: 540162. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:17,577][05312] Avg episode reward: [(0, '19.101')]
[2026-01-17 16:09:17,579][05451] Saving new best policy, reward=19.101!
[2026-01-17 16:09:17,693][05464] Updated weights for policy 0, policy_version 550 (0.0006)
[2026-01-17 16:09:18,811][05464] Updated weights for policy 0, policy_version 560 (0.0006)
[2026-01-17 16:09:19,883][05464] Updated weights for policy 0, policy_version 570 (0.0006)
[2026-01-17 16:09:20,987][05464] Updated weights for policy 0, policy_version 580 (0.0006)
[2026-01-17 16:09:22,056][05464] Updated weights for policy 0, policy_version 590 (0.0006)
[2026-01-17 16:09:22,577][05312] Fps is (10 sec: 38502.5, 60 sec: 37819.7, 300 sec: 37494.2). Total num frames: 2437120. Throughput: 0: 9403.0. Samples: 596806. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)
[2026-01-17 16:09:22,577][05312] Avg episode reward: [(0, '19.839')]
[2026-01-17 16:09:22,578][05451] Saving new best policy, reward=19.839!
[2026-01-17 16:09:23,115][05464] Updated weights for policy 0, policy_version 600 (0.0006)
[2026-01-17 16:09:24,178][05464] Updated weights for policy 0, policy_version 610 (0.0006)
[2026-01-17 16:09:25,236][05464] Updated weights for policy 0, policy_version 620 (0.0006)
[2026-01-17 16:09:26,263][05464] Updated weights for policy 0, policy_version 630 (0.0006)
[2026-01-17 16:09:27,332][05464] Updated weights for policy 0, policy_version 640 (0.0006)
[2026-01-17 16:09:27,577][05312] Fps is (10 sec: 38093.0, 60 sec: 37819.8, 300 sec: 37566.2). Total num frames: 2629632. Throughput: 0: 9497.7. Samples: 655068. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:27,577][05312] Avg episode reward: [(0, '16.363')]
[2026-01-17 16:09:28,401][05464] Updated weights for policy 0, policy_version 650 (0.0006)
[2026-01-17 16:09:29,481][05464] Updated weights for policy 0, policy_version 660 (0.0006)
[2026-01-17 16:09:30,544][05464] Updated weights for policy 0, policy_version 670 (0.0006)
[2026-01-17 16:09:31,615][05464] Updated weights for policy 0, policy_version 680 (0.0006)
[2026-01-17 16:09:32,577][05312] Fps is (10 sec: 38502.6, 60 sec: 37819.8, 300 sec: 37628.6). Total num frames: 2822144. Throughput: 0: 9532.2. Samples: 683724. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:09:32,577][05312] Avg episode reward: [(0, '23.825')]
[2026-01-17 16:09:32,592][05451] Saving new best policy, reward=23.825!
[2026-01-17 16:09:32,665][05464] Updated weights for policy 0, policy_version 690 (0.0006)
[2026-01-17 16:09:33,712][05464] Updated weights for policy 0, policy_version 700 (0.0006)
[2026-01-17 16:09:34,771][05464] Updated weights for policy 0, policy_version 710 (0.0006)
[2026-01-17 16:09:35,801][05464] Updated weights for policy 0, policy_version 720 (0.0006)
[2026-01-17 16:09:36,862][05464] Updated weights for policy 0, policy_version 730 (0.0006)
[2026-01-17 16:09:37,577][05312] Fps is (10 sec: 38502.3, 60 sec: 37888.0, 300 sec: 37683.2). Total num frames: 3014656. Throughput: 0: 9598.4. Samples: 741904. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:09:37,577][05312] Avg episode reward: [(0, '25.278')]
[2026-01-17 16:09:37,594][05451] Saving new best policy, reward=25.278!
[2026-01-17 16:09:37,932][05464] Updated weights for policy 0, policy_version 740 (0.0006)
[2026-01-17 16:09:39,003][05464] Updated weights for policy 0, policy_version 750 (0.0006)
[2026-01-17 16:09:40,054][05464] Updated weights for policy 0, policy_version 760 (0.0006)
[2026-01-17 16:09:41,115][05464] Updated weights for policy 0, policy_version 770 (0.0006)
[2026-01-17 16:09:42,191][05464] Updated weights for policy 0, policy_version 780 (0.0006)
[2026-01-17 16:09:42,577][05312] Fps is (10 sec: 38502.2, 60 sec: 38161.1, 300 sec: 37731.4). Total num frames: 3207168. Throughput: 0: 9622.8. Samples: 799666. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:09:42,577][05312] Avg episode reward: [(0, '20.128')]
[2026-01-17 16:09:43,238][05464] Updated weights for policy 0, policy_version 790 (0.0006)
[2026-01-17 16:09:44,318][05464] Updated weights for policy 0, policy_version 800 (0.0006)
[2026-01-17 16:09:45,393][05464] Updated weights for policy 0, policy_version 810 (0.0006)
[2026-01-17 16:09:46,463][05464] Updated weights for policy 0, policy_version 820 (0.0006)
[2026-01-17 16:09:47,539][05464] Updated weights for policy 0, policy_version 830 (0.0006)
[2026-01-17 16:09:47,577][05312] Fps is (10 sec: 38502.4, 60 sec: 38297.7, 300 sec: 37774.2). Total num frames: 3399680. Throughput: 0: 9624.8. Samples: 828622. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2026-01-17 16:09:47,577][05312] Avg episode reward: [(0, '24.673')]
[2026-01-17 16:09:48,594][05464] Updated weights for policy 0, policy_version 840 (0.0006)
[2026-01-17 16:09:49,666][05464] Updated weights for policy 0, policy_version 850 (0.0006)
[2026-01-17 16:09:50,744][05464] Updated weights for policy 0, policy_version 860 (0.0006)
[2026-01-17 16:09:51,798][05464] Updated weights for policy 0, policy_version 870 (0.0006)
[2026-01-17 16:09:52,577][05312] Fps is (10 sec: 38502.4, 60 sec: 38434.1, 300 sec: 37812.5). Total num frames: 3592192. Throughput: 0: 9612.0. Samples: 885980. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2026-01-17 16:09:52,577][05312] Avg episode reward: [(0, '22.340')]
[2026-01-17 16:09:52,844][05464] Updated weights for policy 0, policy_version 880 (0.0006)
[2026-01-17 16:09:53,899][05464] Updated weights for policy 0, policy_version 890 (0.0006)
[2026-01-17 16:09:54,983][05464] Updated weights for policy 0, policy_version 900 (0.0006)
[2026-01-17 16:09:56,036][05464] Updated weights for policy 0, policy_version 910 (0.0006)
[2026-01-17 16:09:57,100][05464] Updated weights for policy 0, policy_version 920 (0.0006)
[2026-01-17 16:09:57,577][05312] Fps is (10 sec: 38502.3, 60 sec: 38502.4, 300 sec: 37847.0). Total num frames: 3784704. Throughput: 0: 9619.4. Samples: 943922. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2026-01-17 16:09:57,577][05312] Avg episode reward: [(0, '26.223')]
[2026-01-17 16:09:57,579][05451] Saving new best policy, reward=26.223!
[2026-01-17 16:09:58,169][05464] Updated weights for policy 0, policy_version 930 (0.0007)
[2026-01-17 16:09:59,232][05464] Updated weights for policy 0, policy_version 940 (0.0006)
[2026-01-17 16:10:00,288][05464] Updated weights for policy 0, policy_version 950 (0.0006)
[2026-01-17 16:10:01,328][05464] Updated weights for policy 0, policy_version 960 (0.0006)
[2026-01-17 16:10:02,391][05464] Updated weights for policy 0, policy_version 970 (0.0006)
[2026-01-17 16:10:02,577][05312] Fps is (10 sec: 38502.5, 60 sec: 38434.1, 300 sec: 37878.3). Total num frames: 3977216. Throughput: 0: 9612.7. Samples: 972734. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:10:02,577][05312] Avg episode reward: [(0, '24.964')]
[2026-01-17 16:10:03,266][05312] Component Batcher_0 stopped!
[2026-01-17 16:10:03,266][05451] Stopping Batcher_0...
[2026-01-17 16:10:03,266][05451] Saving /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:10:03,266][05451] Loop batcher_evt_loop terminating...
[2026-01-17 16:10:03,281][05464] Weights refcount: 2 0
[2026-01-17 16:10:03,282][05464] Stopping InferenceWorker_p0-w0...
[2026-01-17 16:10:03,282][05464] Loop inference_proc0-0_evt_loop terminating...
[2026-01-17 16:10:03,282][05312] Component InferenceWorker_p0-w0 stopped!
[2026-01-17 16:10:03,312][05467] Stopping RolloutWorker_w3...
[2026-01-17 16:10:03,312][05472] Stopping RolloutWorker_w7...
[2026-01-17 16:10:03,312][05312] Component RolloutWorker_w3 stopped!
[2026-01-17 16:10:03,312][05467] Loop rollout_proc3_evt_loop terminating...
[2026-01-17 16:10:03,313][05472] Loop rollout_proc7_evt_loop terminating...
[2026-01-17 16:10:03,313][05451] Saving /home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:10:03,313][05312] Component RolloutWorker_w7 stopped!
[2026-01-17 16:10:03,313][05312] Component RolloutWorker_w2 stopped!
[2026-01-17 16:10:03,313][05471] Stopping RolloutWorker_w6...
[2026-01-17 16:10:03,313][05466] Stopping RolloutWorker_w2...
[2026-01-17 16:10:03,313][05312] Component RolloutWorker_w6 stopped!
[2026-01-17 16:10:03,313][05466] Loop rollout_proc2_evt_loop terminating...
[2026-01-17 16:10:03,313][05471] Loop rollout_proc6_evt_loop terminating...
[2026-01-17 16:10:03,315][05312] Component RolloutWorker_w1 stopped!
[2026-01-17 16:10:03,315][05312] Component RolloutWorker_w5 stopped!
[2026-01-17 16:10:03,315][05468] Stopping RolloutWorker_w1...
[2026-01-17 16:10:03,315][05470] Stopping RolloutWorker_w5...
[2026-01-17 16:10:03,315][05312] Component RolloutWorker_w4 stopped!
[2026-01-17 16:10:03,315][05469] Stopping RolloutWorker_w4...
[2026-01-17 16:10:03,315][05470] Loop rollout_proc5_evt_loop terminating...
[2026-01-17 16:10:03,315][05468] Loop rollout_proc1_evt_loop terminating...
[2026-01-17 16:10:03,315][05312] Component RolloutWorker_w0 stopped!
[2026-01-17 16:10:03,315][05465] Stopping RolloutWorker_w0...
[2026-01-17 16:10:03,315][05469] Loop rollout_proc4_evt_loop terminating...
[2026-01-17 16:10:03,315][05465] Loop rollout_proc0_evt_loop terminating...
[2026-01-17 16:10:03,386][05451] Stopping LearnerWorker_p0...
[2026-01-17 16:10:03,386][05451] Loop learner_proc0_evt_loop terminating...
[2026-01-17 16:10:03,386][05312] Component LearnerWorker_p0 stopped!
[2026-01-17 16:10:03,386][05312] Waiting for process learner_proc0 to stop...
[2026-01-17 16:10:04,348][05312] Waiting for process inference_proc0-0 to join...
[2026-01-17 16:10:04,348][05312] Waiting for process rollout_proc0 to join...
[2026-01-17 16:10:04,348][05312] Waiting for process rollout_proc1 to join...
[2026-01-17 16:10:04,349][05312] Waiting for process rollout_proc2 to join...
[2026-01-17 16:10:04,349][05312] Waiting for process rollout_proc3 to join...
[2026-01-17 16:10:04,349][05312] Waiting for process rollout_proc4 to join...
[2026-01-17 16:10:04,349][05312] Waiting for process rollout_proc5 to join...
[2026-01-17 16:10:04,349][05312] Waiting for process rollout_proc6 to join...
[2026-01-17 16:10:04,350][05312] Waiting for process rollout_proc7 to join...
[2026-01-17 16:10:04,350][05312] Batcher 0 profile tree view:
batching: 9.8185, releasing_batches: 0.0130
[2026-01-17 16:10:04,350][05312] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 2.3345
update_model: 1.6395
  weight_update: 0.0006
one_step: 0.0028
  handle_policy_step: 98.7542
    deserialize: 4.6549, stack: 0.5050, obs_to_device_normalize: 23.6811, forward: 45.8997, send_messages: 6.6719
    prepare_outputs: 13.8363
      to_cpu: 9.0266
[2026-01-17 16:10:04,350][05312] Learner 0 profile tree view:
misc: 0.0040, prepare_batch: 4.1784
train: 12.8651
  epoch_init: 0.0031, minibatch_init: 0.0027, losses_postprocess: 0.2156, kl_divergence: 0.1665, after_optimizer: 2.9703
  calculate_losses: 4.8768
    losses_init: 0.0016, forward_head: 0.3713, bptt_initial: 2.7198, tail: 0.3398, advantages_returns: 0.1046, losses: 0.6613
    bptt: 0.6007
      bptt_forward_core: 0.5709
  update: 4.4280
    clip: 0.4598
[2026-01-17 16:10:04,350][05312] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0697, enqueue_policy_requests: 4.6283, env_step: 60.8264, overhead: 2.6041, complete_rollouts: 0.1116
save_policy_outputs: 4.9087
  split_output_tensors: 1.6515
[2026-01-17 16:10:04,350][05312] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0653, enqueue_policy_requests: 4.0949, env_step: 52.7072, overhead: 2.3056, complete_rollouts: 0.0921
save_policy_outputs: 4.2696
  split_output_tensors: 1.4337
[2026-01-17 16:10:04,350][05312] Loop Runner_EvtLoop terminating...
[2026-01-17 16:10:04,351][05312] Runner profile tree view:
main_loop: 112.2783
[2026-01-17 16:10:04,351][05312] Collected {0: 4005888}, FPS: 35678.2
[2026-01-17 16:27:30,515][13881] Saving configuration to train_dir/default_experiment/config.json...
[2026-01-17 16:27:31,303][13881] Rollout worker 0 uses device cpu
[2026-01-17 16:27:31,303][13881] Rollout worker 1 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 2 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 3 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 4 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 5 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 6 uses device cpu
[2026-01-17 16:27:31,304][13881] Rollout worker 7 uses device cpu
[2026-01-17 16:27:31,341][13881] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:27:31,342][13881] InferenceWorker_p0-w0: min num requests: 2
[2026-01-17 16:27:31,370][13881] Starting all processes...
[2026-01-17 16:27:31,371][13881] Starting process learner_proc0
[2026-01-17 16:27:32,552][13881] Starting all processes...
[2026-01-17 16:27:32,555][13881] Starting process inference_proc0-0
[2026-01-17 16:27:32,556][13881] Starting process rollout_proc0
[2026-01-17 16:27:32,556][13881] Starting process rollout_proc1
[2026-01-17 16:27:32,556][13881] Starting process rollout_proc2
[2026-01-17 16:27:32,556][13881] Starting process rollout_proc3
[2026-01-17 16:27:32,556][13881] Starting process rollout_proc4
[2026-01-17 16:27:32,558][13881] Starting process rollout_proc5
[2026-01-17 16:27:32,568][13997] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:27:32,568][13997] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2026-01-17 16:27:32,560][13881] Starting process rollout_proc6
[2026-01-17 16:27:32,560][13881] Starting process rollout_proc7
[2026-01-17 16:27:32,592][13997] Num visible devices: 1
[2026-01-17 16:27:32,593][13997] Starting seed is not provided
[2026-01-17 16:27:32,593][13997] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:27:32,593][13997] Initializing actor-critic model on device cuda:0
[2026-01-17 16:27:32,593][13997] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:27:32,594][13997] RunningMeanStd input shape: (1,)
[2026-01-17 16:27:32,606][13997] ConvEncoder: input_channels=3
[2026-01-17 16:27:32,740][13997] Conv encoder output size: 512
[2026-01-17 16:27:32,740][13997] Policy head output size: 512
[2026-01-17 16:27:32,750][13997] Created Actor Critic model with architecture:
[2026-01-17 16:27:32,751][13997] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2026-01-17 16:27:33,128][13997] Using optimizer <class 'torch.optim.adam.Adam'>
[2026-01-17 16:27:33,859][13997] Loading state from checkpoint train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:27:33,861][13997] Could not load from checkpoint, attempt 0
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:27:33,862][13997] Loading state from checkpoint train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:27:33,862][13997] Could not load from checkpoint, attempt 1
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:27:33,862][13997] Loading state from checkpoint train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:27:33,863][13997] Could not load from checkpoint, attempt 2
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/sample_factory/algo/learning/learner.py", line 281, in load_checkpoint
    checkpoint_dict = torch.load(latest_checkpoint, map_location=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
[2026-01-17 16:27:33,863][13997] Did not load from checkpoint, starting from scratch!
[2026-01-17 16:27:33,863][13997] Initialized policy 0 weights for model version 0
[2026-01-17 16:27:33,865][13997] LearnerWorker_p0 finished initialization!
[2026-01-17 16:27:33,865][13997] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:27:33,919][14047] Worker 2 uses CPU cores [6, 7, 8]
[2026-01-17 16:27:33,924][14048] Worker 1 uses CPU cores [3, 4, 5]
[2026-01-17 16:27:33,938][14045] Worker 0 uses CPU cores [0, 1, 2]
[2026-01-17 16:27:33,943][14046] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 16:27:33,943][14046] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2026-01-17 16:27:33,948][14050] Worker 4 uses CPU cores [12, 13, 14]
[2026-01-17 16:27:33,961][14046] Num visible devices: 1
[2026-01-17 16:27:33,961][13881] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 16:27:33,973][14049] Worker 3 uses CPU cores [9, 10, 11]
[2026-01-17 16:27:33,973][14052] Worker 6 uses CPU cores [18, 19, 20]
[2026-01-17 16:27:33,974][14053] Worker 7 uses CPU cores [21, 22, 23]
[2026-01-17 16:27:33,974][14051] Worker 5 uses CPU cores [15, 16, 17]
[2026-01-17 16:27:34,089][14046] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 16:27:34,090][14046] RunningMeanStd input shape: (1,)
[2026-01-17 16:27:34,097][14046] ConvEncoder: input_channels=3
[2026-01-17 16:27:34,163][14046] Conv encoder output size: 512
[2026-01-17 16:27:34,163][14046] Policy head output size: 512
[2026-01-17 16:27:34,197][13881] Inference worker 0-0 is ready!
[2026-01-17 16:27:34,197][13881] All inference workers are ready! Signal rollout workers to start!
[2026-01-17 16:27:34,224][14053] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,225][14047] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,225][14052] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,225][14049] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,226][14048] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,226][14051] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,227][14050] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,227][14045] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 16:27:34,469][14052] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,469][14053] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,471][14048] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,474][14051] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,480][14045] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,685][14053] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,687][14052] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,692][14048] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,708][14045] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,721][14047] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,725][14050] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,920][14049] Decorrelating experience for 0 frames...
[2026-01-17 16:27:34,936][14047] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,945][14051] Decorrelating experience for 32 frames...
[2026-01-17 16:27:34,950][14052] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,139][14050] Decorrelating experience for 32 frames...
[2026-01-17 16:27:35,180][14045] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,188][14052] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,199][14047] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,199][14053] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,212][14051] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,362][14048] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,429][14045] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,442][14049] Decorrelating experience for 32 frames...
[2026-01-17 16:27:35,443][14053] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,445][14047] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,459][14051] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,711][14050] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,714][14049] Decorrelating experience for 64 frames...
[2026-01-17 16:27:35,720][14048] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,960][14050] Decorrelating experience for 96 frames...
[2026-01-17 16:27:35,982][14049] Decorrelating experience for 96 frames...
[2026-01-17 16:27:36,040][13997] Signal inference workers to stop experience collection...
[2026-01-17 16:27:36,043][14046] InferenceWorker_p0-w0: stopping experience collection
[2026-01-17 16:27:36,835][13997] Signal inference workers to resume experience collection...
[2026-01-17 16:27:36,835][14046] InferenceWorker_p0-w0: resuming experience collection
[2026-01-17 16:27:37,608][13881] Fps is (10 sec: 7862.1, 60 sec: 7862.1, 300 sec: 7862.1). Total num frames: 28672. Throughput: 0: 1285.5. Samples: 4688. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[2026-01-17 16:27:37,608][13881] Avg episode reward: [(0, '3.682')]
[2026-01-17 16:27:37,846][14046] Updated weights for policy 0, policy_version 10 (0.0060)
[2026-01-17 16:27:38,914][14046] Updated weights for policy 0, policy_version 20 (0.0006)
[2026-01-17 16:27:39,964][14046] Updated weights for policy 0, policy_version 30 (0.0006)
[2026-01-17 16:27:41,022][14046] Updated weights for policy 0, policy_version 40 (0.0006)
[2026-01-17 16:27:42,074][14046] Updated weights for policy 0, policy_version 50 (0.0006)
[2026-01-17 16:27:42,608][13881] Fps is (10 sec: 26053.3, 60 sec: 26053.3, 300 sec: 26053.3). Total num frames: 225280. Throughput: 0: 3913.1. Samples: 33836. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:27:42,608][13881] Avg episode reward: [(0, '4.548')]
[2026-01-17 16:27:42,609][13997] Saving new best policy, reward=4.548!
[2026-01-17 16:27:43,161][14046] Updated weights for policy 0, policy_version 60 (0.0006)
[2026-01-17 16:27:44,225][14046] Updated weights for policy 0, policy_version 70 (0.0006)
[2026-01-17 16:27:45,296][14046] Updated weights for policy 0, policy_version 80 (0.0006)
[2026-01-17 16:27:46,357][14046] Updated weights for policy 0, policy_version 90 (0.0006)
[2026-01-17 16:27:47,412][14046] Updated weights for policy 0, policy_version 100 (0.0006)
[2026-01-17 16:27:47,608][13881] Fps is (10 sec: 38502.0, 60 sec: 30314.1, 300 sec: 30314.1). Total num frames: 413696. Throughput: 0: 6684.6. Samples: 91224. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)
[2026-01-17 16:27:47,609][13881] Avg episode reward: [(0, '4.545')]
[2026-01-17 16:27:48,486][14046] Updated weights for policy 0, policy_version 110 (0.0006)
[2026-01-17 16:27:49,593][14046] Updated weights for policy 0, policy_version 120 (0.0006)
[2026-01-17 16:27:50,662][14046] Updated weights for policy 0, policy_version 130 (0.0006)
[2026-01-17 16:27:51,334][13881] Heartbeat connected on Batcher_0
[2026-01-17 16:27:51,337][13881] Heartbeat connected on LearnerWorker_p0
[2026-01-17 16:27:51,344][13881] Heartbeat connected on InferenceWorker_p0-w0
[2026-01-17 16:27:51,348][13881] Heartbeat connected on RolloutWorker_w0
[2026-01-17 16:27:51,351][13881] Heartbeat connected on RolloutWorker_w1
[2026-01-17 16:27:51,354][13881] Heartbeat connected on RolloutWorker_w2
[2026-01-17 16:27:51,357][13881] Heartbeat connected on RolloutWorker_w3
[2026-01-17 16:27:51,361][13881] Heartbeat connected on RolloutWorker_w4
[2026-01-17 16:27:51,364][13881] Heartbeat connected on RolloutWorker_w5
[2026-01-17 16:27:51,367][13881] Heartbeat connected on RolloutWorker_w6
[2026-01-17 16:27:51,371][13881] Heartbeat connected on RolloutWorker_w7
[2026-01-17 16:27:51,750][14046] Updated weights for policy 0, policy_version 140 (0.0006)
[2026-01-17 16:27:52,608][13881] Fps is (10 sec: 37683.1, 60 sec: 32290.2, 300 sec: 32290.2). Total num frames: 602112. Throughput: 0: 7952.3. Samples: 148286. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:27:52,608][13881] Avg episode reward: [(0, '4.206')]
[2026-01-17 16:27:52,869][14046] Updated weights for policy 0, policy_version 150 (0.0006)
[2026-01-17 16:27:54,055][14046] Updated weights for policy 0, policy_version 160 (0.0007)
[2026-01-17 16:27:55,225][14046] Updated weights for policy 0, policy_version 170 (0.0007)
[2026-01-17 16:27:56,348][14046] Updated weights for policy 0, policy_version 180 (0.0006)
[2026-01-17 16:27:57,533][14046] Updated weights for policy 0, policy_version 190 (0.0006)
[2026-01-17 16:27:57,608][13881] Fps is (10 sec: 36454.5, 60 sec: 32910.8, 300 sec: 32910.8). Total num frames: 778240. Throughput: 0: 7392.5. Samples: 174810. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:27:57,609][13881] Avg episode reward: [(0, '4.592')]
[2026-01-17 16:27:57,611][13997] Saving new best policy, reward=4.592!
[2026-01-17 16:27:58,671][14046] Updated weights for policy 0, policy_version 200 (0.0006)
[2026-01-17 16:27:59,769][14046] Updated weights for policy 0, policy_version 210 (0.0006)
[2026-01-17 16:28:00,921][14046] Updated weights for policy 0, policy_version 220 (0.0006)
[2026-01-17 16:28:02,080][14046] Updated weights for policy 0, policy_version 230 (0.0006)
[2026-01-17 16:28:02,608][13881] Fps is (10 sec: 35634.9, 60 sec: 33457.7, 300 sec: 33457.7). Total num frames: 958464. Throughput: 0: 7971.0. Samples: 228346. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)
[2026-01-17 16:28:02,609][13881] Avg episode reward: [(0, '4.479')]
[2026-01-17 16:28:03,247][14046] Updated weights for policy 0, policy_version 240 (0.0006)
[2026-01-17 16:28:04,393][14046] Updated weights for policy 0, policy_version 250 (0.0006)
[2026-01-17 16:28:05,551][14046] Updated weights for policy 0, policy_version 260 (0.0006)
[2026-01-17 16:28:06,690][14046] Updated weights for policy 0, policy_version 270 (0.0007)
[2026-01-17 16:28:07,608][13881] Fps is (10 sec: 36045.0, 60 sec: 33842.3, 300 sec: 33842.3). Total num frames: 1138688. Throughput: 0: 8378.1. Samples: 281898. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:07,608][13881] Avg episode reward: [(0, '4.779')]
[2026-01-17 16:28:07,610][13997] Saving new best policy, reward=4.779!
[2026-01-17 16:28:07,850][14046] Updated weights for policy 0, policy_version 280 (0.0006)
[2026-01-17 16:28:09,010][14046] Updated weights for policy 0, policy_version 290 (0.0007)
[2026-01-17 16:28:10,146][14046] Updated weights for policy 0, policy_version 300 (0.0006)
[2026-01-17 16:28:11,295][14046] Updated weights for policy 0, policy_version 310 (0.0007)
[2026-01-17 16:28:12,332][14046] Updated weights for policy 0, policy_version 320 (0.0006)
[2026-01-17 16:28:12,608][13881] Fps is (10 sec: 36045.1, 60 sec: 34127.2, 300 sec: 34127.2). Total num frames: 1318912. Throughput: 0: 7978.9. Samples: 308358. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:12,608][13881] Avg episode reward: [(0, '4.988')]
[2026-01-17 16:28:12,609][13997] Saving new best policy, reward=4.988!
[2026-01-17 16:28:13,395][14046] Updated weights for policy 0, policy_version 330 (0.0006)
[2026-01-17 16:28:14,463][14046] Updated weights for policy 0, policy_version 340 (0.0006)
[2026-01-17 16:28:15,504][14046] Updated weights for policy 0, policy_version 350 (0.0006)
[2026-01-17 16:28:16,567][14046] Updated weights for policy 0, policy_version 360 (0.0006)
[2026-01-17 16:28:17,608][13881] Fps is (10 sec: 37273.6, 60 sec: 34628.4, 300 sec: 34628.4). Total num frames: 1511424. Throughput: 0: 8378.7. Samples: 365706. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)
[2026-01-17 16:28:17,608][13881] Avg episode reward: [(0, '5.031')]
[2026-01-17 16:28:17,610][13997] Saving new best policy, reward=5.031!
[2026-01-17 16:28:17,683][14046] Updated weights for policy 0, policy_version 370 (0.0006)
[2026-01-17 16:28:18,727][14046] Updated weights for policy 0, policy_version 380 (0.0006)
[2026-01-17 16:28:19,772][14046] Updated weights for policy 0, policy_version 390 (0.0006)
[2026-01-17 16:28:20,840][14046] Updated weights for policy 0, policy_version 400 (0.0006)
[2026-01-17 16:28:21,902][14046] Updated weights for policy 0, policy_version 410 (0.0006)
[2026-01-17 16:28:22,608][13881] Fps is (10 sec: 38502.5, 60 sec: 35026.6, 300 sec: 35026.6). Total num frames: 1703936. Throughput: 0: 9305.8. Samples: 423450. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:22,608][13881] Avg episode reward: [(0, '6.077')]
[2026-01-17 16:28:22,609][13997] Saving new best policy, reward=6.077!
[2026-01-17 16:28:22,981][14046] Updated weights for policy 0, policy_version 420 (0.0006)
[2026-01-17 16:28:24,062][14046] Updated weights for policy 0, policy_version 430 (0.0006)
[2026-01-17 16:28:25,091][14046] Updated weights for policy 0, policy_version 440 (0.0006)
[2026-01-17 16:28:26,171][14046] Updated weights for policy 0, policy_version 450 (0.0006)
[2026-01-17 16:28:27,234][14046] Updated weights for policy 0, policy_version 460 (0.0006)
[2026-01-17 16:28:27,608][13881] Fps is (10 sec: 38502.5, 60 sec: 35350.6, 300 sec: 35350.6). Total num frames: 1896448. Throughput: 0: 9301.5. Samples: 452402. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:28:27,608][13881] Avg episode reward: [(0, '6.880')]
[2026-01-17 16:28:27,610][13997] Saving new best policy, reward=6.880!
[2026-01-17 16:28:28,319][14046] Updated weights for policy 0, policy_version 470 (0.0006)
[2026-01-17 16:28:29,376][14046] Updated weights for policy 0, policy_version 480 (0.0006)
[2026-01-17 16:28:30,453][14046] Updated weights for policy 0, policy_version 490 (0.0006)
[2026-01-17 16:28:31,530][14046] Updated weights for policy 0, policy_version 500 (0.0007)
[2026-01-17 16:28:32,589][14046] Updated weights for policy 0, policy_version 510 (0.0006)
[2026-01-17 16:28:32,608][13881] Fps is (10 sec: 38502.3, 60 sec: 35619.3, 300 sec: 35619.3). Total num frames: 2088960. Throughput: 0: 9298.9. Samples: 509672. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:32,608][13881] Avg episode reward: [(0, '8.137')]
[2026-01-17 16:28:32,609][13997] Saving new best policy, reward=8.137!
[2026-01-17 16:28:33,667][14046] Updated weights for policy 0, policy_version 520 (0.0006)
[2026-01-17 16:28:34,727][14046] Updated weights for policy 0, policy_version 530 (0.0006)
[2026-01-17 16:28:35,787][14046] Updated weights for policy 0, policy_version 540 (0.0006)
[2026-01-17 16:28:36,836][14046] Updated weights for policy 0, policy_version 550 (0.0006)
[2026-01-17 16:28:37,608][13881] Fps is (10 sec: 38502.4, 60 sec: 37546.7, 300 sec: 35845.8). Total num frames: 2281472. Throughput: 0: 9314.4. Samples: 567436. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:37,608][13881] Avg episode reward: [(0, '10.211')]
[2026-01-17 16:28:37,610][13997] Saving new best policy, reward=10.211!
[2026-01-17 16:28:37,907][14046] Updated weights for policy 0, policy_version 560 (0.0006)
[2026-01-17 16:28:38,969][14046] Updated weights for policy 0, policy_version 570 (0.0006)
[2026-01-17 16:28:40,043][14046] Updated weights for policy 0, policy_version 580 (0.0006)
[2026-01-17 16:28:41,090][14046] Updated weights for policy 0, policy_version 590 (0.0006)
[2026-01-17 16:28:42,157][14046] Updated weights for policy 0, policy_version 600 (0.0006)
[2026-01-17 16:28:42,608][13881] Fps is (10 sec: 38502.2, 60 sec: 37478.4, 300 sec: 36039.2). Total num frames: 2473984. Throughput: 0: 9365.5. Samples: 596256. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:28:42,608][13881] Avg episode reward: [(0, '12.799')]
[2026-01-17 16:28:42,609][13997] Saving new best policy, reward=12.799!
[2026-01-17 16:28:43,203][14046] Updated weights for policy 0, policy_version 610 (0.0006)
[2026-01-17 16:28:44,234][14046] Updated weights for policy 0, policy_version 620 (0.0006)
[2026-01-17 16:28:45,282][14046] Updated weights for policy 0, policy_version 630 (0.0006)
[2026-01-17 16:28:46,336][14046] Updated weights for policy 0, policy_version 640 (0.0006)
[2026-01-17 16:28:47,392][14046] Updated weights for policy 0, policy_version 650 (0.0006)
[2026-01-17 16:28:47,608][13881] Fps is (10 sec: 38502.3, 60 sec: 37546.7, 300 sec: 36206.5). Total num frames: 2666496. Throughput: 0: 9478.9. Samples: 654896. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:28:47,608][13881] Avg episode reward: [(0, '16.315')]
[2026-01-17 16:28:47,610][13997] Saving new best policy, reward=16.315!
[2026-01-17 16:28:48,450][14046] Updated weights for policy 0, policy_version 660 (0.0006)
[2026-01-17 16:28:49,516][14046] Updated weights for policy 0, policy_version 670 (0.0006)
[2026-01-17 16:28:50,578][14046] Updated weights for policy 0, policy_version 680 (0.0006)
[2026-01-17 16:28:51,605][14046] Updated weights for policy 0, policy_version 690 (0.0006)
[2026-01-17 16:28:52,608][13881] Fps is (10 sec: 38911.8, 60 sec: 37683.1, 300 sec: 36404.5). Total num frames: 2863104. Throughput: 0: 9585.0. Samples: 713224. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:28:52,609][13881] Avg episode reward: [(0, '16.164')]
[2026-01-17 16:28:52,682][14046] Updated weights for policy 0, policy_version 700 (0.0006)
[2026-01-17 16:28:53,736][14046] Updated weights for policy 0, policy_version 710 (0.0006)
[2026-01-17 16:28:54,790][14046] Updated weights for policy 0, policy_version 720 (0.0006)
[2026-01-17 16:28:55,873][14046] Updated weights for policy 0, policy_version 730 (0.0006)
[2026-01-17 16:28:56,918][14046] Updated weights for policy 0, policy_version 740 (0.0006)
[2026-01-17 16:28:57,608][13881] Fps is (10 sec: 38911.9, 60 sec: 37956.3, 300 sec: 36529.9). Total num frames: 3055616. Throughput: 0: 9636.9. Samples: 742018. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:28:57,608][13881] Avg episode reward: [(0, '18.900')]
[2026-01-17 16:28:57,610][13997] Saving new best policy, reward=18.900!
[2026-01-17 16:28:58,009][14046] Updated weights for policy 0, policy_version 750 (0.0006)
[2026-01-17 16:28:59,061][14046] Updated weights for policy 0, policy_version 760 (0.0006)
[2026-01-17 16:29:00,113][14046] Updated weights for policy 0, policy_version 770 (0.0006)
[2026-01-17 16:29:01,175][14046] Updated weights for policy 0, policy_version 780 (0.0006)
[2026-01-17 16:29:02,226][14046] Updated weights for policy 0, policy_version 790 (0.0006)
[2026-01-17 16:29:02,608][13881] Fps is (10 sec: 38502.7, 60 sec: 38161.1, 300 sec: 36641.2). Total num frames: 3248128. Throughput: 0: 9643.3. Samples: 799656. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:29:02,608][13881] Avg episode reward: [(0, '20.547')]
[2026-01-17 16:29:02,609][13997] Saving new best policy, reward=20.547!
[2026-01-17 16:29:03,294][14046] Updated weights for policy 0, policy_version 800 (0.0006)
[2026-01-17 16:29:04,364][14046] Updated weights for policy 0, policy_version 810 (0.0006)
[2026-01-17 16:29:05,425][14046] Updated weights for policy 0, policy_version 820 (0.0006)
[2026-01-17 16:29:06,464][14046] Updated weights for policy 0, policy_version 830 (0.0005)
[2026-01-17 16:29:07,513][14046] Updated weights for policy 0, policy_version 840 (0.0006)
[2026-01-17 16:29:07,608][13881] Fps is (10 sec: 38502.1, 60 sec: 38365.8, 300 sec: 36740.5). Total num frames: 3440640. Throughput: 0: 9655.8. Samples: 857962. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)
[2026-01-17 16:29:07,609][13881] Avg episode reward: [(0, '20.603')]
[2026-01-17 16:29:07,615][13997] Saving new best policy, reward=20.603!
[2026-01-17 16:29:08,571][14046] Updated weights for policy 0, policy_version 850 (0.0006)
[2026-01-17 16:29:09,625][14046] Updated weights for policy 0, policy_version 860 (0.0006)
[2026-01-17 16:29:10,703][14046] Updated weights for policy 0, policy_version 870 (0.0006)
[2026-01-17 16:29:11,788][14046] Updated weights for policy 0, policy_version 880 (0.0006)
[2026-01-17 16:29:12,608][13881] Fps is (10 sec: 38502.7, 60 sec: 38570.7, 300 sec: 36829.9). Total num frames: 3633152. Throughput: 0: 9656.3. Samples: 886936. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)
[2026-01-17 16:29:12,608][13881] Avg episode reward: [(0, '18.835')]
[2026-01-17 16:29:12,845][14046] Updated weights for policy 0, policy_version 890 (0.0006)
[2026-01-17 16:29:13,907][14046] Updated weights for policy 0, policy_version 900 (0.0006)
[2026-01-17 16:29:14,972][14046] Updated weights for policy 0, policy_version 910 (0.0006)
[2026-01-17 16:29:16,042][14046] Updated weights for policy 0, policy_version 920 (0.0006)
[2026-01-17 16:29:17,085][14046] Updated weights for policy 0, policy_version 930 (0.0006)
[2026-01-17 16:29:17,608][13881] Fps is (10 sec: 38912.1, 60 sec: 38638.9, 300 sec: 36950.0). Total num frames: 3829760. Throughput: 0: 9661.0. Samples: 944416. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)
[2026-01-17 16:29:17,609][13881] Avg episode reward: [(0, '21.862')]
[2026-01-17 16:29:17,625][13997] Saving new best policy, reward=21.862!
[2026-01-17 16:29:18,111][14046] Updated weights for policy 0, policy_version 940 (0.0006)
[2026-01-17 16:29:19,169][14046] Updated weights for policy 0, policy_version 950 (0.0006)
[2026-01-17 16:29:20,236][14046] Updated weights for policy 0, policy_version 960 (0.0006)
[2026-01-17 16:29:21,281][14046] Updated weights for policy 0, policy_version 970 (0.0006)
[2026-01-17 16:29:22,134][13881] Component Batcher_0 stopped!
[2026-01-17 16:29:22,134][13997] Stopping Batcher_0...
[2026-01-17 16:29:22,135][13997] Loop batcher_evt_loop terminating...
[2026-01-17 16:29:22,135][13997] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:29:22,149][14046] Weights refcount: 2 0
[2026-01-17 16:29:22,150][14046] Stopping InferenceWorker_p0-w0...
[2026-01-17 16:29:22,150][13881] Component InferenceWorker_p0-w0 stopped!
[2026-01-17 16:29:22,150][14046] Loop inference_proc0-0_evt_loop terminating...
[2026-01-17 16:29:22,178][14048] Stopping RolloutWorker_w1...
[2026-01-17 16:29:22,178][13881] Component RolloutWorker_w1 stopped!
[2026-01-17 16:29:22,178][14048] Loop rollout_proc1_evt_loop terminating...
[2026-01-17 16:29:22,182][14053] Stopping RolloutWorker_w7...
[2026-01-17 16:29:22,182][13881] Component RolloutWorker_w7 stopped!
[2026-01-17 16:29:22,182][14053] Loop rollout_proc7_evt_loop terminating...
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w6 stopped!
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w2 stopped!
[2026-01-17 16:29:22,183][14047] Stopping RolloutWorker_w2...
[2026-01-17 16:29:22,183][14052] Stopping RolloutWorker_w6...
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w3 stopped!
[2026-01-17 16:29:22,184][14052] Loop rollout_proc6_evt_loop terminating...
[2026-01-17 16:29:22,184][14047] Loop rollout_proc2_evt_loop terminating...
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w5 stopped!
[2026-01-17 16:29:22,184][14051] Stopping RolloutWorker_w5...
[2026-01-17 16:29:22,184][14049] Stopping RolloutWorker_w3...
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w4 stopped!
[2026-01-17 16:29:22,184][14049] Loop rollout_proc3_evt_loop terminating...
[2026-01-17 16:29:22,184][14051] Loop rollout_proc5_evt_loop terminating...
[2026-01-17 16:29:22,184][14050] Stopping RolloutWorker_w4...
[2026-01-17 16:29:22,184][13881] Component RolloutWorker_w0 stopped!
[2026-01-17 16:29:22,184][14045] Stopping RolloutWorker_w0...
[2026-01-17 16:29:22,185][14050] Loop rollout_proc4_evt_loop terminating...
[2026-01-17 16:29:22,185][14045] Loop rollout_proc0_evt_loop terminating...
[2026-01-17 16:29:22,211][13997] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 16:29:22,288][13997] Stopping LearnerWorker_p0...
[2026-01-17 16:29:22,289][13997] Loop learner_proc0_evt_loop terminating...
[2026-01-17 16:29:22,289][13881] Component LearnerWorker_p0 stopped!
[2026-01-17 16:29:22,289][13881] Waiting for process learner_proc0 to stop...
[2026-01-17 16:29:23,214][13881] Waiting for process inference_proc0-0 to join...
[2026-01-17 16:29:23,215][13881] Waiting for process rollout_proc0 to join...
[2026-01-17 16:29:23,229][13881] Waiting for process rollout_proc1 to join...
[2026-01-17 16:29:23,229][13881] Waiting for process rollout_proc2 to join...
[2026-01-17 16:29:23,229][13881] Waiting for process rollout_proc3 to join...
[2026-01-17 16:29:23,229][13881] Waiting for process rollout_proc4 to join...
[2026-01-17 16:29:23,230][13881] Waiting for process rollout_proc5 to join...
[2026-01-17 16:29:23,230][13881] Waiting for process rollout_proc6 to join...
[2026-01-17 16:29:23,230][13881] Waiting for process rollout_proc7 to join...
[2026-01-17 16:29:23,230][13881] Batcher 0 profile tree view:
batching: 8.7713, releasing_batches: 0.0138
[2026-01-17 16:29:23,230][13881] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0000
  wait_policy_total: 2.2927
update_model: 1.6569
  weight_update: 0.0006
one_step: 0.0027
  handle_policy_step: 98.8426
    deserialize: 4.5755, stack: 0.5480, obs_to_device_normalize: 23.3763, forward: 46.1452, send_messages: 6.7629
    prepare_outputs: 13.9340
      to_cpu: 9.1003
[2026-01-17 16:29:23,231][13881] Learner 0 profile tree view:
misc: 0.0036, prepare_batch: 4.1183
train: 12.8968
  epoch_init: 0.0025, minibatch_init: 0.0028, losses_postprocess: 0.2263, kl_divergence: 0.1729, after_optimizer: 2.9823
  calculate_losses: 4.9440
    losses_init: 0.0016, forward_head: 0.3716, bptt_initial: 2.7865, tail: 0.3385, advantages_returns: 0.1027, losses: 0.6599
    bptt: 0.6043
      bptt_forward_core: 0.5746
  update: 4.3602
    clip: 0.4515
[2026-01-17 16:29:23,231][13881] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0717, enqueue_policy_requests: 4.6512, env_step: 60.8192, overhead: 2.5778, complete_rollouts: 0.1123
save_policy_outputs: 4.8806
  split_output_tensors: 1.6066
[2026-01-17 16:29:23,231][13881] RolloutWorker_w7 profile tree view:
wait_for_trajectories: 0.0640, enqueue_policy_requests: 4.1292, env_step: 52.4554, overhead: 2.3296, complete_rollouts: 0.0944
save_policy_outputs: 4.3179
  split_output_tensors: 1.4397
[2026-01-17 16:29:23,231][13881] Loop Runner_EvtLoop terminating...
[2026-01-17 16:29:23,231][13881] Runner profile tree view:
main_loop: 111.8608
[2026-01-17 16:29:23,231][13881] Collected {0: 4005888}, FPS: 35811.4
[2026-01-17 17:13:20,075][34446] Saving configuration to train_dir/default_experiment/config.json...
[2026-01-17 17:13:20,863][34446] Rollout worker 0 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 1 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 2 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 3 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 4 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 5 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 6 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 7 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 8 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 9 uses device cpu
[2026-01-17 17:13:20,863][34446] Rollout worker 10 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 11 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 12 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 13 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 14 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 15 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 16 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 17 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 18 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 19 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 20 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 21 uses device cpu
[2026-01-17 17:13:20,864][34446] Rollout worker 22 uses device cpu
[2026-01-17 17:13:20,865][34446] Rollout worker 23 uses device cpu
[2026-01-17 17:13:21,110][34446] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:13:21,110][34446] InferenceWorker_p0-w0: min num requests: 8
[2026-01-17 17:13:21,194][34446] Starting all processes...
[2026-01-17 17:13:21,194][34446] Starting process learner_proc0
[2026-01-17 17:13:22,507][34446] Starting all processes...
[2026-01-17 17:13:22,511][34446] Starting process inference_proc0-0
[2026-01-17 17:13:22,511][34446] Starting process rollout_proc0
[2026-01-17 17:13:22,511][34446] Starting process rollout_proc1
[2026-01-17 17:13:22,511][34446] Starting process rollout_proc2
[2026-01-17 17:13:22,511][34446] Starting process rollout_proc3
[2026-01-17 17:13:22,511][34446] Starting process rollout_proc4
[2026-01-17 17:13:22,513][34446] Starting process rollout_proc5
[2026-01-17 17:13:22,515][34446] Starting process rollout_proc6
[2026-01-17 17:13:22,515][34446] Starting process rollout_proc7
[2026-01-17 17:13:22,522][34563] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:13:22,523][34563] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2026-01-17 17:13:22,516][34446] Starting process rollout_proc8
[2026-01-17 17:13:22,516][34446] Starting process rollout_proc9
[2026-01-17 17:13:22,517][34446] Starting process rollout_proc10
[2026-01-17 17:13:22,517][34446] Starting process rollout_proc11
[2026-01-17 17:13:22,519][34446] Starting process rollout_proc12
[2026-01-17 17:13:22,519][34446] Starting process rollout_proc13
[2026-01-17 17:13:22,519][34446] Starting process rollout_proc14
[2026-01-17 17:13:22,546][34563] Num visible devices: 1
[2026-01-17 17:13:22,568][34563] Starting seed is not provided
[2026-01-17 17:13:22,568][34563] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:13:22,568][34563] Initializing actor-critic model on device cuda:0
[2026-01-17 17:13:22,569][34563] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 17:13:22,570][34563] RunningMeanStd input shape: (1,)
[2026-01-17 17:13:22,639][34563] ConvEncoder: input_channels=3
[2026-01-17 17:13:22,842][34563] Conv encoder output size: 512
[2026-01-17 17:13:22,843][34563] Policy head output size: 512
[2026-01-17 17:13:22,859][34563] Created Actor Critic model with architecture:
[2026-01-17 17:13:22,860][34563] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2026-01-17 17:13:23,374][34563] Using optimizer <class 'torch.optim.adam.Adam'>
[2026-01-17 17:13:24,012][34446] Starting process rollout_proc15
[2026-01-17 17:13:24,012][34446] Starting process rollout_proc16
[2026-01-17 17:13:24,028][34626] Worker 9 uses CPU cores [9]
[2026-01-17 17:13:24,030][34621] Worker 3 uses CPU cores [3]
[2026-01-17 17:13:24,048][34446] Starting process rollout_proc17
[2026-01-17 17:13:24,065][34618] Worker 0 uses CPU cores [0]
[2026-01-17 17:13:24,079][34446] Starting process rollout_proc18
[2026-01-17 17:13:24,090][34563] Loading state from checkpoint train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...
[2026-01-17 17:13:24,097][34446] Starting process rollout_proc19
[2026-01-17 17:13:24,112][34619] Worker 2 uses CPU cores [2]
[2026-01-17 17:13:24,128][34629] Worker 12 uses CPU cores [12]
[2026-01-17 17:13:24,136][34563] Loading model from checkpoint
[2026-01-17 17:13:24,138][34563] Loaded experiment state at self.train_step=978, self.env_steps=4005888
[2026-01-17 17:13:24,138][34563] Initialized policy 0 weights for model version 978
[2026-01-17 17:13:24,148][34446] Starting process rollout_proc20
[2026-01-17 17:13:24,147][34563] LearnerWorker_p0 finished initialization!
[2026-01-17 17:13:24,150][34563] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:13:24,153][34446] Starting process rollout_proc21
[2026-01-17 17:13:24,160][34446] Starting process rollout_proc22
[2026-01-17 17:13:24,176][34622] Worker 5 uses CPU cores [5]
[2026-01-17 17:13:24,218][34617] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:13:24,218][34624] Worker 6 uses CPU cores [6]
[2026-01-17 17:13:24,218][34617] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2026-01-17 17:13:24,265][34617] Num visible devices: 1
[2026-01-17 17:13:24,293][34446] Starting process rollout_proc23
[2026-01-17 17:13:24,317][34625] Worker 7 uses CPU cores [7]
[2026-01-17 17:13:24,356][34633] Worker 8 uses CPU cores [8]
[2026-01-17 17:13:24,369][34630] Worker 11 uses CPU cores [11]
[2026-01-17 17:13:24,453][34627] Worker 10 uses CPU cores [10]
[2026-01-17 17:13:24,461][34631] Worker 13 uses CPU cores [13]
[2026-01-17 17:13:24,508][34617] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 17:13:24,509][34617] RunningMeanStd input shape: (1,)
[2026-01-17 17:13:24,520][34617] ConvEncoder: input_channels=3
[2026-01-17 17:13:24,524][34620] Worker 1 uses CPU cores [1]
[2026-01-17 17:13:24,530][34632] Worker 14 uses CPU cores [14]
[2026-01-17 17:13:24,604][34623] Worker 4 uses CPU cores [4]
[2026-01-17 17:13:24,619][34617] Conv encoder output size: 512
[2026-01-17 17:13:24,620][34617] Policy head output size: 512
[2026-01-17 17:13:25,368][35032] Worker 16 uses CPU cores [16]
[2026-01-17 17:13:25,455][35031] Worker 15 uses CPU cores [15]
[2026-01-17 17:13:25,465][35138] Worker 19 uses CPU cores [19]
[2026-01-17 17:13:25,514][35231] Worker 21 uses CPU cores [21]
[2026-01-17 17:13:25,526][35079] Worker 17 uses CPU cores [17]
[2026-01-17 17:13:25,535][35232] Worker 22 uses CPU cores [22]
[2026-01-17 17:13:25,541][35218] Worker 20 uses CPU cores [20]
[2026-01-17 17:13:25,548][35112] Worker 18 uses CPU cores [18]
[2026-01-17 17:13:25,582][34446] Inference worker 0-0 is ready!
[2026-01-17 17:13:25,582][34446] All inference workers are ready! Signal rollout workers to start!
[2026-01-17 17:13:25,602][35418] Worker 23 uses CPU cores [23]
[2026-01-17 17:13:25,627][34625] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,639][34630] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,662][34633] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,665][34619] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,680][34624] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,684][35031] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,692][34629] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,694][35079] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,694][34626] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,694][35032] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,708][34621] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,710][34627] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,719][34632] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,719][34622] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,724][34618] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,728][34620] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,730][34623] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,739][34631] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,746][35232] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,763][35138] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,773][35218] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,773][35112] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,793][35231] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:25,913][35418] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:13:26,287][34631] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,296][34630] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,299][34622] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,303][34633] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,319][34623] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,327][35138] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,332][35231] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,377][34627] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,391][35232] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,524][34632] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,534][35112] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,544][35231] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,579][34629] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,595][35032] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,598][34618] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,607][34630] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,632][34623] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,668][35232] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,698][34627] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,759][34631] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,783][34632] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,786][34624] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,808][35218] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,841][35112] Decorrelating experience for 32 frames...
[2026-01-17 17:13:26,870][34623] Decorrelating experience for 64 frames...
[2026-01-17 17:13:26,879][35418] Decorrelating experience for 0 frames...
[2026-01-17 17:13:26,941][34630] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,011][34622] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,030][34626] Decorrelating experience for 0 frames...
[2026-01-17 17:13:27,030][35032] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,063][35112] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,070][34633] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,119][35418] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,130][35218] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,178][34627] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,229][34631] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,238][35138] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,254][34632] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,296][34630] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,308][34633] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,359][35232] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,380][35231] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,394][35112] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,408][34626] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,423][34446] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 4005888. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:13:27,472][35138] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,489][35031] Decorrelating experience for 0 frames...
[2026-01-17 17:13:27,514][34619] Decorrelating experience for 0 frames...
[2026-01-17 17:13:27,585][34632] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,595][35232] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,609][35032] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,612][34618] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,618][34631] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,660][34625] Decorrelating experience for 0 frames...
[2026-01-17 17:13:27,692][34633] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,742][35112] Decorrelating experience for 128 frames...
[2026-01-17 17:13:27,755][35031] Decorrelating experience for 32 frames...
[2026-01-17 17:13:27,796][34621] Decorrelating experience for 0 frames...
[2026-01-17 17:13:27,827][35138] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,842][35232] Decorrelating experience for 128 frames...
[2026-01-17 17:13:27,852][34632] Decorrelating experience for 128 frames...
[2026-01-17 17:13:27,907][34622] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,910][35231] Decorrelating experience for 96 frames...
[2026-01-17 17:13:27,910][34626] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,934][34618] Decorrelating experience for 64 frames...
[2026-01-17 17:13:27,966][34631] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,013][34619] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,047][34621] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,057][35218] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,094][34627] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,119][34625] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,129][35031] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,160][34626] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,162][34623] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,235][34619] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,249][35032] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,272][34618] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,281][35232] Decorrelating experience for 160 frames...
[2026-01-17 17:13:28,330][34631] Decorrelating experience for 160 frames...
[2026-01-17 17:13:28,342][34625] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,363][34622] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,375][34621] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,401][34630] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,527][35079] Decorrelating experience for 0 frames...
[2026-01-17 17:13:28,533][35218] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,568][34629] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,578][34625] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,578][34619] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,585][34627] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,598][34618] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,654][34633] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,664][35418] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,742][35079] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,744][34631] Decorrelating experience for 192 frames...
[2026-01-17 17:13:28,756][34624] Decorrelating experience for 32 frames...
[2026-01-17 17:13:28,827][35218] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,829][34629] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,843][34627] Decorrelating experience for 160 frames...
[2026-01-17 17:13:28,872][34623] Decorrelating experience for 128 frames...
[2026-01-17 17:13:28,879][34630] Decorrelating experience for 160 frames...
[2026-01-17 17:13:28,966][35112] Decorrelating experience for 160 frames...
[2026-01-17 17:13:28,972][35418] Decorrelating experience for 96 frames...
[2026-01-17 17:13:28,972][35079] Decorrelating experience for 64 frames...
[2026-01-17 17:13:28,980][34625] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,001][34618] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,065][34621] Decorrelating experience for 96 frames...
[2026-01-17 17:13:29,168][35032] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,183][34619] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,213][34623] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,233][34631] Decorrelating experience for 224 frames...
[2026-01-17 17:13:29,258][35079] Decorrelating experience for 96 frames...
[2026-01-17 17:13:29,272][34629] Decorrelating experience for 96 frames...
[2026-01-17 17:13:29,336][34622] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,341][35112] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,378][35231] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,408][35232] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,489][34633] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,525][34624] Decorrelating experience for 64 frames...
[2026-01-17 17:13:29,557][34618] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,595][34622] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,617][35031] Decorrelating experience for 96 frames...
[2026-01-17 17:13:29,620][34626] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,657][34623] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,669][35032] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,705][34627] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,761][34624] Decorrelating experience for 96 frames...
[2026-01-17 17:13:29,785][35138] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,807][35218] Decorrelating experience for 160 frames...
[2026-01-17 17:13:29,829][34620] Decorrelating experience for 0 frames...
[2026-01-17 17:13:29,944][35079] Decorrelating experience for 128 frames...
[2026-01-17 17:13:29,951][34633] Decorrelating experience for 192 frames...
[2026-01-17 17:13:29,954][35232] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,000][34619] Decorrelating experience for 160 frames...
[2026-01-17 17:13:30,019][34624] Decorrelating experience for 128 frames...
[2026-01-17 17:13:30,023][35031] Decorrelating experience for 128 frames...
[2026-01-17 17:13:30,027][34629] Decorrelating experience for 128 frames...
[2026-01-17 17:13:30,042][34622] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,081][34630] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,184][34626] Decorrelating experience for 160 frames...
[2026-01-17 17:13:30,198][34625] Decorrelating experience for 160 frames...
[2026-01-17 17:13:30,221][35032] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,251][34620] Decorrelating experience for 32 frames...
[2026-01-17 17:13:30,262][34627] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,284][35031] Decorrelating experience for 160 frames...
[2026-01-17 17:13:30,330][34622] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,387][35232] Decorrelating experience for 256 frames...
[2026-01-17 17:13:30,405][34633] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,419][34619] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,460][34623] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,492][34620] Decorrelating experience for 64 frames...
[2026-01-17 17:13:30,550][35138] Decorrelating experience for 160 frames...
[2026-01-17 17:13:30,634][34622] Decorrelating experience for 256 frames...
[2026-01-17 17:13:30,660][34631] Decorrelating experience for 256 frames...
[2026-01-17 17:13:30,672][34621] Decorrelating experience for 128 frames...
[2026-01-17 17:13:30,680][35218] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,695][34625] Decorrelating experience for 192 frames...
[2026-01-17 17:13:30,699][35232] Decorrelating experience for 288 frames...
[2026-01-17 17:13:30,726][34630] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,792][35032] Decorrelating experience for 224 frames...
[2026-01-17 17:13:30,914][34620] Decorrelating experience for 96 frames...
[2026-01-17 17:13:30,937][34633] Decorrelating experience for 256 frames...
[2026-01-17 17:13:30,937][35418] Decorrelating experience for 128 frames...
[2026-01-17 17:13:30,980][34624] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,001][34623] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,037][35138] Decorrelating experience for 192 frames...
[2026-01-17 17:13:31,075][35112] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,075][34625] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,153][34626] Decorrelating experience for 192 frames...
[2026-01-17 17:13:31,153][35031] Decorrelating experience for 192 frames...
[2026-01-17 17:13:31,177][35232] Decorrelating experience for 320 frames...
[2026-01-17 17:13:31,208][34622] Decorrelating experience for 288 frames...
[2026-01-17 17:13:31,208][34620] Decorrelating experience for 128 frames...
[2026-01-17 17:13:31,244][35218] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,297][35418] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,308][34631] Decorrelating experience for 288 frames...
[2026-01-17 17:13:31,406][35112] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,408][35138] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,428][35079] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,446][34623] Decorrelating experience for 288 frames...
[2026-01-17 17:13:31,511][34621] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,515][35031] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,526][34625] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,545][35218] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,597][35032] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,693][34626] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,713][34632] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,728][34619] Decorrelating experience for 224 frames...
[2026-01-17 17:13:31,754][34629] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,805][34631] Decorrelating experience for 320 frames...
[2026-01-17 17:13:31,828][34630] Decorrelating experience for 256 frames...
[2026-01-17 17:13:31,828][34620] Decorrelating experience for 160 frames...
[2026-01-17 17:13:31,871][35418] Decorrelating experience for 192 frames...
[2026-01-17 17:13:31,937][34625] Decorrelating experience for 288 frames...
[2026-01-17 17:13:31,962][34621] Decorrelating experience for 192 frames...
[2026-01-17 17:13:32,011][34633] Decorrelating experience for 288 frames...
[2026-01-17 17:13:32,045][34626] Decorrelating experience for 256 frames...
[2026-01-17 17:13:32,047][35232] Decorrelating experience for 352 frames...
[2026-01-17 17:13:32,087][34624] Decorrelating experience for 192 frames...
[2026-01-17 17:13:32,188][35418] Decorrelating experience for 224 frames...
[2026-01-17 17:13:32,218][34632] Decorrelating experience for 192 frames...
[2026-01-17 17:13:32,220][34627] Decorrelating experience for 256 frames...
[2026-01-17 17:13:32,248][34621] Decorrelating experience for 224 frames...
[2026-01-17 17:13:32,268][34623] Decorrelating experience for 320 frames...
[2026-01-17 17:13:32,268][34625] Decorrelating experience for 320 frames...
[2026-01-17 17:13:32,315][35079] Decorrelating experience for 192 frames...
[2026-01-17 17:13:32,386][34622] Decorrelating experience for 320 frames...
[2026-01-17 17:13:32,407][34626] Decorrelating experience for 288 frames...
[2026-01-17 17:13:32,423][34446] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4005888. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:13:32,494][34633] Decorrelating experience for 320 frames...
[2026-01-17 17:13:32,536][35218] Decorrelating experience for 288 frames...
[2026-01-17 17:13:32,563][35418] Decorrelating experience for 256 frames...
[2026-01-17 17:13:32,563][34630] Decorrelating experience for 288 frames...
[2026-01-17 17:13:32,576][34619] Decorrelating experience for 256 frames...
[2026-01-17 17:13:32,603][34621] Decorrelating experience for 256 frames...
[2026-01-17 17:13:32,610][34623] Decorrelating experience for 352 frames...
[2026-01-17 17:13:32,640][34624] Decorrelating experience for 224 frames...
[2026-01-17 17:13:32,736][34626] Decorrelating experience for 320 frames...
[2026-01-17 17:13:32,745][34618] Decorrelating experience for 224 frames...
[2026-01-17 17:13:32,823][34625] Decorrelating experience for 352 frames...
[2026-01-17 17:13:32,892][34633] Decorrelating experience for 352 frames...
[2026-01-17 17:13:32,892][34629] Decorrelating experience for 192 frames...
[2026-01-17 17:13:32,924][34632] Decorrelating experience for 224 frames...
[2026-01-17 17:13:32,973][34619] Decorrelating experience for 288 frames...
[2026-01-17 17:13:32,994][34624] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,023][35138] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,103][35031] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,110][34631] Decorrelating experience for 352 frames...
[2026-01-17 17:13:33,183][34629] Decorrelating experience for 224 frames...
[2026-01-17 17:13:33,186][34621] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,240][34626] Decorrelating experience for 352 frames...
[2026-01-17 17:13:33,260][35218] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,284][34632] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,316][34624] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,337][35418] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,402][35138] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,418][35032] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,496][34618] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,580][35231] Decorrelating experience for 160 frames...
[2026-01-17 17:13:33,582][34629] Decorrelating experience for 256 frames...
[2026-01-17 17:13:33,595][34622] Decorrelating experience for 352 frames...
[2026-01-17 17:13:33,667][34624] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,671][34621] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,675][34563] Signal inference workers to stop experience collection...
[2026-01-17 17:13:33,688][35418] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,688][34617] InferenceWorker_p0-w0: stopping experience collection
[2026-01-17 17:13:33,794][35138] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,823][34619] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,833][35031] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,857][35218] Decorrelating experience for 352 frames...
[2026-01-17 17:13:33,901][34630] Decorrelating experience for 320 frames...
[2026-01-17 17:13:33,968][34618] Decorrelating experience for 288 frames...
[2026-01-17 17:13:33,987][34629] Decorrelating experience for 288 frames...
[2026-01-17 17:13:34,040][35079] Decorrelating experience for 224 frames...
[2026-01-17 17:13:34,121][34621] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,135][35138] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,150][34627] Decorrelating experience for 288 frames...
[2026-01-17 17:13:34,218][34632] Decorrelating experience for 288 frames...
[2026-01-17 17:13:34,236][35032] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,265][34630] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,288][35112] Decorrelating experience for 288 frames...
[2026-01-17 17:13:34,360][34629] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,435][34619] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,439][35418] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,473][34627] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,509][34563] Signal inference workers to resume experience collection...
[2026-01-17 17:13:34,510][34563] Stopping Batcher_0...
[2026-01-17 17:13:34,511][34563] Loop batcher_evt_loop terminating...
[2026-01-17 17:13:34,514][34446] Component Batcher_0 stopped!
[2026-01-17 17:13:34,520][34617] Weights refcount: 2 0
[2026-01-17 17:13:34,521][34617] Stopping InferenceWorker_p0-w0...
[2026-01-17 17:13:34,521][34446] Component InferenceWorker_p0-w0 stopped!
[2026-01-17 17:13:34,521][34617] Loop inference_proc0-0_evt_loop terminating...
[2026-01-17 17:13:34,629][35032] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,641][34446] Component RolloutWorker_w19 stopped!
[2026-01-17 17:13:34,642][34631] Stopping RolloutWorker_w13...
[2026-01-17 17:13:34,642][34446] Component RolloutWorker_w13 stopped!
[2026-01-17 17:13:34,642][34631] Loop rollout_proc13_evt_loop terminating...
[2026-01-17 17:13:34,642][35138] Stopping RolloutWorker_w19...
[2026-01-17 17:13:34,642][35138] Loop rollout_proc19_evt_loop terminating...
[2026-01-17 17:13:34,645][34626] Stopping RolloutWorker_w9...
[2026-01-17 17:13:34,646][34446] Component RolloutWorker_w9 stopped!
[2026-01-17 17:13:34,646][34626] Loop rollout_proc9_evt_loop terminating...
[2026-01-17 17:13:34,649][34622] Stopping RolloutWorker_w5...
[2026-01-17 17:13:34,649][34446] Component RolloutWorker_w5 stopped!
[2026-01-17 17:13:34,649][34622] Loop rollout_proc5_evt_loop terminating...
[2026-01-17 17:13:34,649][34446] Component RolloutWorker_w7 stopped!
[2026-01-17 17:13:34,649][34625] Stopping RolloutWorker_w7...
[2026-01-17 17:13:34,649][34446] Component RolloutWorker_w3 stopped!
[2026-01-17 17:13:34,649][34625] Loop rollout_proc7_evt_loop terminating...
[2026-01-17 17:13:34,649][34621] Stopping RolloutWorker_w3...
[2026-01-17 17:13:34,650][34621] Loop rollout_proc3_evt_loop terminating...
[2026-01-17 17:13:34,650][35218] Stopping RolloutWorker_w20...
[2026-01-17 17:13:34,650][35218] Loop rollout_proc20_evt_loop terminating...
[2026-01-17 17:13:34,651][34446] Component RolloutWorker_w20 stopped!
[2026-01-17 17:13:34,652][34446] Component RolloutWorker_w22 stopped!
[2026-01-17 17:13:34,653][35232] Stopping RolloutWorker_w22...
[2026-01-17 17:13:34,653][35232] Loop rollout_proc22_evt_loop terminating...
[2026-01-17 17:13:34,657][34630] Stopping RolloutWorker_w11...
[2026-01-17 17:13:34,658][34630] Loop rollout_proc11_evt_loop terminating...
[2026-01-17 17:13:34,659][34446] Component RolloutWorker_w11 stopped!
[2026-01-17 17:13:34,665][34623] Stopping RolloutWorker_w4...
[2026-01-17 17:13:34,665][34446] Component RolloutWorker_w4 stopped!
[2026-01-17 17:13:34,666][34623] Loop rollout_proc4_evt_loop terminating...
[2026-01-17 17:13:34,675][34632] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,686][34446] Component RolloutWorker_w8 stopped!
[2026-01-17 17:13:34,686][34633] Stopping RolloutWorker_w8...
[2026-01-17 17:13:34,687][34633] Loop rollout_proc8_evt_loop terminating...
[2026-01-17 17:13:34,692][35112] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,745][35231] Decorrelating experience for 192 frames...
[2026-01-17 17:13:34,753][35418] Stopping RolloutWorker_w23...
[2026-01-17 17:13:34,753][34446] Component RolloutWorker_w23 stopped!
[2026-01-17 17:13:34,753][35418] Loop rollout_proc23_evt_loop terminating...
[2026-01-17 17:13:34,756][34618] Decorrelating experience for 320 frames...
[2026-01-17 17:13:34,763][34624] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,766][34446] Component RolloutWorker_w2 stopped!
[2026-01-17 17:13:34,767][34619] Stopping RolloutWorker_w2...
[2026-01-17 17:13:34,767][34619] Loop rollout_proc2_evt_loop terminating...
[2026-01-17 17:13:34,927][34627] Decorrelating experience for 352 frames...
[2026-01-17 17:13:34,973][35032] Stopping RolloutWorker_w16...
[2026-01-17 17:13:34,974][34446] Component RolloutWorker_w16 stopped!
[2026-01-17 17:13:34,974][35032] Loop rollout_proc16_evt_loop terminating...
[2026-01-17 17:13:35,055][34563] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2026-01-17 17:13:35,074][35031] Decorrelating experience for 320 frames...
[2026-01-17 17:13:35,096][34629] Decorrelating experience for 352 frames...
[2026-01-17 17:13:35,115][34624] Stopping RolloutWorker_w6...
[2026-01-17 17:13:35,115][34446] Component RolloutWorker_w6 stopped!
[2026-01-17 17:13:35,115][34624] Loop rollout_proc6_evt_loop terminating...
[2026-01-17 17:13:35,127][34563] Removing train_dir/default_experiment/checkpoint_p0/checkpoint_000000064_262144.pth
[2026-01-17 17:13:35,133][34563] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2026-01-17 17:13:35,143][35079] Decorrelating experience for 256 frames...
[2026-01-17 17:13:35,146][34632] Decorrelating experience for 352 frames...
[2026-01-17 17:13:35,162][35231] Decorrelating experience for 224 frames...
[2026-01-17 17:13:35,187][35112] Decorrelating experience for 352 frames...
[2026-01-17 17:13:35,234][34563] Stopping LearnerWorker_p0...
[2026-01-17 17:13:35,235][34563] Loop learner_proc0_evt_loop terminating...
[2026-01-17 17:13:35,236][34446] Component LearnerWorker_p0 stopped!
[2026-01-17 17:13:35,284][34627] Stopping RolloutWorker_w10...
[2026-01-17 17:13:35,285][34446] Component RolloutWorker_w10 stopped!
[2026-01-17 17:13:35,287][34627] Loop rollout_proc10_evt_loop terminating...
[2026-01-17 17:13:35,411][34629] Stopping RolloutWorker_w12...
[2026-01-17 17:13:35,412][34446] Component RolloutWorker_w12 stopped!
[2026-01-17 17:13:35,412][34629] Loop rollout_proc12_evt_loop terminating...
[2026-01-17 17:13:35,482][34632] Stopping RolloutWorker_w14...
[2026-01-17 17:13:35,482][34446] Component RolloutWorker_w14 stopped!
[2026-01-17 17:13:35,483][34632] Loop rollout_proc14_evt_loop terminating...
[2026-01-17 17:13:35,533][35112] Stopping RolloutWorker_w18...
[2026-01-17 17:13:35,533][34446] Component RolloutWorker_w18 stopped!
[2026-01-17 17:13:35,533][35112] Loop rollout_proc18_evt_loop terminating...
[2026-01-17 17:13:35,543][35031] Decorrelating experience for 352 frames...
[2026-01-17 17:13:35,546][34618] Decorrelating experience for 352 frames...
[2026-01-17 17:13:35,581][35231] Decorrelating experience for 256 frames...
[2026-01-17 17:13:35,603][35079] Decorrelating experience for 288 frames...
[2026-01-17 17:13:35,677][34620] Decorrelating experience for 192 frames...
[2026-01-17 17:13:35,782][35031] Stopping RolloutWorker_w15...
[2026-01-17 17:13:35,782][34446] Component RolloutWorker_w15 stopped!
[2026-01-17 17:13:35,782][35031] Loop rollout_proc15_evt_loop terminating...
[2026-01-17 17:13:35,837][34618] Stopping RolloutWorker_w0...
[2026-01-17 17:13:35,837][34446] Component RolloutWorker_w0 stopped!
[2026-01-17 17:13:35,837][34618] Loop rollout_proc0_evt_loop terminating...
[2026-01-17 17:13:35,908][35231] Decorrelating experience for 288 frames...
[2026-01-17 17:13:35,964][35079] Decorrelating experience for 320 frames...
[2026-01-17 17:13:35,986][34620] Decorrelating experience for 224 frames...
[2026-01-17 17:13:36,220][35231] Decorrelating experience for 320 frames...
[2026-01-17 17:13:36,283][34620] Decorrelating experience for 256 frames...
[2026-01-17 17:13:36,300][35079] Decorrelating experience for 352 frames...
[2026-01-17 17:13:36,514][35079] Stopping RolloutWorker_w17...
[2026-01-17 17:13:36,514][34446] Component RolloutWorker_w17 stopped!
[2026-01-17 17:13:36,515][35079] Loop rollout_proc17_evt_loop terminating...
[2026-01-17 17:13:36,547][35231] Decorrelating experience for 352 frames...
[2026-01-17 17:13:36,588][34620] Decorrelating experience for 288 frames...
[2026-01-17 17:13:36,747][35231] Stopping RolloutWorker_w21...
[2026-01-17 17:13:36,747][34446] Component RolloutWorker_w21 stopped!
[2026-01-17 17:13:36,747][35231] Loop rollout_proc21_evt_loop terminating...
[2026-01-17 17:13:36,905][34620] Decorrelating experience for 320 frames...
[2026-01-17 17:13:37,231][34620] Decorrelating experience for 352 frames...
[2026-01-17 17:13:37,444][34620] Stopping RolloutWorker_w1...
[2026-01-17 17:13:37,444][34446] Component RolloutWorker_w1 stopped!
[2026-01-17 17:13:37,444][34446] Waiting for process learner_proc0 to stop...
[2026-01-17 17:13:37,444][34620] Loop rollout_proc1_evt_loop terminating...
[2026-01-17 17:13:37,444][34446] Waiting for process inference_proc0-0 to join...
[2026-01-17 17:13:37,444][34446] Waiting for process rollout_proc0 to join...
[2026-01-17 17:13:37,445][34446] Waiting for process rollout_proc1 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc2 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc3 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc4 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc5 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc6 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc7 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc8 to join...
[2026-01-17 17:13:37,743][34446] Waiting for process rollout_proc9 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc10 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc11 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc12 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc13 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc14 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc15 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc16 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc17 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc18 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc19 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc20 to join...
[2026-01-17 17:13:37,744][34446] Waiting for process rollout_proc21 to join...
[2026-01-17 17:13:37,745][34446] Waiting for process rollout_proc22 to join...
[2026-01-17 17:13:37,745][34446] Waiting for process rollout_proc23 to join...
[2026-01-17 17:13:37,745][34446] Batcher 0 profile tree view:
batching: 0.0204, releasing_batches: 0.0004
[2026-01-17 17:13:37,745][34446] InferenceWorker_p0-w0 profile tree view:
update_model: 0.0049
wait_policy: 0.0014
  wait_policy_total: 8.3215
one_step: 0.0031
  handle_policy_step: 0.6277
    deserialize: 0.0297, stack: 0.0016, obs_to_device_normalize: 0.1178, forward: 0.4397, send_messages: 0.0112
    prepare_outputs: 0.0206
      to_cpu: 0.0086
[2026-01-17 17:13:37,745][34446] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 1.0321
train: 0.5292
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0002, kl_divergence: 0.0085, after_optimizer: 0.0408
  calculate_losses: 0.1604
    losses_init: 0.0000, forward_head: 0.0621, bptt_initial: 0.0639, tail: 0.0120, advantages_returns: 0.0007, losses: 0.0198
    bptt: 0.0017
      bptt_forward_core: 0.0017
  update: 0.3185
    clip: 0.0308
[2026-01-17 17:13:37,745][34446] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0007, enqueue_policy_requests: 0.0010
[2026-01-17 17:13:37,745][34446] RolloutWorker_w23 profile tree view:
wait_for_trajectories: 0.0005, enqueue_policy_requests: 0.0012
[2026-01-17 17:13:37,745][34446] Loop Runner_EvtLoop terminating...
[2026-01-17 17:13:37,746][34446] Runner profile tree view:
main_loop: 16.5520
[2026-01-17 17:13:37,746][34446] Collected {0: 4014080}, FPS: 494.9
[2026-01-17 17:15:31,365][38814] Saving configuration to train_dir/default_experiment/config.json...
[2026-01-17 17:15:32,197][38814] Rollout worker 0 uses device cpu
[2026-01-17 17:15:32,197][38814] Rollout worker 1 uses device cpu
[2026-01-17 17:15:32,197][38814] Rollout worker 2 uses device cpu
[2026-01-17 17:15:32,197][38814] Rollout worker 3 uses device cpu
[2026-01-17 17:15:32,197][38814] Rollout worker 4 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 5 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 6 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 7 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 8 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 9 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 10 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 11 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 12 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 13 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 14 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 15 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 16 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 17 uses device cpu
[2026-01-17 17:15:32,198][38814] Rollout worker 18 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 19 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 20 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 21 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 22 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 23 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 24 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 25 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 26 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 27 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 28 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 29 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 30 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 31 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 32 uses device cpu
[2026-01-17 17:15:32,199][38814] Rollout worker 33 uses device cpu
[2026-01-17 17:15:32,200][38814] Rollout worker 34 uses device cpu
[2026-01-17 17:15:32,200][38814] Rollout worker 35 uses device cpu
[2026-01-17 17:15:32,930][38814] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:15:32,931][38814] InferenceWorker_p0-w0: min num requests: 12
[2026-01-17 17:15:33,058][38814] Starting all processes...
[2026-01-17 17:15:33,058][38814] Starting process learner_proc0
[2026-01-17 17:15:34,361][38814] Starting all processes...
[2026-01-17 17:15:34,377][38936] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:15:34,377][38936] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0
[2026-01-17 17:15:34,380][38814] Starting process inference_proc0-0
[2026-01-17 17:15:34,380][38814] Starting process rollout_proc0
[2026-01-17 17:15:34,380][38814] Starting process rollout_proc1
[2026-01-17 17:15:34,380][38814] Starting process rollout_proc2
[2026-01-17 17:15:34,384][38814] Starting process rollout_proc3
[2026-01-17 17:15:34,385][38814] Starting process rollout_proc4
[2026-01-17 17:15:34,386][38814] Starting process rollout_proc5
[2026-01-17 17:15:34,386][38814] Starting process rollout_proc6
[2026-01-17 17:15:34,387][38814] Starting process rollout_proc7
[2026-01-17 17:15:34,387][38814] Starting process rollout_proc8
[2026-01-17 17:15:34,399][38936] Num visible devices: 1
[2026-01-17 17:15:34,387][38814] Starting process rollout_proc9
[2026-01-17 17:15:34,390][38814] Starting process rollout_proc10
[2026-01-17 17:15:34,390][38814] Starting process rollout_proc11
[2026-01-17 17:15:34,407][38936] Starting seed is not provided
[2026-01-17 17:15:34,407][38936] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:15:34,407][38936] Initializing actor-critic model on device cuda:0
[2026-01-17 17:15:34,407][38936] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 17:15:34,390][38814] Starting process rollout_proc12
[2026-01-17 17:15:34,408][38936] RunningMeanStd input shape: (1,)
[2026-01-17 17:15:34,390][38814] Starting process rollout_proc13
[2026-01-17 17:15:34,392][38814] Starting process rollout_proc14
[2026-01-17 17:15:34,416][38936] ConvEncoder: input_channels=3
[2026-01-17 17:15:34,643][38936] Conv encoder output size: 512
[2026-01-17 17:15:34,644][38936] Policy head output size: 512
[2026-01-17 17:15:34,664][38936] Created Actor Critic model with architecture:
[2026-01-17 17:15:34,664][38936] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): VizdoomEncoder(
    (basic_encoder): ConvEncoder(
      (enc): RecursiveScriptModule(
        original_name=ConvEncoderImpl
        (conv_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Conv2d)
          (1): RecursiveScriptModule(original_name=ELU)
          (2): RecursiveScriptModule(original_name=Conv2d)
          (3): RecursiveScriptModule(original_name=ELU)
          (4): RecursiveScriptModule(original_name=Conv2d)
          (5): RecursiveScriptModule(original_name=ELU)
        )
        (mlp_layers): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=ELU)
        )
      )
    )
  )
  (core): ModelCoreRNN(
    (core): GRU(512, 512)
  )
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=512, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationDefault(
    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)
  )
)
[2026-01-17 17:15:35,013][38936] Using optimizer <class 'torch.optim.adam.Adam'>
[2026-01-17 17:15:35,581][38936] Loading state from checkpoint train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth...
[2026-01-17 17:15:35,605][38936] Loading model from checkpoint
[2026-01-17 17:15:35,606][38936] Loaded experiment state at self.train_step=980, self.env_steps=4014080
[2026-01-17 17:15:35,606][38936] Initialized policy 0 weights for model version 980
[2026-01-17 17:15:35,608][38936] LearnerWorker_p0 finished initialization!
[2026-01-17 17:15:35,608][38936] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:15:35,816][38814] Starting process rollout_proc15
[2026-01-17 17:15:35,829][38814] Starting process rollout_proc16
[2026-01-17 17:15:35,831][39023] Worker 10 uses CPU cores [10]
[2026-01-17 17:15:35,846][38990] Worker 0 uses CPU cores [0]
[2026-01-17 17:15:35,936][38814] Starting process rollout_proc17
[2026-01-17 17:15:35,956][38991] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[2026-01-17 17:15:35,956][38991] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0
[2026-01-17 17:15:35,956][38814] Starting process rollout_proc18
[2026-01-17 17:15:35,975][38991] Num visible devices: 1
[2026-01-17 17:15:35,978][39024] Worker 9 uses CPU cores [9]
[2026-01-17 17:15:35,996][38814] Starting process rollout_proc19
[2026-01-17 17:15:36,007][38814] Starting process rollout_proc20
[2026-01-17 17:15:36,012][38995] Worker 3 uses CPU cores [3]
[2026-01-17 17:15:36,041][38814] Starting process rollout_proc21
[2026-01-17 17:15:36,089][38814] Starting process rollout_proc22
[2026-01-17 17:15:36,093][38996] Worker 4 uses CPU cores [4]
[2026-01-17 17:15:36,109][38992] Worker 1 uses CPU cores [1]
[2026-01-17 17:15:36,113][39028] Worker 13 uses CPU cores [13]
[2026-01-17 17:15:36,137][38814] Starting process rollout_proc23
[2026-01-17 17:15:36,182][38814] Starting process rollout_proc24
[2026-01-17 17:15:36,192][38814] Starting process rollout_proc25
[2026-01-17 17:15:36,202][38998] Worker 6 uses CPU cores [6]
[2026-01-17 17:15:36,213][38994] Worker 2 uses CPU cores [2]
[2026-01-17 17:15:36,229][38814] Starting process rollout_proc26
[2026-01-17 17:15:36,230][39027] Worker 12 uses CPU cores [12]
[2026-01-17 17:15:36,261][38814] Starting process rollout_proc27
[2026-01-17 17:15:36,284][39026] Worker 8 uses CPU cores [8]
[2026-01-17 17:15:36,310][39029] Worker 14 uses CPU cores [14]
[2026-01-17 17:15:36,311][38991] RunningMeanStd input shape: (3, 72, 128)
[2026-01-17 17:15:36,312][38991] RunningMeanStd input shape: (1,)
[2026-01-17 17:15:36,318][38814] Starting process rollout_proc28
[2026-01-17 17:15:36,324][38991] ConvEncoder: input_channels=3
[2026-01-17 17:15:36,335][38999] Worker 7 uses CPU cores [7]
[2026-01-17 17:15:36,337][38814] Starting process rollout_proc29
[2026-01-17 17:15:36,349][38814] Starting process rollout_proc30
[2026-01-17 17:15:36,354][39030] Worker 11 uses CPU cores [11]
[2026-01-17 17:15:36,429][38997] Worker 5 uses CPU cores [5]
[2026-01-17 17:15:36,536][38991] Conv encoder output size: 512
[2026-01-17 17:15:36,536][38991] Policy head output size: 512
[2026-01-17 17:15:37,366][38814] Starting process rollout_proc31
[2026-01-17 17:15:37,386][39415] Worker 15 uses CPU cores [15]
[2026-01-17 17:15:37,449][38814] Starting process rollout_proc32
[2026-01-17 17:15:37,482][39416] Worker 16 uses CPU cores [16]
[2026-01-17 17:15:37,491][38814] Starting process rollout_proc33
[2026-01-17 17:15:37,507][39544] Worker 19 uses CPU cores [19]
[2026-01-17 17:15:37,568][38814] Starting process rollout_proc34
[2026-01-17 17:15:37,590][39510] Worker 18 uses CPU cores [18]
[2026-01-17 17:15:37,602][38814] Starting process rollout_proc35
[2026-01-17 17:15:37,626][39629] Worker 21 uses CPU cores [21]
[2026-01-17 17:15:37,658][39795] Worker 24 uses CPU cores [0, 1]
[2026-01-17 17:15:37,753][39509] Worker 17 uses CPU cores [17]
[2026-01-17 17:15:37,783][39582] Worker 20 uses CPU cores [20]
[2026-01-17 17:15:37,882][39796] Worker 25 uses CPU cores [2, 3]
[2026-01-17 17:15:37,890][39768] Worker 23 uses CPU cores [23]
[2026-01-17 17:15:37,901][39936] Worker 27 uses CPU cores [6, 7]
[2026-01-17 17:15:37,906][39653] Worker 22 uses CPU cores [22]
[2026-01-17 17:15:37,913][39826] Worker 26 uses CPU cores [4, 5]
[2026-01-17 17:15:37,933][39974] Worker 28 uses CPU cores [8, 9]
[2026-01-17 17:15:37,964][40052] Worker 30 uses CPU cores [12, 13]
[2026-01-17 17:15:37,985][39999] Worker 29 uses CPU cores [10, 11]
[2026-01-17 17:15:38,670][40170] Worker 31 uses CPU cores [14, 15]
[2026-01-17 17:15:38,753][40228] Worker 33 uses CPU cores [18, 19]
[2026-01-17 17:15:38,772][40217] Worker 32 uses CPU cores [16, 17]
[2026-01-17 17:15:38,807][40311] Worker 34 uses CPU cores [20, 21]
[2026-01-17 17:15:38,822][38814] Inference worker 0-0 is ready!
[2026-01-17 17:15:38,822][38814] All inference workers are ready! Signal rollout workers to start!
[2026-01-17 17:15:38,823][38814] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 4014080. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:15:38,877][39024] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,882][40335] Worker 35 uses CPU cores [22, 23]
[2026-01-17 17:15:38,887][39027] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,890][39030] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,891][38998] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,892][38996] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,893][39415] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,894][38992] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,891][38994] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,892][39544] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,906][39509] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,919][39653] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,941][39768] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,956][39026] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,954][39974] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,962][39999] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,965][39936] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,966][38995] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,968][38997] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,986][40052] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:38,995][39629] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,008][40228] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,011][39029] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,012][39795] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,015][39796] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,019][38999] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,017][39510] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,031][39826] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,037][40217] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,042][39023] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,058][39416] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,068][38990] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,073][39028] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,070][40170] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,079][39582] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,080][40311] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:39,150][40335] Doom resolution: 160x120, resize resolution: (128, 72)
[2026-01-17 17:15:40,081][39415] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,170][38996] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,185][39629] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,196][39030] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,300][39027] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,318][39024] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,359][39416] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,396][39653] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,397][40228] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,414][39510] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,423][39026] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,430][39768] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,495][38996] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,522][38998] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,536][39795] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,571][39582] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,596][39796] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,633][39999] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,633][38999] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,647][39629] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,678][39510] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,722][39936] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,730][39653] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,744][39026] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,764][39826] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,810][39029] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,850][38990] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,874][38997] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,881][40335] Decorrelating experience for 0 frames...
[2026-01-17 17:15:40,923][39030] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,950][39999] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,953][39795] Decorrelating experience for 32 frames...
[2026-01-17 17:15:40,959][39415] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,010][39768] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,015][39509] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,047][39510] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,094][38994] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,095][40217] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,129][40335] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,145][39582] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,223][39544] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,238][39629] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,240][39023] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,253][39024] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,272][38996] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,304][39653] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,352][39796] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,372][39974] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,422][40311] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,432][40170] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,450][39028] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,456][39415] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,463][39027] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,509][38996] Decorrelating experience for 96 frames...
[2026-01-17 17:15:41,566][38994] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,579][39509] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,587][39024] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,643][39629] Decorrelating experience for 96 frames...
[2026-01-17 17:15:41,645][39768] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,703][39999] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,740][39026] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,740][39796] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,740][39029] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,753][39582] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,806][39023] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,811][40217] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,859][38994] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,882][39795] Decorrelating experience for 64 frames...
[2026-01-17 17:15:41,927][38998] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,942][39974] Decorrelating experience for 32 frames...
[2026-01-17 17:15:41,965][40052] Decorrelating experience for 0 frames...
[2026-01-17 17:15:41,968][38996] Decorrelating experience for 128 frames...
[2026-01-17 17:15:42,007][39029] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,025][39510] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,037][39024] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,083][39768] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,137][39826] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,150][39582] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,163][39030] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,237][39028] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,246][38992] Decorrelating experience for 0 frames...
[2026-01-17 17:15:42,269][39510] Decorrelating experience for 128 frames...
[2026-01-17 17:15:42,313][39936] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,313][39544] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,338][38994] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,394][38996] Decorrelating experience for 160 frames...
[2026-01-17 17:15:42,394][39796] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,455][39029] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,457][39509] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,547][40170] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,554][39795] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,577][40217] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,585][40052] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,613][40335] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,636][38992] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,638][40311] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,711][38990] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,725][39030] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,731][39509] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,798][39415] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,842][39544] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,843][38997] Decorrelating experience for 32 frames...
[2026-01-17 17:15:42,850][39023] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,881][39768] Decorrelating experience for 128 frames...
[2026-01-17 17:15:42,901][39795] Decorrelating experience for 128 frames...
[2026-01-17 17:15:42,914][40335] Decorrelating experience for 96 frames...
[2026-01-17 17:15:42,925][39974] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,976][40052] Decorrelating experience for 64 frames...
[2026-01-17 17:15:42,986][40217] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,008][38998] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,081][39544] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,092][39024] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,093][38997] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,101][39415] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,154][39416] Decorrelating experience for 32 frames...
[2026-01-17 17:15:43,208][39030] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,211][40052] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,212][39768] Decorrelating experience for 160 frames...
[2026-01-17 17:15:43,224][40170] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,225][39795] Decorrelating experience for 160 frames...
[2026-01-17 17:15:43,321][40311] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,345][38998] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,381][38997] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,385][39936] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,466][38990] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,467][39024] Decorrelating experience for 160 frames...
[2026-01-17 17:15:43,470][39544] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,476][40217] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,510][40052] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,597][39026] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,599][39653] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,603][39795] Decorrelating experience for 192 frames...
[2026-01-17 17:15:43,680][39509] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,702][38814] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4014080. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:15:43,709][38997] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,725][39936] Decorrelating experience for 96 frames...
[2026-01-17 17:15:43,730][38996] Decorrelating experience for 192 frames...
[2026-01-17 17:15:43,748][39415] Decorrelating experience for 160 frames...
[2026-01-17 17:15:43,850][39653] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,853][40228] Decorrelating experience for 32 frames...
[2026-01-17 17:15:43,870][38995] Decorrelating experience for 0 frames...
[2026-01-17 17:15:43,885][39416] Decorrelating experience for 64 frames...
[2026-01-17 17:15:43,956][38994] Decorrelating experience for 128 frames...
[2026-01-17 17:15:43,963][39974] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,004][38998] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,083][40228] Decorrelating experience for 64 frames...
[2026-01-17 17:15:44,106][39027] Decorrelating experience for 64 frames...
[2026-01-17 17:15:44,126][38995] Decorrelating experience for 32 frames...
[2026-01-17 17:15:44,159][39510] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,205][40217] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,257][38994] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,283][40170] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,344][38998] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,367][39653] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,371][39416] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,418][39415] Decorrelating experience for 192 frames...
[2026-01-17 17:15:44,431][40335] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,432][39029] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,481][40228] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,534][39510] Decorrelating experience for 192 frames...
[2026-01-17 17:15:44,598][39027] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,621][39999] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,621][39416] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,623][38994] Decorrelating experience for 192 frames...
[2026-01-17 17:15:44,662][39024] Decorrelating experience for 192 frames...
[2026-01-17 17:15:44,715][40170] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,823][39974] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,832][40228] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,834][40311] Decorrelating experience for 96 frames...
[2026-01-17 17:15:44,850][39026] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,881][39416] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,909][39629] Decorrelating experience for 128 frames...
[2026-01-17 17:15:44,936][40052] Decorrelating experience for 160 frames...
[2026-01-17 17:15:44,957][38994] Decorrelating experience for 224 frames...
[2026-01-17 17:15:45,064][39415] Decorrelating experience for 224 frames...
[2026-01-17 17:15:45,069][38999] Decorrelating experience for 32 frames...
[2026-01-17 17:15:45,084][39936] Decorrelating experience for 128 frames...
[2026-01-17 17:15:45,102][39030] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,122][40311] Decorrelating experience for 128 frames...
[2026-01-17 17:15:45,124][40170] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,141][38992] Decorrelating experience for 64 frames...
[2026-01-17 17:15:45,156][40335] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,293][38997] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,305][39024] Decorrelating experience for 224 frames...
[2026-01-17 17:15:45,336][39027] Decorrelating experience for 128 frames...
[2026-01-17 17:15:45,352][39936] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,371][39023] Decorrelating experience for 96 frames...
[2026-01-17 17:15:45,392][40311] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,395][39029] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,398][38992] Decorrelating experience for 96 frames...
[2026-01-17 17:15:45,493][39795] Decorrelating experience for 224 frames...
[2026-01-17 17:15:45,569][38997] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,584][39030] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,593][39415] Decorrelating experience for 256 frames...
[2026-01-17 17:15:45,595][39796] Decorrelating experience for 128 frames...
[2026-01-17 17:15:45,666][38990] Decorrelating experience for 96 frames...
[2026-01-17 17:15:45,700][38994] Decorrelating experience for 256 frames...
[2026-01-17 17:15:45,712][39416] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,750][39029] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,812][39024] Decorrelating experience for 256 frames...
[2026-01-17 17:15:45,828][39026] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,831][39023] Decorrelating experience for 128 frames...
[2026-01-17 17:15:45,842][39027] Decorrelating experience for 160 frames...
[2026-01-17 17:15:45,932][39936] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,962][38997] Decorrelating experience for 224 frames...
[2026-01-17 17:15:45,964][40335] Decorrelating experience for 192 frames...
[2026-01-17 17:15:45,968][39030] Decorrelating experience for 224 frames...
[2026-01-17 17:15:46,051][39795] Decorrelating experience for 256 frames...
[2026-01-17 17:15:46,056][39544] Decorrelating experience for 160 frames...
[2026-01-17 17:15:46,164][38995] Decorrelating experience for 64 frames...
[2026-01-17 17:15:46,170][39999] Decorrelating experience for 128 frames...
[2026-01-17 17:15:46,195][39023] Decorrelating experience for 160 frames...
[2026-01-17 17:15:46,223][40170] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,228][39024] Decorrelating experience for 288 frames...
[2026-01-17 17:15:46,238][39027] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,275][39510] Decorrelating experience for 224 frames...
[2026-01-17 17:15:46,291][40217] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,432][39795] Decorrelating experience for 288 frames...
[2026-01-17 17:15:46,434][39030] Decorrelating experience for 256 frames...
[2026-01-17 17:15:46,455][38994] Decorrelating experience for 288 frames...
[2026-01-17 17:15:46,476][38995] Decorrelating experience for 96 frames...
[2026-01-17 17:15:46,539][39029] Decorrelating experience for 224 frames...
[2026-01-17 17:15:46,546][39026] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,551][39653] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,571][39999] Decorrelating experience for 160 frames...
[2026-01-17 17:15:46,588][40217] Decorrelating experience for 224 frames...
[2026-01-17 17:15:46,674][39024] Decorrelating experience for 320 frames...
[2026-01-17 17:15:46,695][39936] Decorrelating experience for 224 frames...
[2026-01-17 17:15:46,695][38997] Decorrelating experience for 256 frames...
[2026-01-17 17:15:46,771][39795] Decorrelating experience for 320 frames...
[2026-01-17 17:15:46,784][39974] Decorrelating experience for 160 frames...
[2026-01-17 17:15:46,831][38995] Decorrelating experience for 128 frames...
[2026-01-17 17:15:46,840][39510] Decorrelating experience for 256 frames...
[2026-01-17 17:15:46,881][38998] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,913][39023] Decorrelating experience for 192 frames...
[2026-01-17 17:15:46,916][39768] Decorrelating experience for 192 frames...
[2026-01-17 17:15:47,017][39024] Decorrelating experience for 352 frames...
[2026-01-17 17:15:47,035][40311] Decorrelating experience for 192 frames...
[2026-01-17 17:15:47,044][39027] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,136][39795] Decorrelating experience for 352 frames...
[2026-01-17 17:15:47,198][38999] Decorrelating experience for 64 frames...
[2026-01-17 17:15:47,199][39936] Decorrelating experience for 256 frames...
[2026-01-17 17:15:47,206][39544] Decorrelating experience for 192 frames...
[2026-01-17 17:15:47,239][40228] Decorrelating experience for 160 frames...
[2026-01-17 17:15:47,255][39653] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,273][39768] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,416][39027] Decorrelating experience for 256 frames...
[2026-01-17 17:15:47,427][39029] Decorrelating experience for 256 frames...
[2026-01-17 17:15:47,470][38997] Decorrelating experience for 288 frames...
[2026-01-17 17:15:47,472][40335] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,487][39023] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,500][40170] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,508][39026] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,548][39582] Decorrelating experience for 128 frames...
[2026-01-17 17:15:47,586][39024] Decorrelating experience for 384 frames...
[2026-01-17 17:15:47,658][40228] Decorrelating experience for 192 frames...
[2026-01-17 17:15:47,708][39629] Decorrelating experience for 160 frames...
[2026-01-17 17:15:47,719][38992] Decorrelating experience for 128 frames...
[2026-01-17 17:15:47,727][39936] Decorrelating experience for 288 frames...
[2026-01-17 17:15:47,740][38994] Decorrelating experience for 320 frames...
[2026-01-17 17:15:47,778][38998] Decorrelating experience for 224 frames...
[2026-01-17 17:15:47,817][40170] Decorrelating experience for 256 frames...
[2026-01-17 17:15:47,875][39974] Decorrelating experience for 192 frames...
[2026-01-17 17:15:47,890][39653] Decorrelating experience for 256 frames...
[2026-01-17 17:15:47,987][40311] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,001][39795] Decorrelating experience for 384 frames...
[2026-01-17 17:15:48,030][39026] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,049][39027] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,050][39030] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,081][39936] Decorrelating experience for 320 frames...
[2026-01-17 17:15:48,104][38994] Decorrelating experience for 352 frames...
[2026-01-17 17:15:48,139][40228] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,158][39768] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,215][40217] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,295][39582] Decorrelating experience for 160 frames...
[2026-01-17 17:15:48,300][39544] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,357][39629] Decorrelating experience for 192 frames...
[2026-01-17 17:15:48,408][39795] Decorrelating experience for 416 frames...
[2026-01-17 17:15:48,415][39024] Decorrelating experience for 416 frames...
[2026-01-17 17:15:48,433][38997] Decorrelating experience for 320 frames...
[2026-01-17 17:15:48,447][40052] Decorrelating experience for 192 frames...
[2026-01-17 17:15:48,457][39974] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,495][39936] Decorrelating experience for 352 frames...
[2026-01-17 17:15:48,528][39999] Decorrelating experience for 192 frames...
[2026-01-17 17:15:48,580][40228] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,635][38994] Decorrelating experience for 384 frames...
[2026-01-17 17:15:48,649][38998] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,667][39653] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,671][38996] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,702][38814] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4014080. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:15:48,705][39582] Decorrelating experience for 192 frames...
[2026-01-17 17:15:48,731][39027] Decorrelating experience for 320 frames...
[2026-01-17 17:15:48,756][40217] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,798][39629] Decorrelating experience for 224 frames...
[2026-01-17 17:15:48,849][40311] Decorrelating experience for 256 frames...
[2026-01-17 17:15:48,855][39510] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,900][39768] Decorrelating experience for 288 frames...
[2026-01-17 17:15:48,903][38992] Decorrelating experience for 160 frames...
[2026-01-17 17:15:48,958][39795] Decorrelating experience for 448 frames...
[2026-01-17 17:15:49,012][38994] Decorrelating experience for 416 frames...
[2026-01-17 17:15:49,083][39999] Decorrelating experience for 224 frames...
[2026-01-17 17:15:49,084][38997] Decorrelating experience for 352 frames...
[2026-01-17 17:15:49,096][40217] Decorrelating experience for 320 frames...
[2026-01-17 17:15:49,113][39629] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,125][39023] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,152][40228] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,165][39027] Decorrelating experience for 352 frames...
[2026-01-17 17:15:49,205][40170] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,238][39582] Decorrelating experience for 224 frames...
[2026-01-17 17:15:49,364][39510] Decorrelating experience for 320 frames...
[2026-01-17 17:15:49,364][39936] Decorrelating experience for 384 frames...
[2026-01-17 17:15:49,395][39416] Decorrelating experience for 224 frames...
[2026-01-17 17:15:49,395][40052] Decorrelating experience for 224 frames...
[2026-01-17 17:15:49,450][39023] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,454][38992] Decorrelating experience for 192 frames...
[2026-01-17 17:15:49,466][39768] Decorrelating experience for 320 frames...
[2026-01-17 17:15:49,496][39026] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,553][40217] Decorrelating experience for 352 frames...
[2026-01-17 17:15:49,635][39544] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,651][38996] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,682][40311] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,696][39795] Decorrelating experience for 480 frames...
[2026-01-17 17:15:49,733][39629] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,762][39974] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,800][39416] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,801][40052] Decorrelating experience for 256 frames...
[2026-01-17 17:15:49,870][39415] Decorrelating experience for 288 frames...
[2026-01-17 17:15:49,886][39030] Decorrelating experience for 320 frames...
[2026-01-17 17:15:49,930][39936] Decorrelating experience for 416 frames...
[2026-01-17 17:15:49,962][39026] Decorrelating experience for 320 frames...
[2026-01-17 17:15:49,984][39027] Decorrelating experience for 384 frames...
[2026-01-17 17:15:50,068][38995] Decorrelating experience for 160 frames...
[2026-01-17 17:15:50,080][39510] Decorrelating experience for 352 frames...
[2026-01-17 17:15:50,111][39999] Decorrelating experience for 256 frames...
[2026-01-17 17:15:50,112][38996] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,113][39629] Decorrelating experience for 320 frames...
[2026-01-17 17:15:50,122][39582] Decorrelating experience for 256 frames...
[2026-01-17 17:15:50,291][39544] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,302][39415] Decorrelating experience for 320 frames...
[2026-01-17 17:15:50,335][40217] Decorrelating experience for 384 frames...
[2026-01-17 17:15:50,335][39030] Decorrelating experience for 352 frames...
[2026-01-17 17:15:50,388][38998] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,438][39023] Decorrelating experience for 320 frames...
[2026-01-17 17:15:50,440][39936] Decorrelating experience for 448 frames...
[2026-01-17 17:15:50,447][40228] Decorrelating experience for 320 frames...
[2026-01-17 17:15:50,458][39629] Decorrelating experience for 352 frames...
[2026-01-17 17:15:50,471][38996] Decorrelating experience for 320 frames...
[2026-01-17 17:15:50,547][39582] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,563][39795] Decorrelating experience for 512 frames...
[2026-01-17 17:15:50,608][39826] Decorrelating experience for 64 frames...
[2026-01-17 17:15:50,667][38994] Decorrelating experience for 448 frames...
[2026-01-17 17:15:50,691][39768] Decorrelating experience for 352 frames...
[2026-01-17 17:15:50,769][40217] Decorrelating experience for 416 frames...
[2026-01-17 17:15:50,816][39974] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,832][40052] Decorrelating experience for 288 frames...
[2026-01-17 17:15:50,890][39509] Decorrelating experience for 160 frames...
[2026-01-17 17:15:50,894][39023] Decorrelating experience for 352 frames...
[2026-01-17 17:15:50,897][39936] Decorrelating experience for 480 frames...
[2026-01-17 17:15:50,946][40228] Decorrelating experience for 352 frames...
[2026-01-17 17:15:51,016][39629] Decorrelating experience for 384 frames...
[2026-01-17 17:15:51,070][39826] Decorrelating experience for 96 frames...
[2026-01-17 17:15:51,108][39544] Decorrelating experience for 320 frames...
[2026-01-17 17:15:51,123][38994] Decorrelating experience for 480 frames...
[2026-01-17 17:15:51,139][40311] Decorrelating experience for 320 frames...
[2026-01-17 17:15:51,165][39028] Decorrelating experience for 64 frames...
[2026-01-17 17:15:51,167][39416] Decorrelating experience for 288 frames...
[2026-01-17 17:15:51,234][38992] Decorrelating experience for 224 frames...
[2026-01-17 17:15:51,362][39509] Decorrelating experience for 192 frames...
[2026-01-17 17:15:51,365][39027] Decorrelating experience for 416 frames...
[2026-01-17 17:15:51,410][39030] Decorrelating experience for 384 frames...
[2026-01-17 17:15:51,413][39974] Decorrelating experience for 320 frames...
[2026-01-17 17:15:51,413][38996] Decorrelating experience for 352 frames...
[2026-01-17 17:15:51,415][40217] Decorrelating experience for 448 frames...
[2026-01-17 17:15:51,416][39936] Decorrelating experience for 512 frames...
[2026-01-17 17:15:51,453][39028] Decorrelating experience for 96 frames...
[2026-01-17 17:15:51,505][39415] Decorrelating experience for 352 frames...
[2026-01-17 17:15:51,632][38999] Decorrelating experience for 96 frames...
[2026-01-17 17:15:51,649][39629] Decorrelating experience for 416 frames...
[2026-01-17 17:15:51,652][40228] Decorrelating experience for 384 frames...
[2026-01-17 17:15:51,666][39826] Decorrelating experience for 128 frames...
[2026-01-17 17:15:51,693][38994] Decorrelating experience for 512 frames...
[2026-01-17 17:15:51,703][40311] Decorrelating experience for 352 frames...
[2026-01-17 17:15:51,720][39768] Decorrelating experience for 384 frames...
[2026-01-17 17:15:51,763][39509] Decorrelating experience for 224 frames...
[2026-01-17 17:15:51,870][39028] Decorrelating experience for 128 frames...
[2026-01-17 17:15:51,934][38999] Decorrelating experience for 128 frames...
[2026-01-17 17:15:51,942][40217] Decorrelating experience for 480 frames...
[2026-01-17 17:15:51,954][38995] Decorrelating experience for 192 frames...
[2026-01-17 17:15:51,995][39027] Decorrelating experience for 448 frames...
[2026-01-17 17:15:51,998][39936] Decorrelating experience for 544 frames...
[2026-01-17 17:15:51,999][39795] Decorrelating experience for 544 frames...
[2026-01-17 17:15:52,077][39030] Decorrelating experience for 416 frames...
[2026-01-17 17:15:52,128][39415] Decorrelating experience for 384 frames...
[2026-01-17 17:15:52,226][38998] Decorrelating experience for 320 frames...
[2026-01-17 17:15:52,236][39999] Decorrelating experience for 288 frames...
[2026-01-17 17:15:52,248][39544] Decorrelating experience for 352 frames...
[2026-01-17 17:15:52,249][39768] Decorrelating experience for 416 frames...
[2026-01-17 17:15:52,260][39974] Decorrelating experience for 352 frames...
[2026-01-17 17:15:52,336][39023] Decorrelating experience for 384 frames...
[2026-01-17 17:15:52,400][40311] Decorrelating experience for 384 frames...
[2026-01-17 17:15:52,429][39027] Decorrelating experience for 480 frames...
[2026-01-17 17:15:52,467][39826] Decorrelating experience for 160 frames...
[2026-01-17 17:15:52,467][40217] Decorrelating experience for 512 frames...
[2026-01-17 17:15:52,499][39415] Decorrelating experience for 416 frames...
[2026-01-17 17:15:52,534][39028] Decorrelating experience for 160 frames...
[2026-01-17 17:15:52,585][38999] Decorrelating experience for 160 frames...
[2026-01-17 17:15:52,634][39653] Decorrelating experience for 320 frames...
[2026-01-17 17:15:52,771][40052] Decorrelating experience for 320 frames...
[2026-01-17 17:15:52,834][39030] Decorrelating experience for 448 frames...
[2026-01-17 17:15:52,864][39999] Decorrelating experience for 320 frames...
[2026-01-17 17:15:52,865][39023] Decorrelating experience for 416 frames...
[2026-01-17 17:15:52,872][40228] Decorrelating experience for 416 frames...
[2026-01-17 17:15:52,888][38998] Decorrelating experience for 352 frames...
[2026-01-17 17:15:52,923][38814] Heartbeat connected on Batcher_0
[2026-01-17 17:15:52,926][38814] Heartbeat connected on LearnerWorker_p0
[2026-01-17 17:15:52,947][39544] Decorrelating experience for 384 frames...
[2026-01-17 17:15:52,950][38999] Decorrelating experience for 192 frames...
[2026-01-17 17:15:52,970][38814] Heartbeat connected on InferenceWorker_p0-w0
[2026-01-17 17:15:53,033][38996] Decorrelating experience for 384 frames...
[2026-01-17 17:15:53,062][39028] Decorrelating experience for 192 frames...
[2026-01-17 17:15:53,082][39510] Decorrelating experience for 384 frames...
[2026-01-17 17:15:53,127][39026] Decorrelating experience for 352 frames...
[2026-01-17 17:15:53,148][39629] Decorrelating experience for 448 frames...
[2026-01-17 17:15:53,150][39826] Decorrelating experience for 192 frames...
[2026-01-17 17:15:53,190][40217] Decorrelating experience for 544 frames...
[2026-01-17 17:15:53,190][39415] Decorrelating experience for 448 frames...
[2026-01-17 17:15:53,285][38990] Decorrelating experience for 128 frames...
[2026-01-17 17:15:53,310][40170] Decorrelating experience for 320 frames...
[2026-01-17 17:15:53,322][39795] Decorrelating experience for 576 frames...
[2026-01-17 17:15:53,419][38999] Decorrelating experience for 224 frames...
[2026-01-17 17:15:53,420][39023] Decorrelating experience for 448 frames...
[2026-01-17 17:15:53,424][39030] Decorrelating experience for 480 frames...
[2026-01-17 17:15:53,456][38995] Decorrelating experience for 224 frames...
[2026-01-17 17:15:53,547][38998] Decorrelating experience for 384 frames...
[2026-01-17 17:15:53,574][39796] Decorrelating experience for 160 frames...
[2026-01-17 17:15:53,585][40311] Decorrelating experience for 416 frames...
[2026-01-17 17:15:53,611][39027] Decorrelating experience for 512 frames...
[2026-01-17 17:15:53,639][39999] Decorrelating experience for 352 frames...
[2026-01-17 17:15:53,697][40228] Decorrelating experience for 448 frames...
[2026-01-17 17:15:53,702][38814] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4014080. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:15:53,716][39974] Decorrelating experience for 384 frames...
[2026-01-17 17:15:53,716][39415] Decorrelating experience for 480 frames...
[2026-01-17 17:15:53,730][38990] Decorrelating experience for 160 frames...
[2026-01-17 17:15:53,792][38992] Decorrelating experience for 256 frames...
[2026-01-17 17:15:53,801][39629] Decorrelating experience for 480 frames...
[2026-01-17 17:15:53,861][38995] Decorrelating experience for 256 frames...
[2026-01-17 17:15:53,908][38994] Decorrelating experience for 544 frames...
[2026-01-17 17:15:53,921][39653] Decorrelating experience for 352 frames...
[2026-01-17 17:15:53,949][39510] Decorrelating experience for 416 frames...
[2026-01-17 17:15:53,974][39826] Decorrelating experience for 224 frames...
[2026-01-17 17:15:54,029][40170] Decorrelating experience for 352 frames...
[2026-01-17 17:15:54,066][39544] Decorrelating experience for 416 frames...
[2026-01-17 17:15:54,103][38990] Decorrelating experience for 192 frames...
[2026-01-17 17:15:54,110][38999] Decorrelating experience for 256 frames...
[2026-01-17 17:15:54,111][39023] Decorrelating experience for 480 frames...
[2026-01-17 17:15:54,126][39026] Decorrelating experience for 384 frames...
[2026-01-17 17:15:54,147][38997] Decorrelating experience for 384 frames...
[2026-01-17 17:15:54,216][39027] Decorrelating experience for 544 frames...
[2026-01-17 17:15:54,236][39795] Decorrelating experience for 608 frames...
[2026-01-17 17:15:54,295][38996] Decorrelating experience for 416 frames...
[2026-01-17 17:15:54,325][38995] Decorrelating experience for 288 frames...
[2026-01-17 17:15:54,344][40335] Decorrelating experience for 256 frames...
[2026-01-17 17:15:54,404][39509] Decorrelating experience for 256 frames...
[2026-01-17 17:15:54,406][39974] Decorrelating experience for 416 frames...
[2026-01-17 17:15:54,427][38992] Decorrelating experience for 288 frames...
[2026-01-17 17:15:54,474][38998] Decorrelating experience for 416 frames...
[2026-01-17 17:15:54,494][38994] Decorrelating experience for 576 frames...
[2026-01-17 17:15:54,506][39026] Decorrelating experience for 416 frames...
[2026-01-17 17:15:54,562][40217] Decorrelating experience for 576 frames...
[2026-01-17 17:15:54,605][39028] Decorrelating experience for 224 frames...
[2026-01-17 17:15:54,628][39510] Decorrelating experience for 448 frames...
[2026-01-17 17:15:54,635][39629] Decorrelating experience for 512 frames...
[2026-01-17 17:15:54,653][39999] Decorrelating experience for 384 frames...
[2026-01-17 17:15:54,689][40335] Decorrelating experience for 288 frames...
[2026-01-17 17:15:54,710][39544] Decorrelating experience for 448 frames...
[2026-01-17 17:15:54,803][40170] Decorrelating experience for 384 frames...
[2026-01-17 17:15:54,803][39027] Decorrelating experience for 576 frames...
[2026-01-17 17:15:54,809][38995] Decorrelating experience for 320 frames...
[2026-01-17 17:15:54,837][39796] Decorrelating experience for 192 frames...
[2026-01-17 17:15:54,897][39509] Decorrelating experience for 288 frames...
[2026-01-17 17:15:54,924][39826] Decorrelating experience for 256 frames...
[2026-01-17 17:15:54,960][39024] Decorrelating experience for 448 frames...
[2026-01-17 17:15:55,058][39510] Decorrelating experience for 480 frames...
[2026-01-17 17:15:55,064][39023] Decorrelating experience for 512 frames...
[2026-01-17 17:15:55,064][40335] Decorrelating experience for 320 frames...
[2026-01-17 17:15:55,081][39028] Decorrelating experience for 256 frames...
[2026-01-17 17:15:55,082][40217] Decorrelating experience for 608 frames...
[2026-01-17 17:15:55,125][40228] Decorrelating experience for 480 frames...
[2026-01-17 17:15:55,193][38997] Decorrelating experience for 416 frames...
[2026-01-17 17:15:55,193][39629] Decorrelating experience for 544 frames...
[2026-01-17 17:15:55,203][39029] Decorrelating experience for 288 frames...
[2026-01-17 17:15:55,341][39796] Decorrelating experience for 224 frames...
[2026-01-17 17:15:55,346][38990] Decorrelating experience for 224 frames...
[2026-01-17 17:15:55,351][38992] Decorrelating experience for 320 frames...
[2026-01-17 17:15:55,374][39936] Decorrelating experience for 576 frames...
[2026-01-17 17:15:55,397][39509] Decorrelating experience for 320 frames...
[2026-01-17 17:15:55,438][38998] Decorrelating experience for 448 frames...
[2026-01-17 17:15:55,448][40311] Decorrelating experience for 448 frames...
[2026-01-17 17:15:55,502][39027] Decorrelating experience for 608 frames...
[2026-01-17 17:15:55,520][39795] Decorrelating experience for 640 frames...
[2026-01-17 17:15:55,550][39544] Decorrelating experience for 480 frames...
[2026-01-17 17:15:55,578][38995] Decorrelating experience for 352 frames...
[2026-01-17 17:15:55,597][39826] Decorrelating experience for 288 frames...
[2026-01-17 17:15:55,637][39768] Decorrelating experience for 448 frames...
[2026-01-17 17:15:55,668][39028] Decorrelating experience for 288 frames...
[2026-01-17 17:15:55,668][39023] Decorrelating experience for 544 frames...
[2026-01-17 17:15:55,816][38990] Decorrelating experience for 256 frames...
[2026-01-17 17:15:55,836][39510] Decorrelating experience for 512 frames...
[2026-01-17 17:15:55,837][39629] Decorrelating experience for 576 frames...
[2026-01-17 17:15:55,866][39653] Decorrelating experience for 384 frames...
[2026-01-17 17:15:55,871][39030] Decorrelating experience for 512 frames...
[2026-01-17 17:15:55,902][38996] Decorrelating experience for 448 frames...
[2026-01-17 17:15:55,919][38999] Decorrelating experience for 288 frames...
[2026-01-17 17:15:55,986][40335] Decorrelating experience for 352 frames...
[2026-01-17 17:15:56,087][38997] Decorrelating experience for 448 frames...
[2026-01-17 17:15:56,148][38990] Decorrelating experience for 288 frames...
[2026-01-17 17:15:56,149][40052] Decorrelating experience for 352 frames...
[2026-01-17 17:15:56,165][39416] Decorrelating experience for 320 frames...
[2026-01-17 17:15:56,221][39796] Decorrelating experience for 256 frames...
[2026-01-17 17:15:56,279][39544] Decorrelating experience for 512 frames...
[2026-01-17 17:15:56,349][40228] Decorrelating experience for 512 frames...
[2026-01-17 17:15:56,352][38999] Decorrelating experience for 320 frames...
[2026-01-17 17:15:56,376][39510] Decorrelating experience for 544 frames...
[2026-01-17 17:15:56,409][39629] Decorrelating experience for 608 frames...
[2026-01-17 17:15:56,411][40170] Decorrelating experience for 416 frames...
[2026-01-17 17:15:56,411][39030] Decorrelating experience for 544 frames...
[2026-01-17 17:15:56,423][40311] Decorrelating experience for 480 frames...
[2026-01-17 17:15:56,441][39027] Decorrelating experience for 640 frames...
[2026-01-17 17:15:56,467][39826] Decorrelating experience for 320 frames...
[2026-01-17 17:15:56,553][38997] Decorrelating experience for 480 frames...
[2026-01-17 17:15:56,663][40335] Decorrelating experience for 384 frames...
[2026-01-17 17:15:56,672][39023] Decorrelating experience for 576 frames...
[2026-01-17 17:15:56,693][38990] Decorrelating experience for 320 frames...
[2026-01-17 17:15:56,725][39026] Decorrelating experience for 448 frames...
[2026-01-17 17:15:56,744][39416] Decorrelating experience for 352 frames...
[2026-01-17 17:15:56,778][39768] Decorrelating experience for 480 frames...
[2026-01-17 17:15:56,847][39582] Decorrelating experience for 320 frames...
[2026-01-17 17:15:56,899][38999] Decorrelating experience for 352 frames...
[2026-01-17 17:15:56,929][39826] Decorrelating experience for 352 frames...
[2026-01-17 17:15:56,940][39796] Decorrelating experience for 288 frames...
[2026-01-17 17:15:56,958][39999] Decorrelating experience for 416 frames...
[2026-01-17 17:15:56,975][39510] Decorrelating experience for 576 frames...
[2026-01-17 17:15:57,019][40217] Decorrelating experience for 640 frames...
[2026-01-17 17:15:57,023][39028] Decorrelating experience for 320 frames...
[2026-01-17 17:15:57,195][40052] Decorrelating experience for 384 frames...
[2026-01-17 17:15:57,232][39029] Decorrelating experience for 320 frames...
[2026-01-17 17:15:57,234][39026] Decorrelating experience for 480 frames...
[2026-01-17 17:15:57,234][40311] Decorrelating experience for 512 frames...
[2026-01-17 17:15:57,237][39027] Decorrelating experience for 672 frames...
[2026-01-17 17:15:57,265][39023] Decorrelating experience for 608 frames...
[2026-01-17 17:15:57,319][38994] Decorrelating experience for 608 frames...
[2026-01-17 17:15:57,399][39030] Decorrelating experience for 576 frames...
[2026-01-17 17:15:57,428][39510] Decorrelating experience for 608 frames...
[2026-01-17 17:15:57,496][38999] Decorrelating experience for 384 frames...
[2026-01-17 17:15:57,499][38990] Decorrelating experience for 352 frames...
[2026-01-17 17:15:57,522][39509] Decorrelating experience for 352 frames...
[2026-01-17 17:15:57,544][39653] Decorrelating experience for 416 frames...
[2026-01-17 17:15:57,545][39582] Decorrelating experience for 352 frames...
[2026-01-17 17:15:57,560][39826] Decorrelating experience for 384 frames...
[2026-01-17 17:15:57,649][39974] Decorrelating experience for 448 frames...
[2026-01-17 17:15:57,705][39999] Decorrelating experience for 448 frames...
[2026-01-17 17:15:57,707][39629] Decorrelating experience for 640 frames...
[2026-01-17 17:15:57,707][39028] Decorrelating experience for 352 frames...
[2026-01-17 17:15:57,787][39415] Decorrelating experience for 512 frames...
[2026-01-17 17:15:57,790][39796] Decorrelating experience for 320 frames...
[2026-01-17 17:15:57,812][40228] Decorrelating experience for 544 frames...
[2026-01-17 17:15:57,882][38997] Decorrelating experience for 512 frames...
[2026-01-17 17:15:57,972][39029] Decorrelating experience for 352 frames...
[2026-01-17 17:15:57,988][40052] Decorrelating experience for 416 frames...
[2026-01-17 17:15:58,027][39416] Decorrelating experience for 384 frames...
[2026-01-17 17:15:58,071][40217] Decorrelating experience for 672 frames...
[2026-01-17 17:15:58,091][39544] Decorrelating experience for 544 frames...
[2026-01-17 17:15:58,092][38998] Decorrelating experience for 480 frames...
[2026-01-17 17:15:58,134][39999] Decorrelating experience for 480 frames...
[2026-01-17 17:15:58,202][38994] Decorrelating experience for 640 frames...
[2026-01-17 17:15:58,214][38996] Decorrelating experience for 480 frames...
[2026-01-17 17:15:58,303][38992] Decorrelating experience for 352 frames...
[2026-01-17 17:15:58,317][38999] Decorrelating experience for 416 frames...
[2026-01-17 17:15:58,336][39826] Decorrelating experience for 416 frames...
[2026-01-17 17:15:58,356][39415] Decorrelating experience for 544 frames...
[2026-01-17 17:15:58,406][40335] Decorrelating experience for 416 frames...
[2026-01-17 17:15:58,498][39796] Decorrelating experience for 352 frames...
[2026-01-17 17:15:58,506][39023] Decorrelating experience for 640 frames...
[2026-01-17 17:15:58,507][39026] Decorrelating experience for 512 frames...
[2026-01-17 17:15:58,508][39936] Decorrelating experience for 608 frames...
[2026-01-17 17:15:58,554][40311] Decorrelating experience for 544 frames...
[2026-01-17 17:15:58,600][39510] Decorrelating experience for 640 frames...
[2026-01-17 17:15:58,628][39582] Decorrelating experience for 384 frames...
[2026-01-17 17:15:58,661][38996] Decorrelating experience for 512 frames...
[2026-01-17 17:15:58,697][40170] Decorrelating experience for 448 frames...
[2026-01-17 17:15:58,702][38814] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4014080. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:15:58,835][39795] Decorrelating experience for 672 frames...
[2026-01-17 17:15:58,838][38990] Decorrelating experience for 384 frames...
[2026-01-17 17:15:58,839][38998] Decorrelating experience for 512 frames...
[2026-01-17 17:15:58,868][39826] Decorrelating experience for 448 frames...
[2026-01-17 17:15:58,868][39024] Decorrelating experience for 480 frames...
[2026-01-17 17:15:58,870][39027] Decorrelating experience for 704 frames...
[2026-01-17 17:15:58,930][39544] Decorrelating experience for 576 frames...
[2026-01-17 17:15:59,030][38999] Decorrelating experience for 448 frames...
[2026-01-17 17:15:59,031][39026] Decorrelating experience for 544 frames...
[2026-01-17 17:15:59,076][39999] Decorrelating experience for 512 frames...
[2026-01-17 17:15:59,081][39768] Decorrelating experience for 512 frames...
[2026-01-17 17:15:59,142][39796] Decorrelating experience for 384 frames...
[2026-01-17 17:15:59,182][38996] Decorrelating experience for 544 frames...
[2026-01-17 17:15:59,182][40217] Decorrelating experience for 704 frames...
[2026-01-17 17:15:59,182][40311] Decorrelating experience for 576 frames...
[2026-01-17 17:15:59,224][39029] Decorrelating experience for 384 frames...
[2026-01-17 17:15:59,242][40052] Decorrelating experience for 448 frames...
[2026-01-17 17:15:59,267][40170] Decorrelating experience for 480 frames...
[2026-01-17 17:15:59,353][40335] Decorrelating experience for 448 frames...
[2026-01-17 17:15:59,381][39023] Decorrelating experience for 672 frames...
[2026-01-17 17:15:59,428][39028] Decorrelating experience for 384 frames...
[2026-01-17 17:15:59,452][39582] Decorrelating experience for 416 frames...
[2026-01-17 17:15:59,502][38990] Decorrelating experience for 416 frames...
[2026-01-17 17:15:59,502][39024] Decorrelating experience for 512 frames...
[2026-01-17 17:15:59,518][39544] Decorrelating experience for 608 frames...
[2026-01-17 17:15:59,544][38992] Decorrelating experience for 384 frames...
[2026-01-17 17:15:59,618][38994] Decorrelating experience for 672 frames...
[2026-01-17 17:15:59,638][39629] Decorrelating experience for 672 frames...
[2026-01-17 17:15:59,667][39030] Decorrelating experience for 608 frames...
[2026-01-17 17:15:59,693][39796] Decorrelating experience for 416 frames...
[2026-01-17 17:15:59,693][38996] Decorrelating experience for 576 frames...
[2026-01-17 17:15:59,777][40052] Decorrelating experience for 480 frames...
[2026-01-17 17:15:59,809][39974] Decorrelating experience for 480 frames...
[2026-01-17 17:15:59,826][39768] Decorrelating experience for 544 frames...
[2026-01-17 17:15:59,829][39936] Decorrelating experience for 640 frames...
[2026-01-17 17:15:59,924][39029] Decorrelating experience for 416 frames...
[2026-01-17 17:15:59,939][38998] Decorrelating experience for 544 frames...
[2026-01-17 17:15:59,994][39416] Decorrelating experience for 416 frames...
[2026-01-17 17:16:00,069][39028] Decorrelating experience for 416 frames...
[2026-01-17 17:16:00,079][40335] Decorrelating experience for 480 frames...
[2026-01-17 17:16:00,095][39582] Decorrelating experience for 448 frames...
[2026-01-17 17:16:00,111][39795] Decorrelating experience for 704 frames...
[2026-01-17 17:16:00,151][39023] Decorrelating experience for 704 frames...
[2026-01-17 17:16:00,152][39653] Decorrelating experience for 448 frames...
[2026-01-17 17:16:00,227][39509] Decorrelating experience for 384 frames...
[2026-01-17 17:16:00,274][38996] Decorrelating experience for 608 frames...
[2026-01-17 17:16:00,307][39544] Decorrelating experience for 640 frames...
[2026-01-17 17:16:00,422][39030] Decorrelating experience for 640 frames...
[2026-01-17 17:16:00,423][39629] Decorrelating experience for 704 frames...
[2026-01-17 17:16:00,424][39510] Decorrelating experience for 672 frames...
[2026-01-17 17:16:00,424][39415] Decorrelating experience for 576 frames...
[2026-01-17 17:16:00,424][38992] Decorrelating experience for 416 frames...
[2026-01-17 17:16:00,426][40311] Decorrelating experience for 608 frames...
[2026-01-17 17:16:00,546][39974] Decorrelating experience for 512 frames...
[2026-01-17 17:16:00,568][39029] Decorrelating experience for 448 frames...
[2026-01-17 17:16:00,644][39509] Decorrelating experience for 416 frames...
[2026-01-17 17:16:00,644][40217] Decorrelating experience for 736 frames...
[2026-01-17 17:16:00,703][38999] Decorrelating experience for 480 frames...
[2026-01-17 17:16:00,719][38994] Decorrelating experience for 704 frames...
[2026-01-17 17:16:00,749][40335] Decorrelating experience for 512 frames...
[2026-01-17 17:16:00,774][39026] Decorrelating experience for 576 frames...
[2026-01-17 17:16:00,833][38992] Decorrelating experience for 448 frames...
[2026-01-17 17:16:00,854][39826] Decorrelating experience for 480 frames...
[2026-01-17 17:16:00,870][39999] Decorrelating experience for 544 frames...
[2026-01-17 17:16:00,916][38998] Decorrelating experience for 576 frames...
[2026-01-17 17:16:00,968][39653] Decorrelating experience for 480 frames...
[2026-01-17 17:16:00,983][39027] Decorrelating experience for 736 frames...
[2026-01-17 17:16:00,989][38814] Heartbeat connected on RolloutWorker_w32
[2026-01-17 17:16:00,992][39023] Decorrelating experience for 736 frames...
[2026-01-17 17:16:01,044][40052] Decorrelating experience for 512 frames...
[2026-01-17 17:16:01,102][39629] Decorrelating experience for 736 frames...
[2026-01-17 17:16:01,110][40311] Decorrelating experience for 640 frames...
[2026-01-17 17:16:01,159][38997] Decorrelating experience for 544 frames...
[2026-01-17 17:16:01,169][40170] Decorrelating experience for 512 frames...
[2026-01-17 17:16:01,227][39030] Decorrelating experience for 672 frames...
[2026-01-17 17:16:01,288][39416] Decorrelating experience for 448 frames...
[2026-01-17 17:16:01,331][39510] Decorrelating experience for 704 frames...
[2026-01-17 17:16:01,365][38814] Heartbeat connected on RolloutWorker_w12
[2026-01-17 17:16:01,381][39415] Decorrelating experience for 608 frames...
[2026-01-17 17:16:01,382][38992] Decorrelating experience for 480 frames...
[2026-01-17 17:16:01,412][39795] Decorrelating experience for 736 frames...
[2026-01-17 17:16:01,440][38814] Heartbeat connected on RolloutWorker_w10
[2026-01-17 17:16:01,479][38814] Heartbeat connected on RolloutWorker_w21
[2026-01-17 17:16:01,506][39974] Decorrelating experience for 544 frames...
[2026-01-17 17:16:01,508][39509] Decorrelating experience for 448 frames...
[2026-01-17 17:16:01,512][39582] Decorrelating experience for 480 frames...
[2026-01-17 17:16:01,608][40228] Decorrelating experience for 576 frames...
[2026-01-17 17:16:01,632][39028] Decorrelating experience for 448 frames...
[2026-01-17 17:16:01,679][39999] Decorrelating experience for 576 frames...
[2026-01-17 17:16:01,681][39826] Decorrelating experience for 512 frames...
[2026-01-17 17:16:01,771][38814] Heartbeat connected on RolloutWorker_w24
[2026-01-17 17:16:01,808][38997] Decorrelating experience for 576 frames...
[2026-01-17 17:16:01,846][39026] Decorrelating experience for 608 frames...
[2026-01-17 17:16:01,848][39544] Decorrelating experience for 672 frames...
[2026-01-17 17:16:01,876][39029] Decorrelating experience for 480 frames...
[2026-01-17 17:16:01,901][40335] Decorrelating experience for 544 frames...
[2026-01-17 17:16:01,903][40170] Decorrelating experience for 544 frames...
[2026-01-17 17:16:01,941][39415] Decorrelating experience for 640 frames...
[2026-01-17 17:16:01,941][38999] Decorrelating experience for 512 frames...
[2026-01-17 17:16:02,013][39416] Decorrelating experience for 480 frames...
[2026-01-17 17:16:02,140][38995] Decorrelating experience for 384 frames...
[2026-01-17 17:16:02,194][39028] Decorrelating experience for 480 frames...
[2026-01-17 17:16:02,194][38992] Decorrelating experience for 512 frames...
[2026-01-17 17:16:02,240][39030] Decorrelating experience for 704 frames...
[2026-01-17 17:16:02,240][39974] Decorrelating experience for 576 frames...
[2026-01-17 17:16:02,296][39024] Decorrelating experience for 544 frames...
[2026-01-17 17:16:02,297][39509] Decorrelating experience for 480 frames...
[2026-01-17 17:16:02,459][39653] Decorrelating experience for 512 frames...
[2026-01-17 17:16:02,507][39510] Decorrelating experience for 736 frames...
[2026-01-17 17:16:02,555][39826] Decorrelating experience for 544 frames...
[2026-01-17 17:16:02,559][39029] Decorrelating experience for 512 frames...
[2026-01-17 17:16:02,561][39936] Decorrelating experience for 672 frames...
[2026-01-17 17:16:02,609][38997] Decorrelating experience for 608 frames...
[2026-01-17 17:16:02,650][38936] Signal inference workers to stop experience collection...
[2026-01-17 17:16:02,704][38991] InferenceWorker_p0-w0: stopping experience collection
[2026-01-17 17:16:02,714][38999] Decorrelating experience for 544 frames...
[2026-01-17 17:16:02,743][39974] Decorrelating experience for 608 frames...
[2026-01-17 17:16:02,829][40052] Decorrelating experience for 544 frames...
[2026-01-17 17:16:02,832][39999] Decorrelating experience for 608 frames...
[2026-01-17 17:16:02,893][38992] Decorrelating experience for 544 frames...
[2026-01-17 17:16:02,895][40311] Decorrelating experience for 672 frames...
[2026-01-17 17:16:02,900][40170] Decorrelating experience for 576 frames...
[2026-01-17 17:16:02,901][38814] Heartbeat connected on RolloutWorker_w18
[2026-01-17 17:16:02,986][39653] Decorrelating experience for 544 frames...
[2026-01-17 17:16:03,020][39030] Decorrelating experience for 736 frames...
[2026-01-17 17:16:03,044][39026] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,118][38997] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,149][39826] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,207][39415] Decorrelating experience for 672 frames...
[2026-01-17 17:16:03,218][38998] Decorrelating experience for 608 frames...
[2026-01-17 17:16:03,229][40228] Decorrelating experience for 608 frames...
[2026-01-17 17:16:03,236][38999] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,272][39024] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,337][38814] Heartbeat connected on RolloutWorker_w11
[2026-01-17 17:16:03,367][40052] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,391][39029] Decorrelating experience for 544 frames...
[2026-01-17 17:16:03,432][39999] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,501][39416] Decorrelating experience for 512 frames...
[2026-01-17 17:16:03,562][39582] Decorrelating experience for 512 frames...
[2026-01-17 17:16:03,618][39936] Decorrelating experience for 704 frames...
[2026-01-17 17:16:03,700][40311] Decorrelating experience for 704 frames...
[2026-01-17 17:16:03,702][38814] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 4014080. Throughput: 0: 165.9. Samples: 4128. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[2026-01-17 17:16:03,702][38814] Avg episode reward: [(0, '0.835')]
[2026-01-17 17:16:03,707][39826] Decorrelating experience for 608 frames...
[2026-01-17 17:16:03,801][39974] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,816][38996] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,819][38992] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,868][39029] Decorrelating experience for 576 frames...
[2026-01-17 17:16:03,874][39026] Decorrelating experience for 672 frames...
[2026-01-17 17:16:03,969][38936] Signal inference workers to resume experience collection...
[2026-01-17 17:16:03,970][38991] InferenceWorker_p0-w0: resuming experience collection
[2026-01-17 17:16:03,989][39999] Decorrelating experience for 672 frames...
[2026-01-17 17:16:03,992][38998] Decorrelating experience for 640 frames...
[2026-01-17 17:16:03,994][39024] Decorrelating experience for 608 frames...
[2026-01-17 17:16:04,078][38997] Decorrelating experience for 672 frames...
[2026-01-17 17:16:04,079][39768] Decorrelating experience for 576 frames...
[2026-01-17 17:16:04,106][40052] Decorrelating experience for 608 frames...
[2026-01-17 17:16:04,139][39028] Decorrelating experience for 512 frames...
[2026-01-17 17:16:04,183][40228] Decorrelating experience for 640 frames...
[2026-01-17 17:16:04,293][39416] Decorrelating experience for 544 frames...
[2026-01-17 17:16:04,346][39582] Decorrelating experience for 544 frames...
[2026-01-17 17:16:04,380][39415] Decorrelating experience for 704 frames...
[2026-01-17 17:16:04,431][38994] Decorrelating experience for 736 frames...
[2026-01-17 17:16:04,432][39653] Decorrelating experience for 576 frames...
[2026-01-17 17:16:04,444][39029] Decorrelating experience for 608 frames...
[2026-01-17 17:16:04,449][38995] Decorrelating experience for 416 frames...
[2026-01-17 17:16:04,595][39826] Decorrelating experience for 640 frames...
[2026-01-17 17:16:04,764][39999] Decorrelating experience for 704 frames...
[2026-01-17 17:16:04,767][40311] Decorrelating experience for 736 frames...
[2026-01-17 17:16:04,767][38996] Decorrelating experience for 672 frames...
[2026-01-17 17:16:04,768][39974] Decorrelating experience for 672 frames...
[2026-01-17 17:16:04,842][38814] Heartbeat connected on RolloutWorker_w2
[2026-01-17 17:16:04,844][39416] Decorrelating experience for 576 frames...
[2026-01-17 17:16:04,861][40170] Decorrelating experience for 608 frames...
[2026-01-17 17:16:04,926][39026] Decorrelating experience for 704 frames...
[2026-01-17 17:16:05,039][39653] Decorrelating experience for 608 frames...
[2026-01-17 17:16:05,098][39768] Decorrelating experience for 608 frames...
[2026-01-17 17:16:05,098][38995] Decorrelating experience for 448 frames...
[2026-01-17 17:16:05,104][40052] Decorrelating experience for 640 frames...
[2026-01-17 17:16:05,157][38990] Decorrelating experience for 448 frames...
[2026-01-17 17:16:05,175][38814] Heartbeat connected on RolloutWorker_w34
[2026-01-17 17:16:05,211][39029] Decorrelating experience for 640 frames...
[2026-01-17 17:16:05,425][38998] Decorrelating experience for 672 frames...
[2026-01-17 17:16:05,426][39509] Decorrelating experience for 512 frames...
[2026-01-17 17:16:05,427][39024] Decorrelating experience for 640 frames...
[2026-01-17 17:16:05,461][39028] Decorrelating experience for 544 frames...
[2026-01-17 17:16:05,469][39415] Decorrelating experience for 736 frames...
[2026-01-17 17:16:05,612][39974] Decorrelating experience for 704 frames...
[2026-01-17 17:16:05,649][39653] Decorrelating experience for 640 frames...
[2026-01-17 17:16:05,728][38990] Decorrelating experience for 480 frames...
[2026-01-17 17:16:05,770][39826] Decorrelating experience for 672 frames...
[2026-01-17 17:16:05,772][38999] Decorrelating experience for 608 frames...
[2026-01-17 17:16:05,775][38814] Heartbeat connected on RolloutWorker_w15
[2026-01-17 17:16:05,806][40228] Decorrelating experience for 672 frames...
[2026-01-17 17:16:05,842][39582] Decorrelating experience for 576 frames...
[2026-01-17 17:16:05,926][39509] Decorrelating experience for 544 frames...
[2026-01-17 17:16:05,934][38991] Updated weights for policy 0, policy_version 990 (0.0010)
[2026-01-17 17:16:05,958][39999] Decorrelating experience for 736 frames...
[2026-01-17 17:16:06,026][39768] Decorrelating experience for 640 frames...
[2026-01-17 17:16:06,036][38992] Decorrelating experience for 608 frames...
[2026-01-17 17:16:06,108][38997] Decorrelating experience for 704 frames...
[2026-01-17 17:16:06,108][39026] Decorrelating experience for 736 frames...
[2026-01-17 17:16:06,129][39028] Decorrelating experience for 576 frames...
[2026-01-17 17:16:06,148][38995] Decorrelating experience for 480 frames...
[2026-01-17 17:16:06,318][38990] Decorrelating experience for 512 frames...
[2026-01-17 17:16:06,339][39544] Decorrelating experience for 704 frames...
[2026-01-17 17:16:06,432][38996] Decorrelating experience for 704 frames...
[2026-01-17 17:16:06,544][38814] Heartbeat connected on RolloutWorker_w8
[2026-01-17 17:16:06,584][39024] Decorrelating experience for 672 frames...
[2026-01-17 17:16:06,601][40052] Decorrelating experience for 672 frames...
[2026-01-17 17:16:06,644][38814] Heartbeat connected on RolloutWorker_w29
[2026-01-17 17:16:06,682][39768] Decorrelating experience for 672 frames...
[2026-01-17 17:16:06,719][38995] Decorrelating experience for 512 frames...
[2026-01-17 17:16:06,721][39582] Decorrelating experience for 608 frames...
[2026-01-17 17:16:06,775][38997] Decorrelating experience for 736 frames...
[2026-01-17 17:16:06,777][40228] Decorrelating experience for 704 frames...
[2026-01-17 17:16:06,784][39653] Decorrelating experience for 672 frames...
[2026-01-17 17:16:06,811][39974] Decorrelating experience for 736 frames...
[2026-01-17 17:16:06,912][38999] Decorrelating experience for 640 frames...
[2026-01-17 17:16:06,937][38998] Decorrelating experience for 704 frames...
[2026-01-17 17:16:07,080][39028] Decorrelating experience for 608 frames...
[2026-01-17 17:16:07,115][39544] Decorrelating experience for 736 frames...
[2026-01-17 17:16:07,133][39029] Decorrelating experience for 672 frames...
[2026-01-17 17:16:07,167][38814] Heartbeat connected on RolloutWorker_w5
[2026-01-17 17:16:07,175][39509] Decorrelating experience for 576 frames...
[2026-01-17 17:16:07,215][38991] Updated weights for policy 0, policy_version 1000 (0.0009)
[2026-01-17 17:16:07,250][40335] Decorrelating experience for 576 frames...
[2026-01-17 17:16:07,350][38992] Decorrelating experience for 640 frames...
[2026-01-17 17:16:07,499][38996] Decorrelating experience for 736 frames...
[2026-01-17 17:16:07,500][39582] Decorrelating experience for 640 frames...
[2026-01-17 17:16:07,525][38814] Heartbeat connected on RolloutWorker_w19
[2026-01-17 17:16:07,586][38814] Heartbeat connected on RolloutWorker_w28
[2026-01-17 17:16:07,635][39768] Decorrelating experience for 704 frames...
[2026-01-17 17:16:07,637][40052] Decorrelating experience for 704 frames...
[2026-01-17 17:16:07,686][38999] Decorrelating experience for 672 frames...
[2026-01-17 17:16:07,722][38998] Decorrelating experience for 736 frames...
[2026-01-17 17:16:07,781][39024] Decorrelating experience for 704 frames...
[2026-01-17 17:16:07,899][38814] Heartbeat connected on RolloutWorker_w4
[2026-01-17 17:16:07,978][39653] Decorrelating experience for 704 frames...
[2026-01-17 17:16:08,048][38992] Decorrelating experience for 672 frames...
[2026-01-17 17:16:08,106][39028] Decorrelating experience for 640 frames...
[2026-01-17 17:16:08,131][38814] Heartbeat connected on RolloutWorker_w6
[2026-01-17 17:16:08,205][39582] Decorrelating experience for 672 frames...
[2026-01-17 17:16:08,215][39826] Decorrelating experience for 704 frames...
[2026-01-17 17:16:08,248][39029] Decorrelating experience for 704 frames...
[2026-01-17 17:16:08,325][40170] Decorrelating experience for 640 frames...
[2026-01-17 17:16:08,401][38991] Updated weights for policy 0, policy_version 1011 (0.0011)
[2026-01-17 17:16:08,402][38999] Decorrelating experience for 704 frames...
[2026-01-17 17:16:08,411][40228] Decorrelating experience for 736 frames...
[2026-01-17 17:16:08,528][39509] Decorrelating experience for 608 frames...
[2026-01-17 17:16:08,610][39768] Decorrelating experience for 736 frames...
[2026-01-17 17:16:08,702][38814] Fps is (10 sec: 13516.7, 60 sec: 4523.8, 300 sec: 4523.8). Total num frames: 4149248. Throughput: 0: 694.0. Samples: 20736. Policy #0 lag: (min: 0.0, avg: 4.1, max: 7.0)
[2026-01-17 17:16:08,702][38814] Avg episode reward: [(0, '5.605')]
[2026-01-17 17:16:08,727][39653] Decorrelating experience for 736 frames...
[2026-01-17 17:16:08,816][39028] Decorrelating experience for 672 frames...
[2026-01-17 17:16:08,972][38990] Decorrelating experience for 544 frames...
[2026-01-17 17:16:08,981][39582] Decorrelating experience for 704 frames...
[2026-01-17 17:16:08,983][38995] Decorrelating experience for 544 frames...
[2026-01-17 17:16:08,983][38992] Decorrelating experience for 704 frames...
[2026-01-17 17:16:09,088][40335] Decorrelating experience for 608 frames...
[2026-01-17 17:16:09,109][38999] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,224][38991] Updated weights for policy 0, policy_version 1021 (0.0010)
[2026-01-17 17:16:09,232][38814] Heartbeat connected on RolloutWorker_w22
[2026-01-17 17:16:09,364][38814] Heartbeat connected on RolloutWorker_w33
[2026-01-17 17:16:09,404][38814] Heartbeat connected on RolloutWorker_w23
[2026-01-17 17:16:09,406][40052] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,525][38814] Heartbeat connected on RolloutWorker_w7
[2026-01-17 17:16:09,560][39024] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,564][39826] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,723][38995] Decorrelating experience for 576 frames...
[2026-01-17 17:16:09,739][39796] Decorrelating experience for 448 frames...
[2026-01-17 17:16:09,740][39582] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,746][40170] Decorrelating experience for 672 frames...
[2026-01-17 17:16:09,747][39029] Decorrelating experience for 736 frames...
[2026-01-17 17:16:09,925][39028] Decorrelating experience for 704 frames...
[2026-01-17 17:16:09,975][38990] Decorrelating experience for 576 frames...
[2026-01-17 17:16:09,996][38814] Heartbeat connected on RolloutWorker_w9
[2026-01-17 17:16:10,145][38991] Updated weights for policy 0, policy_version 1031 (0.0012)
[2026-01-17 17:16:10,189][39509] Decorrelating experience for 640 frames...
[2026-01-17 17:16:10,302][38814] Heartbeat connected on RolloutWorker_w30
[2026-01-17 17:16:10,488][40335] Decorrelating experience for 640 frames...
[2026-01-17 17:16:10,503][38814] Heartbeat connected on RolloutWorker_w26
[2026-01-17 17:16:10,551][38814] Heartbeat connected on RolloutWorker_w14
[2026-01-17 17:16:10,592][38814] Heartbeat connected on RolloutWorker_w20
[2026-01-17 17:16:10,602][38990] Decorrelating experience for 608 frames...
[2026-01-17 17:16:10,648][38992] Decorrelating experience for 736 frames...
[2026-01-17 17:16:10,651][39028] Decorrelating experience for 736 frames...
[2026-01-17 17:16:10,682][39936] Decorrelating experience for 736 frames...
[2026-01-17 17:16:10,683][39796] Decorrelating experience for 480 frames...
[2026-01-17 17:16:10,887][40170] Decorrelating experience for 704 frames...
[2026-01-17 17:16:11,060][39416] Decorrelating experience for 608 frames...
[2026-01-17 17:16:11,107][38814] Heartbeat connected on RolloutWorker_w13
[2026-01-17 17:16:11,130][38991] Updated weights for policy 0, policy_version 1041 (0.0012)
[2026-01-17 17:16:11,230][38990] Decorrelating experience for 640 frames...
[2026-01-17 17:16:11,395][39509] Decorrelating experience for 672 frames...
[2026-01-17 17:16:11,493][38814] Heartbeat connected on RolloutWorker_w1
[2026-01-17 17:16:11,500][38814] Heartbeat connected on RolloutWorker_w27
[2026-01-17 17:16:11,726][39796] Decorrelating experience for 512 frames...
[2026-01-17 17:16:11,727][38995] Decorrelating experience for 608 frames...
[2026-01-17 17:16:11,935][38990] Decorrelating experience for 672 frames...
[2026-01-17 17:16:12,003][38991] Updated weights for policy 0, policy_version 1051 (0.0011)
[2026-01-17 17:16:12,143][39509] Decorrelating experience for 704 frames...
[2026-01-17 17:16:12,190][40335] Decorrelating experience for 672 frames...
[2026-01-17 17:16:12,292][40170] Decorrelating experience for 736 frames...
[2026-01-17 17:16:12,380][38995] Decorrelating experience for 640 frames...
[2026-01-17 17:16:12,624][38990] Decorrelating experience for 704 frames...
[2026-01-17 17:16:12,755][39796] Decorrelating experience for 544 frames...
[2026-01-17 17:16:12,820][38991] Updated weights for policy 0, policy_version 1063 (0.0012)
[2026-01-17 17:16:12,823][39416] Decorrelating experience for 640 frames...
[2026-01-17 17:16:12,889][39509] Decorrelating experience for 736 frames...
[2026-01-17 17:16:13,091][38995] Decorrelating experience for 672 frames...
[2026-01-17 17:16:13,136][38814] Heartbeat connected on RolloutWorker_w31
[2026-01-17 17:16:13,301][38814] Heartbeat connected on RolloutWorker_w17
[2026-01-17 17:16:13,507][40335] Decorrelating experience for 704 frames...
[2026-01-17 17:16:13,620][38991] Updated weights for policy 0, policy_version 1074 (0.0013)
[2026-01-17 17:16:13,702][38814] Fps is (10 sec: 38911.7, 60 sec: 11156.3, 300 sec: 11156.3). Total num frames: 4403200. Throughput: 0: 2665.7. Samples: 92976. Policy #0 lag: (min: 0.0, avg: 7.8, max: 17.0)
[2026-01-17 17:16:13,702][38814] Avg episode reward: [(0, '9.620')]
[2026-01-17 17:16:13,805][38995] Decorrelating experience for 704 frames...
[2026-01-17 17:16:14,075][39416] Decorrelating experience for 672 frames...
[2026-01-17 17:16:14,188][38990] Decorrelating experience for 736 frames...
[2026-01-17 17:16:14,244][39796] Decorrelating experience for 576 frames...
[2026-01-17 17:16:14,287][38991] Updated weights for policy 0, policy_version 1084 (0.0014)
[2026-01-17 17:16:14,501][38995] Decorrelating experience for 736 frames...
[2026-01-17 17:16:14,823][38814] Heartbeat connected on RolloutWorker_w0
[2026-01-17 17:16:14,884][38814] Heartbeat connected on RolloutWorker_w3
[2026-01-17 17:16:14,917][39416] Decorrelating experience for 704 frames...
[2026-01-17 17:16:14,920][38991] Updated weights for policy 0, policy_version 1094 (0.0013)
[2026-01-17 17:16:15,016][40335] Decorrelating experience for 736 frames...
[2026-01-17 17:16:15,540][39796] Decorrelating experience for 608 frames...
[2026-01-17 17:16:15,661][39416] Decorrelating experience for 736 frames...
[2026-01-17 17:16:15,720][38991] Updated weights for policy 0, policy_version 1105 (0.0011)
[2026-01-17 17:16:15,896][38814] Heartbeat connected on RolloutWorker_w35
[2026-01-17 17:16:16,089][38814] Heartbeat connected on RolloutWorker_w16
[2026-01-17 17:16:16,342][38936] Signal inference workers to stop experience collection... (50 times)
[2026-01-17 17:16:16,356][38991] Updated weights for policy 0, policy_version 1117 (0.0014)
[2026-01-17 17:16:16,355][38936] Signal inference workers to resume experience collection... (50 times)
[2026-01-17 17:16:16,401][38991] InferenceWorker_p0-w0: stopping experience collection (50 times)
[2026-01-17 17:16:16,459][38991] InferenceWorker_p0-w0: resuming experience collection (50 times)
[2026-01-17 17:16:16,577][39796] Decorrelating experience for 640 frames...
[2026-01-17 17:16:17,220][38991] Updated weights for policy 0, policy_version 1129 (0.0012)
[2026-01-17 17:16:17,802][38991] Updated weights for policy 0, policy_version 1140 (0.0015)
[2026-01-17 17:16:17,873][39796] Decorrelating experience for 672 frames...
[2026-01-17 17:16:18,408][38991] Updated weights for policy 0, policy_version 1151 (0.0011)
[2026-01-17 17:16:18,702][38814] Fps is (10 sec: 58572.8, 60 sec: 18077.1, 300 sec: 18077.1). Total num frames: 4734976. Throughput: 0: 4838.9. Samples: 192972. Policy #0 lag: (min: 2.0, avg: 12.7, max: 30.0)
[2026-01-17 17:16:18,702][38814] Avg episode reward: [(0, '14.018')]
[2026-01-17 17:16:18,995][38991] Updated weights for policy 0, policy_version 1161 (0.0013)
[2026-01-17 17:16:19,290][39796] Decorrelating experience for 704 frames...
[2026-01-17 17:16:19,505][38991] Updated weights for policy 0, policy_version 1171 (0.0012)
[2026-01-17 17:16:20,030][38991] Updated weights for policy 0, policy_version 1181 (0.0011)
[2026-01-17 17:16:20,605][38991] Updated weights for policy 0, policy_version 1191 (0.0014)
[2026-01-17 17:16:20,837][39796] Decorrelating experience for 736 frames...
[2026-01-17 17:16:21,219][38991] Updated weights for policy 0, policy_version 1201 (0.0013)
[2026-01-17 17:16:21,766][38991] Updated weights for policy 0, policy_version 1211 (0.0015)
[2026-01-17 17:16:21,788][38814] Heartbeat connected on RolloutWorker_w25
[2026-01-17 17:16:21,857][38936] Signal inference workers to stop experience collection... (100 times)
[2026-01-17 17:16:21,876][38936] Signal inference workers to resume experience collection... (100 times)
[2026-01-17 17:16:21,881][38991] InferenceWorker_p0-w0: stopping experience collection (100 times)
[2026-01-17 17:16:21,901][38991] InferenceWorker_p0-w0: resuming experience collection (100 times)
[2026-01-17 17:16:22,443][38991] Updated weights for policy 0, policy_version 1222 (0.0014)
[2026-01-17 17:16:23,131][38991] Updated weights for policy 0, policy_version 1232 (0.0012)
[2026-01-17 17:16:23,563][38991] Updated weights for policy 0, policy_version 1242 (0.0013)
[2026-01-17 17:16:23,702][38814] Fps is (10 sec: 69631.2, 60 sec: 24185.8, 300 sec: 24185.8). Total num frames: 5099520. Throughput: 0: 5544.5. Samples: 248832. Policy #0 lag: (min: 0.0, avg: 11.1, max: 28.0)
[2026-01-17 17:16:23,702][38814] Avg episode reward: [(0, '18.577')]
[2026-01-17 17:16:24,026][38991] Updated weights for policy 0, policy_version 1252 (0.0012)
[2026-01-17 17:16:24,812][38991] Updated weights for policy 0, policy_version 1262 (0.0013)
[2026-01-17 17:16:25,231][38991] Updated weights for policy 0, policy_version 1272 (0.0013)
[2026-01-17 17:16:25,751][38991] Updated weights for policy 0, policy_version 1282 (0.0011)
[2026-01-17 17:16:26,168][38936] Signal inference workers to stop experience collection... (150 times)
[2026-01-17 17:16:26,180][38936] Signal inference workers to resume experience collection... (150 times)
[2026-01-17 17:16:26,186][38991] InferenceWorker_p0-w0: stopping experience collection (150 times)
[2026-01-17 17:16:26,201][38991] InferenceWorker_p0-w0: resuming experience collection (150 times)
[2026-01-17 17:16:26,454][38991] Updated weights for policy 0, policy_version 1292 (0.0017)
[2026-01-17 17:16:26,882][38991] Updated weights for policy 0, policy_version 1302 (0.0012)
[2026-01-17 17:16:27,426][38991] Updated weights for policy 0, policy_version 1312 (0.0013)
[2026-01-17 17:16:28,141][38991] Updated weights for policy 0, policy_version 1322 (0.0013)
[2026-01-17 17:16:28,574][38991] Updated weights for policy 0, policy_version 1332 (0.0016)
[2026-01-17 17:16:28,702][38814] Fps is (10 sec: 72908.3, 60 sec: 29070.0, 300 sec: 29070.0). Total num frames: 5464064. Throughput: 0: 7940.2. Samples: 357312. Policy #0 lag: (min: 0.0, avg: 12.9, max: 29.0)
[2026-01-17 17:16:28,702][38814] Avg episode reward: [(0, '21.109')]
[2026-01-17 17:16:29,089][38991] Updated weights for policy 0, policy_version 1342 (0.0011)
[2026-01-17 17:16:29,757][38991] Updated weights for policy 0, policy_version 1352 (0.0020)
[2026-01-17 17:16:30,150][38936] Signal inference workers to stop experience collection... (200 times)
[2026-01-17 17:16:30,166][38936] Signal inference workers to resume experience collection... (200 times)
[2026-01-17 17:16:30,169][38991] InferenceWorker_p0-w0: stopping experience collection (200 times)
[2026-01-17 17:16:30,182][38991] InferenceWorker_p0-w0: resuming experience collection (200 times)
[2026-01-17 17:16:30,248][38991] Updated weights for policy 0, policy_version 1362 (0.0012)
[2026-01-17 17:16:30,806][38991] Updated weights for policy 0, policy_version 1372 (0.0019)
[2026-01-17 17:16:31,330][38991] Updated weights for policy 0, policy_version 1382 (0.0011)
[2026-01-17 17:16:31,855][38991] Updated weights for policy 0, policy_version 1392 (0.0016)
[2026-01-17 17:16:32,319][38991] Updated weights for policy 0, policy_version 1402 (0.0012)
[2026-01-17 17:16:32,985][38991] Updated weights for policy 0, policy_version 1412 (0.0012)
[2026-01-17 17:16:33,513][38991] Updated weights for policy 0, policy_version 1422 (0.0014)
[2026-01-17 17:16:33,702][38814] Fps is (10 sec: 74134.6, 60 sec: 33287.7, 300 sec: 33287.7). Total num frames: 5840896. Throughput: 0: 10426.0. Samples: 469176. Policy #0 lag: (min: 1.0, avg: 15.1, max: 31.0)
[2026-01-17 17:16:33,703][38814] Avg episode reward: [(0, '22.450')]
[2026-01-17 17:16:33,735][38936] Saving new best policy, reward=22.450!
[2026-01-17 17:16:34,108][38991] Updated weights for policy 0, policy_version 1433 (0.0011)
[2026-01-17 17:16:34,789][38991] Updated weights for policy 0, policy_version 1443 (0.0017)
[2026-01-17 17:16:35,031][38936] Signal inference workers to stop experience collection... (250 times)
[2026-01-17 17:16:35,039][38936] Signal inference workers to resume experience collection... (250 times)
[2026-01-17 17:16:35,061][38991] InferenceWorker_p0-w0: stopping experience collection (250 times)
[2026-01-17 17:16:35,061][38991] InferenceWorker_p0-w0: resuming experience collection (250 times)
[2026-01-17 17:16:35,192][38991] Updated weights for policy 0, policy_version 1453 (0.0017)
[2026-01-17 17:16:35,862][38991] Updated weights for policy 0, policy_version 1463 (0.0015)
[2026-01-17 17:16:36,340][38991] Updated weights for policy 0, policy_version 1473 (0.0014)
[2026-01-17 17:16:36,864][38991] Updated weights for policy 0, policy_version 1483 (0.0019)
[2026-01-17 17:16:37,393][38991] Updated weights for policy 0, policy_version 1493 (0.0013)
[2026-01-17 17:16:37,905][38991] Updated weights for policy 0, policy_version 1503 (0.0014)
[2026-01-17 17:16:38,560][38991] Updated weights for policy 0, policy_version 1513 (0.0011)
[2026-01-17 17:16:38,702][38814] Fps is (10 sec: 74547.0, 60 sec: 36664.8, 300 sec: 36664.8). Total num frames: 6209536. Throughput: 0: 11663.2. Samples: 524844. Policy #0 lag: (min: 0.0, avg: 15.0, max: 31.0)
[2026-01-17 17:16:38,702][38814] Avg episode reward: [(0, '22.231')]
[2026-01-17 17:16:39,103][38991] Updated weights for policy 0, policy_version 1523 (0.0014)
[2026-01-17 17:16:39,300][38936] Signal inference workers to stop experience collection... (300 times)
[2026-01-17 17:16:39,318][38936] Signal inference workers to resume experience collection... (300 times)
[2026-01-17 17:16:39,320][38991] InferenceWorker_p0-w0: stopping experience collection (300 times)
[2026-01-17 17:16:39,339][38991] InferenceWorker_p0-w0: resuming experience collection (300 times)
[2026-01-17 17:16:39,541][38991] Updated weights for policy 0, policy_version 1533 (0.0014)
[2026-01-17 17:16:40,155][38991] Updated weights for policy 0, policy_version 1543 (0.0011)
[2026-01-17 17:16:40,599][38991] Updated weights for policy 0, policy_version 1553 (0.0014)
[2026-01-17 17:16:41,248][38991] Updated weights for policy 0, policy_version 1563 (0.0014)
[2026-01-17 17:16:41,729][38991] Updated weights for policy 0, policy_version 1573 (0.0014)
[2026-01-17 17:16:42,353][38991] Updated weights for policy 0, policy_version 1583 (0.0013)
[2026-01-17 17:16:42,886][38991] Updated weights for policy 0, policy_version 1593 (0.0015)
[2026-01-17 17:16:43,228][38936] Signal inference workers to stop experience collection... (350 times)
[2026-01-17 17:16:43,231][38936] Signal inference workers to resume experience collection... (350 times)
[2026-01-17 17:16:43,259][38991] InferenceWorker_p0-w0: stopping experience collection (350 times)
[2026-01-17 17:16:43,259][38991] InferenceWorker_p0-w0: resuming experience collection (350 times)
[2026-01-17 17:16:43,276][38991] Updated weights for policy 0, policy_version 1603 (0.0014)
[2026-01-17 17:16:43,703][38814] Fps is (10 sec: 74949.0, 60 sec: 42938.5, 300 sec: 39709.6). Total num frames: 6590464. Throughput: 0: 14194.1. Samples: 638760. Policy #0 lag: (min: 0.0, avg: 15.9, max: 30.0)
[2026-01-17 17:16:43,704][38814] Avg episode reward: [(0, '23.178')]
[2026-01-17 17:16:43,704][38936] Saving new best policy, reward=23.178!
[2026-01-17 17:16:44,076][38991] Updated weights for policy 0, policy_version 1613 (0.0012)
[2026-01-17 17:16:44,494][38991] Updated weights for policy 0, policy_version 1623 (0.0015)
[2026-01-17 17:16:44,948][38991] Updated weights for policy 0, policy_version 1633 (0.0013)
[2026-01-17 17:16:45,699][38991] Updated weights for policy 0, policy_version 1643 (0.0013)
[2026-01-17 17:16:46,081][38991] Updated weights for policy 0, policy_version 1653 (0.0013)
[2026-01-17 17:16:46,603][38991] Updated weights for policy 0, policy_version 1663 (0.0013)
[2026-01-17 17:16:47,241][38991] Updated weights for policy 0, policy_version 1673 (0.0015)
[2026-01-17 17:16:47,286][38936] Signal inference workers to stop experience collection... (400 times)
[2026-01-17 17:16:47,305][38936] Signal inference workers to resume experience collection... (400 times)
[2026-01-17 17:16:47,313][38991] InferenceWorker_p0-w0: stopping experience collection (400 times)
[2026-01-17 17:16:47,333][38991] InferenceWorker_p0-w0: resuming experience collection (400 times)
[2026-01-17 17:16:47,692][38991] Updated weights for policy 0, policy_version 1683 (0.0014)
[2026-01-17 17:16:48,309][38991] Updated weights for policy 0, policy_version 1693 (0.0012)
[2026-01-17 17:16:48,702][38814] Fps is (10 sec: 76185.2, 60 sec: 49288.4, 300 sec: 42320.4). Total num frames: 6971392. Throughput: 0: 16613.5. Samples: 751740. Policy #0 lag: (min: 1.0, avg: 14.8, max: 32.0)
[2026-01-17 17:16:48,703][38814] Avg episode reward: [(0, '24.409')]
[2026-01-17 17:16:48,708][38936] Saving new best policy, reward=24.409!
[2026-01-17 17:16:48,711][38991] Updated weights for policy 0, policy_version 1703 (0.0014)
[2026-01-17 17:16:49,353][38991] Updated weights for policy 0, policy_version 1713 (0.0014)
[2026-01-17 17:16:49,851][38991] Updated weights for policy 0, policy_version 1723 (0.0017)
[2026-01-17 17:16:50,355][38991] Updated weights for policy 0, policy_version 1733 (0.0016)
[2026-01-17 17:16:51,026][38991] Updated weights for policy 0, policy_version 1743 (0.0020)
[2026-01-17 17:16:51,204][38936] Signal inference workers to stop experience collection... (450 times)
[2026-01-17 17:16:51,213][38936] Signal inference workers to resume experience collection... (450 times)
[2026-01-17 17:16:51,223][38991] InferenceWorker_p0-w0: stopping experience collection (450 times)
[2026-01-17 17:16:51,240][38991] InferenceWorker_p0-w0: resuming experience collection (450 times)
[2026-01-17 17:16:51,419][38991] Updated weights for policy 0, policy_version 1753 (0.0015)
[2026-01-17 17:16:52,089][38991] Updated weights for policy 0, policy_version 1763 (0.0014)
[2026-01-17 17:16:52,580][38991] Updated weights for policy 0, policy_version 1773 (0.0013)
[2026-01-17 17:16:53,215][38991] Updated weights for policy 0, policy_version 1783 (0.0012)
[2026-01-17 17:16:53,645][38991] Updated weights for policy 0, policy_version 1793 (0.0012)
[2026-01-17 17:16:53,702][38814] Fps is (10 sec: 75786.1, 60 sec: 55568.8, 300 sec: 44526.9). Total num frames: 7348224. Throughput: 0: 17501.8. Samples: 808320. Policy #0 lag: (min: 2.0, avg: 16.3, max: 36.0)
[2026-01-17 17:16:53,703][38814] Avg episode reward: [(0, '18.686')]
[2026-01-17 17:16:54,163][38991] Updated weights for policy 0, policy_version 1803 (0.0013)
[2026-01-17 17:16:54,852][38991] Updated weights for policy 0, policy_version 1813 (0.0013)
[2026-01-17 17:16:55,415][38991] Updated weights for policy 0, policy_version 1824 (0.0011)
[2026-01-17 17:16:56,135][38991] Updated weights for policy 0, policy_version 1834 (0.0013)
[2026-01-17 17:16:56,149][38936] Signal inference workers to stop experience collection... (500 times)
[2026-01-17 17:16:56,152][38991] InferenceWorker_p0-w0: stopping experience collection (500 times)
[2026-01-17 17:16:56,161][38936] Signal inference workers to resume experience collection... (500 times)
[2026-01-17 17:16:56,170][38991] InferenceWorker_p0-w0: resuming experience collection (500 times)
[2026-01-17 17:16:56,521][38991] Updated weights for policy 0, policy_version 1844 (0.0015)
[2026-01-17 17:16:57,211][38991] Updated weights for policy 0, policy_version 1854 (0.0014)
[2026-01-17 17:16:57,678][38991] Updated weights for policy 0, policy_version 1864 (0.0012)
[2026-01-17 17:16:58,249][38991] Updated weights for policy 0, policy_version 1874 (0.0013)
[2026-01-17 17:16:58,702][38814] Fps is (10 sec: 72907.4, 60 sec: 61439.6, 300 sec: 46149.6). Total num frames: 7700480. Throughput: 0: 18353.2. Samples: 918876. Policy #0 lag: (min: 1.0, avg: 14.9, max: 30.0)
[2026-01-17 17:16:58,703][38814] Avg episode reward: [(0, '24.074')]
[2026-01-17 17:16:58,927][38991] Updated weights for policy 0, policy_version 1884 (0.0013)
[2026-01-17 17:16:59,428][38991] Updated weights for policy 0, policy_version 1894 (0.0012)
[2026-01-17 17:16:59,829][38936] Signal inference workers to stop experience collection... (550 times)
[2026-01-17 17:16:59,845][38936] Signal inference workers to resume experience collection... (550 times)
[2026-01-17 17:16:59,864][38991] InferenceWorker_p0-w0: stopping experience collection (550 times)
[2026-01-17 17:16:59,879][38991] InferenceWorker_p0-w0: resuming experience collection (550 times)
[2026-01-17 17:16:59,974][38991] Updated weights for policy 0, policy_version 1904 (0.0012)
[2026-01-17 17:17:00,590][38991] Updated weights for policy 0, policy_version 1914 (0.0014)
[2026-01-17 17:17:01,172][38991] Updated weights for policy 0, policy_version 1924 (0.0015)
[2026-01-17 17:17:01,661][38991] Updated weights for policy 0, policy_version 1934 (0.0014)
[2026-01-17 17:17:02,212][38991] Updated weights for policy 0, policy_version 1944 (0.0013)
[2026-01-17 17:17:02,822][38991] Updated weights for policy 0, policy_version 1954 (0.0012)
[2026-01-17 17:17:03,385][38991] Updated weights for policy 0, policy_version 1964 (0.0012)
[2026-01-17 17:17:03,702][38814] Fps is (10 sec: 71270.8, 60 sec: 67447.2, 300 sec: 47677.7). Total num frames: 8060928. Throughput: 0: 18513.0. Samples: 1026060. Policy #0 lag: (min: 1.0, avg: 14.8, max: 31.0)
[2026-01-17 17:17:03,704][38814] Avg episode reward: [(0, '25.124')]
[2026-01-17 17:17:03,717][38936] Saving new best policy, reward=25.124!
[2026-01-17 17:17:04,253][38991] Updated weights for policy 0, policy_version 1976 (0.0013)
[2026-01-17 17:17:04,610][38936] Signal inference workers to stop experience collection... (600 times)
[2026-01-17 17:17:04,621][38936] Signal inference workers to resume experience collection... (600 times)
[2026-01-17 17:17:04,639][38991] InferenceWorker_p0-w0: stopping experience collection (600 times)
[2026-01-17 17:17:04,639][38991] InferenceWorker_p0-w0: resuming experience collection (600 times)
[2026-01-17 17:17:04,770][38991] Updated weights for policy 0, policy_version 1986 (0.0015)
[2026-01-17 17:17:05,283][38991] Updated weights for policy 0, policy_version 1996 (0.0012)
[2026-01-17 17:17:05,968][38991] Updated weights for policy 0, policy_version 2006 (0.0012)
[2026-01-17 17:17:06,502][38991] Updated weights for policy 0, policy_version 2016 (0.0017)
[2026-01-17 17:17:07,069][38991] Updated weights for policy 0, policy_version 2026 (0.0014)
[2026-01-17 17:17:07,559][38991] Updated weights for policy 0, policy_version 2036 (0.0014)
[2026-01-17 17:17:08,323][38991] Updated weights for policy 0, policy_version 2046 (0.0014)
[2026-01-17 17:17:08,704][38814] Fps is (10 sec: 71663.5, 60 sec: 71130.7, 300 sec: 48988.9). Total num frames: 8417280. Throughput: 0: 18426.2. Samples: 1078056. Policy #0 lag: (min: 2.0, avg: 15.2, max: 31.0)
[2026-01-17 17:17:08,705][38814] Avg episode reward: [(0, '24.453')]
[2026-01-17 17:17:08,745][38991] Updated weights for policy 0, policy_version 2056 (0.0019)
[2026-01-17 17:17:08,943][38936] Signal inference workers to stop experience collection... (650 times)
[2026-01-17 17:17:08,945][38991] InferenceWorker_p0-w0: stopping experience collection (650 times)
[2026-01-17 17:17:08,957][38936] Signal inference workers to resume experience collection... (650 times)
[2026-01-17 17:17:08,960][38991] InferenceWorker_p0-w0: resuming experience collection (650 times)
[2026-01-17 17:17:09,269][38991] Updated weights for policy 0, policy_version 2066 (0.0013)
[2026-01-17 17:17:09,931][38991] Updated weights for policy 0, policy_version 2076 (0.0012)
[2026-01-17 17:17:10,355][38991] Updated weights for policy 0, policy_version 2086 (0.0013)
[2026-01-17 17:17:10,789][38991] Updated weights for policy 0, policy_version 2096 (0.0014)
[2026-01-17 17:17:11,501][38991] Updated weights for policy 0, policy_version 2106 (0.0014)
[2026-01-17 17:17:12,014][38991] Updated weights for policy 0, policy_version 2116 (0.0011)
[2026-01-17 17:17:12,424][38991] Updated weights for policy 0, policy_version 2126 (0.0011)
[2026-01-17 17:17:13,094][38991] Updated weights for policy 0, policy_version 2136 (0.0012)
[2026-01-17 17:17:13,574][38991] Updated weights for policy 0, policy_version 2146 (0.0015)
[2026-01-17 17:17:13,650][38936] Signal inference workers to stop experience collection... (700 times)
[2026-01-17 17:17:13,651][38936] Signal inference workers to resume experience collection... (700 times)
[2026-01-17 17:17:13,678][38991] InferenceWorker_p0-w0: stopping experience collection (700 times)
[2026-01-17 17:17:13,678][38991] InferenceWorker_p0-w0: resuming experience collection (700 times)
[2026-01-17 17:17:13,702][38814] Fps is (10 sec: 74139.7, 60 sec: 73318.5, 300 sec: 50466.7). Total num frames: 8802304. Throughput: 0: 18503.0. Samples: 1189944. Policy #0 lag: (min: 2.0, avg: 15.3, max: 34.0)
[2026-01-17 17:17:13,702][38814] Avg episode reward: [(0, '22.358')]
[2026-01-17 17:17:14,059][38991] Updated weights for policy 0, policy_version 2156 (0.0011)
[2026-01-17 17:17:14,736][38991] Updated weights for policy 0, policy_version 2166 (0.0012)
[2026-01-17 17:17:15,198][38991] Updated weights for policy 0, policy_version 2176 (0.0016)
[2026-01-17 17:17:15,812][38991] Updated weights for policy 0, policy_version 2186 (0.0013)
[2026-01-17 17:17:16,309][38991] Updated weights for policy 0, policy_version 2196 (0.0014)
[2026-01-17 17:17:16,838][38991] Updated weights for policy 0, policy_version 2206 (0.0014)
[2026-01-17 17:17:17,397][38991] Updated weights for policy 0, policy_version 2216 (0.0016)
[2026-01-17 17:17:17,795][38991] Updated weights for policy 0, policy_version 2226 (0.0014)
[2026-01-17 17:17:17,887][38936] Signal inference workers to stop experience collection... (750 times)
[2026-01-17 17:17:17,888][38936] Signal inference workers to resume experience collection... (750 times)
[2026-01-17 17:17:17,919][38991] InferenceWorker_p0-w0: stopping experience collection (750 times)
[2026-01-17 17:17:17,920][38991] InferenceWorker_p0-w0: resuming experience collection (750 times)
[2026-01-17 17:17:18,498][38991] Updated weights for policy 0, policy_version 2236 (0.0014)
[2026-01-17 17:17:18,702][38814] Fps is (10 sec: 75795.5, 60 sec: 74001.0, 300 sec: 51672.1). Total num frames: 9175040. Throughput: 0: 18569.5. Samples: 1304796. Policy #0 lag: (min: 0.0, avg: 14.0, max: 33.0)
[2026-01-17 17:17:18,702][38814] Avg episode reward: [(0, '24.179')]
[2026-01-17 17:17:18,932][38991] Updated weights for policy 0, policy_version 2246 (0.0014)
[2026-01-17 17:17:19,500][38991] Updated weights for policy 0, policy_version 2256 (0.0015)
[2026-01-17 17:17:20,049][38991] Updated weights for policy 0, policy_version 2266 (0.0015)
[2026-01-17 17:17:20,481][38991] Updated weights for policy 0, policy_version 2276 (0.0012)
[2026-01-17 17:17:21,098][38991] Updated weights for policy 0, policy_version 2286 (0.0013)
[2026-01-17 17:17:21,571][38991] Updated weights for policy 0, policy_version 2296 (0.0013)
[2026-01-17 17:17:22,263][38991] Updated weights for policy 0, policy_version 2306 (0.0016)
[2026-01-17 17:17:22,279][38936] Signal inference workers to stop experience collection... (800 times)
[2026-01-17 17:17:22,289][38936] Signal inference workers to resume experience collection... (800 times)
[2026-01-17 17:17:22,296][38991] InferenceWorker_p0-w0: stopping experience collection (800 times)
[2026-01-17 17:17:22,314][38991] InferenceWorker_p0-w0: resuming experience collection (800 times)
[2026-01-17 17:17:22,658][38991] Updated weights for policy 0, policy_version 2316 (0.0014)
[2026-01-17 17:17:23,168][38991] Updated weights for policy 0, policy_version 2326 (0.0016)
[2026-01-17 17:17:23,702][38814] Fps is (10 sec: 74956.5, 60 sec: 74206.1, 300 sec: 52801.7). Total num frames: 9551872. Throughput: 0: 18607.5. Samples: 1362180. Policy #0 lag: (min: 1.0, avg: 15.3, max: 33.0)
[2026-01-17 17:17:23,702][38814] Avg episode reward: [(0, '24.798')]
[2026-01-17 17:17:23,808][38991] Updated weights for policy 0, policy_version 2336 (0.0011)
[2026-01-17 17:17:24,357][38991] Updated weights for policy 0, policy_version 2346 (0.0014)
[2026-01-17 17:17:24,974][38991] Updated weights for policy 0, policy_version 2356 (0.0012)
[2026-01-17 17:17:25,536][38991] Updated weights for policy 0, policy_version 2366 (0.0013)
[2026-01-17 17:17:25,836][38936] Signal inference workers to stop experience collection... (850 times)
[2026-01-17 17:17:25,840][38991] InferenceWorker_p0-w0: stopping experience collection (850 times)
[2026-01-17 17:17:25,852][38936] Signal inference workers to resume experience collection... (850 times)
[2026-01-17 17:17:25,854][38991] InferenceWorker_p0-w0: resuming experience collection (850 times)
[2026-01-17 17:17:25,926][38991] Updated weights for policy 0, policy_version 2376 (0.0015)
[2026-01-17 17:17:26,599][38991] Updated weights for policy 0, policy_version 2386 (0.0011)
[2026-01-17 17:17:27,152][38991] Updated weights for policy 0, policy_version 2396 (0.0014)
[2026-01-17 17:17:27,539][38991] Updated weights for policy 0, policy_version 2406 (0.0015)
[2026-01-17 17:17:28,292][38991] Updated weights for policy 0, policy_version 2416 (0.0012)
[2026-01-17 17:17:28,702][38814] Fps is (10 sec: 74956.5, 60 sec: 74342.4, 300 sec: 53791.2). Total num frames: 9924608. Throughput: 0: 18561.2. Samples: 1473984. Policy #0 lag: (min: 1.0, avg: 13.6, max: 33.0)
[2026-01-17 17:17:28,702][38814] Avg episode reward: [(0, '24.557')]
[2026-01-17 17:17:28,750][38936] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000002425_9932800.pth...
[2026-01-17 17:17:28,837][38936] Removing train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth
[2026-01-17 17:17:28,856][38991] Updated weights for policy 0, policy_version 2426 (0.0016)
[2026-01-17 17:17:29,273][38991] Updated weights for policy 0, policy_version 2436 (0.0016)
[2026-01-17 17:17:29,783][38936] Signal inference workers to stop experience collection... (900 times)
[2026-01-17 17:17:29,788][38936] Signal inference workers to resume experience collection... (900 times)
[2026-01-17 17:17:29,809][38991] InferenceWorker_p0-w0: stopping experience collection (900 times)
[2026-01-17 17:17:29,809][38991] InferenceWorker_p0-w0: resuming experience collection (900 times)
[2026-01-17 17:17:29,983][38991] Updated weights for policy 0, policy_version 2446 (0.0015)
[2026-01-17 17:17:30,609][38991] Updated weights for policy 0, policy_version 2456 (0.0011)
[2026-01-17 17:17:31,038][38991] Updated weights for policy 0, policy_version 2466 (0.0017)
[2026-01-17 17:17:31,695][38991] Updated weights for policy 0, policy_version 2476 (0.0014)
[2026-01-17 17:17:32,380][38991] Updated weights for policy 0, policy_version 2486 (0.0014)
[2026-01-17 17:17:32,787][38991] Updated weights for policy 0, policy_version 2496 (0.0016)
[2026-01-17 17:17:33,397][38991] Updated weights for policy 0, policy_version 2506 (0.0013)
[2026-01-17 17:17:33,702][38814] Fps is (10 sec: 72908.2, 60 sec: 74001.7, 300 sec: 54552.0). Total num frames: 10280960. Throughput: 0: 18428.3. Samples: 1581012. Policy #0 lag: (min: 0.0, avg: 13.2, max: 30.0)
[2026-01-17 17:17:33,702][38814] Avg episode reward: [(0, '25.542')]
[2026-01-17 17:17:33,703][38936] Saving new best policy, reward=25.542!
[2026-01-17 17:17:34,125][38936] Signal inference workers to stop experience collection... (950 times)
[2026-01-17 17:17:34,142][38936] Signal inference workers to resume experience collection... (950 times)
[2026-01-17 17:17:34,145][38991] InferenceWorker_p0-w0: stopping experience collection (950 times)
[2026-01-17 17:17:34,162][38991] InferenceWorker_p0-w0: resuming experience collection (950 times)
[2026-01-17 17:17:34,197][38991] Updated weights for policy 0, policy_version 2517 (0.0012)
[2026-01-17 17:17:34,686][38991] Updated weights for policy 0, policy_version 2527 (0.0014)
[2026-01-17 17:17:35,282][38991] Updated weights for policy 0, policy_version 2537 (0.0014)
[2026-01-17 17:17:35,899][38991] Updated weights for policy 0, policy_version 2547 (0.0013)
[2026-01-17 17:17:36,334][38991] Updated weights for policy 0, policy_version 2557 (0.0016)
[2026-01-17 17:17:37,021][38991] Updated weights for policy 0, policy_version 2567 (0.0014)
[2026-01-17 17:17:37,493][38991] Updated weights for policy 0, policy_version 2577 (0.0015)
[2026-01-17 17:17:38,042][38991] Updated weights for policy 0, policy_version 2587 (0.0014)
[2026-01-17 17:17:38,540][38936] Signal inference workers to stop experience collection... (1000 times)
[2026-01-17 17:17:38,541][38936] Signal inference workers to resume experience collection... (1000 times)
[2026-01-17 17:17:38,567][38991] InferenceWorker_p0-w0: stopping experience collection (1000 times)
[2026-01-17 17:17:38,567][38991] InferenceWorker_p0-w0: resuming experience collection (1000 times)
[2026-01-17 17:17:38,664][38991] Updated weights for policy 0, policy_version 2597 (0.0012)
[2026-01-17 17:17:38,702][38814] Fps is (10 sec: 71678.3, 60 sec: 73864.2, 300 sec: 55283.3). Total num frames: 10641408. Throughput: 0: 18339.2. Samples: 1633584. Policy #0 lag: (min: 2.0, avg: 14.3, max: 33.0)
[2026-01-17 17:17:38,703][38814] Avg episode reward: [(0, '21.736')]
[2026-01-17 17:17:39,083][38991] Updated weights for policy 0, policy_version 2607 (0.0012)
[2026-01-17 17:17:39,718][38991] Updated weights for policy 0, policy_version 2617 (0.0015)
[2026-01-17 17:17:40,256][38991] Updated weights for policy 0, policy_version 2627 (0.0012)
[2026-01-17 17:17:40,750][38991] Updated weights for policy 0, policy_version 2637 (0.0013)
[2026-01-17 17:17:41,295][38991] Updated weights for policy 0, policy_version 2647 (0.0015)
[2026-01-17 17:17:41,862][38991] Updated weights for policy 0, policy_version 2657 (0.0014)
[2026-01-17 17:17:42,370][38991] Updated weights for policy 0, policy_version 2667 (0.0016)
[2026-01-17 17:17:42,393][38936] Signal inference workers to stop experience collection... (1050 times)
[2026-01-17 17:17:42,408][38936] Signal inference workers to resume experience collection... (1050 times)
[2026-01-17 17:17:42,411][38991] InferenceWorker_p0-w0: stopping experience collection (1050 times)
[2026-01-17 17:17:42,425][38991] InferenceWorker_p0-w0: resuming experience collection (1050 times)
[2026-01-17 17:17:42,885][38991] Updated weights for policy 0, policy_version 2677 (0.0011)
[2026-01-17 17:17:43,549][38991] Updated weights for policy 0, policy_version 2687 (0.0012)
[2026-01-17 17:17:43,702][38814] Fps is (10 sec: 74137.8, 60 sec: 73866.5, 300 sec: 56120.4). Total num frames: 11022336. Throughput: 0: 18396.9. Samples: 1746732. Policy #0 lag: (min: 2.0, avg: 16.1, max: 33.0)
[2026-01-17 17:17:43,702][38814] Avg episode reward: [(0, '24.757')]
[2026-01-17 17:17:44,001][38991] Updated weights for policy 0, policy_version 2697 (0.0016)
[2026-01-17 17:17:44,484][38991] Updated weights for policy 0, policy_version 2707 (0.0012)
[2026-01-17 17:17:45,152][38991] Updated weights for policy 0, policy_version 2717 (0.0014)
[2026-01-17 17:17:45,616][38991] Updated weights for policy 0, policy_version 2727 (0.0014)
[2026-01-17 17:17:46,106][38991] Updated weights for policy 0, policy_version 2737 (0.0013)
[2026-01-17 17:17:46,809][38991] Updated weights for policy 0, policy_version 2747 (0.0015)
[2026-01-17 17:17:46,915][38936] Signal inference workers to stop experience collection... (1100 times)
[2026-01-17 17:17:46,917][38936] Signal inference workers to resume experience collection... (1100 times)
[2026-01-17 17:17:46,948][38991] InferenceWorker_p0-w0: stopping experience collection (1100 times)
[2026-01-17 17:17:46,949][38991] InferenceWorker_p0-w0: resuming experience collection (1100 times)
[2026-01-17 17:17:47,193][38991] Updated weights for policy 0, policy_version 2757 (0.0014)
[2026-01-17 17:17:47,765][38991] Updated weights for policy 0, policy_version 2767 (0.0014)
[2026-01-17 17:17:48,395][38991] Updated weights for policy 0, policy_version 2777 (0.0013)
[2026-01-17 17:17:48,702][38814] Fps is (10 sec: 76598.2, 60 sec: 73933.1, 300 sec: 56924.4). Total num frames: 11407360. Throughput: 0: 18547.3. Samples: 1860684. Policy #0 lag: (min: 0.0, avg: 11.6, max: 27.0)
[2026-01-17 17:17:48,702][38814] Avg episode reward: [(0, '22.043')]
[2026-01-17 17:17:48,802][38991] Updated weights for policy 0, policy_version 2787 (0.0013)
[2026-01-17 17:17:49,510][38991] Updated weights for policy 0, policy_version 2797 (0.0013)
[2026-01-17 17:17:49,929][38991] Updated weights for policy 0, policy_version 2807 (0.0012)
[2026-01-17 17:17:50,403][38991] Updated weights for policy 0, policy_version 2817 (0.0015)
[2026-01-17 17:17:50,777][38936] Signal inference workers to stop experience collection... (1150 times)
[2026-01-17 17:17:50,778][38936] Signal inference workers to resume experience collection... (1150 times)
[2026-01-17 17:17:50,804][38991] InferenceWorker_p0-w0: stopping experience collection (1150 times)
[2026-01-17 17:17:50,805][38991] InferenceWorker_p0-w0: resuming experience collection (1150 times)
[2026-01-17 17:17:51,037][38991] Updated weights for policy 0, policy_version 2827 (0.0015)
[2026-01-17 17:17:51,612][38991] Updated weights for policy 0, policy_version 2837 (0.0014)
[2026-01-17 17:17:52,016][38991] Updated weights for policy 0, policy_version 2847 (0.0012)
[2026-01-17 17:17:52,744][38991] Updated weights for policy 0, policy_version 2857 (0.0013)
[2026-01-17 17:17:53,196][38991] Updated weights for policy 0, policy_version 2867 (0.0014)
[2026-01-17 17:17:53,702][38814] Fps is (10 sec: 75773.9, 60 sec: 73864.5, 300 sec: 57577.5). Total num frames: 11780096. Throughput: 0: 18646.8. Samples: 1917120. Policy #0 lag: (min: 2.0, avg: 16.8, max: 35.0)
[2026-01-17 17:17:53,703][38814] Avg episode reward: [(0, '24.796')]
[2026-01-17 17:17:53,736][38991] Updated weights for policy 0, policy_version 2877 (0.0012)
[2026-01-17 17:17:54,361][38991] Updated weights for policy 0, policy_version 2887 (0.0017)
[2026-01-17 17:17:54,855][38991] Updated weights for policy 0, policy_version 2897 (0.0011)
[2026-01-17 17:17:55,468][38991] Updated weights for policy 0, policy_version 2907 (0.0013)
[2026-01-17 17:17:55,490][38936] Signal inference workers to stop experience collection... (1200 times)
[2026-01-17 17:17:55,500][38991] InferenceWorker_p0-w0: stopping experience collection (1200 times)
[2026-01-17 17:17:55,503][38936] Signal inference workers to resume experience collection... (1200 times)
[2026-01-17 17:17:55,512][38991] InferenceWorker_p0-w0: resuming experience collection (1200 times)
[2026-01-17 17:17:55,962][38991] Updated weights for policy 0, policy_version 2917 (0.0012)
[2026-01-17 17:17:56,553][38991] Updated weights for policy 0, policy_version 2927 (0.0011)
[2026-01-17 17:17:57,051][38991] Updated weights for policy 0, policy_version 2937 (0.0012)
[2026-01-17 17:17:57,625][38991] Updated weights for policy 0, policy_version 2947 (0.0018)
[2026-01-17 17:17:58,218][38991] Updated weights for policy 0, policy_version 2957 (0.0013)
[2026-01-17 17:17:58,643][38991] Updated weights for policy 0, policy_version 2967 (0.0013)
[2026-01-17 17:17:58,702][38814] Fps is (10 sec: 74955.9, 60 sec: 74274.5, 300 sec: 58213.5). Total num frames: 12156928. Throughput: 0: 18648.2. Samples: 2029116. Policy #0 lag: (min: 1.0, avg: 13.2, max: 28.0)
[2026-01-17 17:17:58,702][38814] Avg episode reward: [(0, '23.183')]
[2026-01-17 17:17:59,168][38991] Updated weights for policy 0, policy_version 2977 (0.0015)
[2026-01-17 17:17:59,543][38936] Signal inference workers to stop experience collection... (1250 times)
[2026-01-17 17:17:59,557][38936] Signal inference workers to resume experience collection... (1250 times)
[2026-01-17 17:17:59,561][38991] InferenceWorker_p0-w0: stopping experience collection (1250 times)
[2026-01-17 17:17:59,580][38991] InferenceWorker_p0-w0: resuming experience collection (1250 times)
[2026-01-17 17:17:59,816][38991] Updated weights for policy 0, policy_version 2987 (0.0021)
[2026-01-17 17:18:00,211][38991] Updated weights for policy 0, policy_version 2997 (0.0015)
[2026-01-17 17:18:00,940][38991] Updated weights for policy 0, policy_version 3007 (0.0012)
[2026-01-17 17:18:01,380][38991] Updated weights for policy 0, policy_version 3017 (0.0018)
[2026-01-17 17:18:01,846][38991] Updated weights for policy 0, policy_version 3027 (0.0015)
[2026-01-17 17:18:02,518][38991] Updated weights for policy 0, policy_version 3037 (0.0015)
[2026-01-17 17:18:02,909][38991] Updated weights for policy 0, policy_version 3047 (0.0012)
[2026-01-17 17:18:03,081][38936] Signal inference workers to stop experience collection... (1300 times)
[2026-01-17 17:18:03,094][38936] Signal inference workers to resume experience collection... (1300 times)
[2026-01-17 17:18:03,100][38991] InferenceWorker_p0-w0: stopping experience collection (1300 times)
[2026-01-17 17:18:03,121][38991] InferenceWorker_p0-w0: resuming experience collection (1300 times)
[2026-01-17 17:18:03,540][38991] Updated weights for policy 0, policy_version 3057 (0.0013)
[2026-01-17 17:18:03,702][38814] Fps is (10 sec: 74958.1, 60 sec: 74479.0, 300 sec: 58777.2). Total num frames: 12529664. Throughput: 0: 18625.6. Samples: 2142948. Policy #0 lag: (min: 1.0, avg: 15.6, max: 33.0)
[2026-01-17 17:18:03,702][38814] Avg episode reward: [(0, '24.912')]
[2026-01-17 17:18:04,060][38991] Updated weights for policy 0, policy_version 3067 (0.0012)
[2026-01-17 17:18:04,597][38991] Updated weights for policy 0, policy_version 3077 (0.0012)
[2026-01-17 17:18:05,134][38991] Updated weights for policy 0, policy_version 3087 (0.0011)
[2026-01-17 17:18:05,813][38991] Updated weights for policy 0, policy_version 3097 (0.0013)
[2026-01-17 17:18:06,185][38991] Updated weights for policy 0, policy_version 3107 (0.0020)
[2026-01-17 17:18:06,735][38991] Updated weights for policy 0, policy_version 3117 (0.0014)
[2026-01-17 17:18:07,402][38991] Updated weights for policy 0, policy_version 3127 (0.0014)
[2026-01-17 17:18:07,491][38936] Signal inference workers to stop experience collection... (1350 times)
[2026-01-17 17:18:07,510][38936] Signal inference workers to resume experience collection... (1350 times)
[2026-01-17 17:18:07,511][38991] InferenceWorker_p0-w0: stopping experience collection (1350 times)
[2026-01-17 17:18:07,525][38991] InferenceWorker_p0-w0: resuming experience collection (1350 times)
[2026-01-17 17:18:07,774][38991] Updated weights for policy 0, policy_version 3137 (0.0017)
[2026-01-17 17:18:08,355][38991] Updated weights for policy 0, policy_version 3147 (0.0012)
[2026-01-17 17:18:08,702][38814] Fps is (10 sec: 75776.3, 60 sec: 74960.1, 300 sec: 59385.3). Total num frames: 12914688. Throughput: 0: 18625.8. Samples: 2200344. Policy #0 lag: (min: 2.0, avg: 14.2, max: 29.0)
[2026-01-17 17:18:08,703][38814] Avg episode reward: [(0, '26.499')]
[2026-01-17 17:18:08,717][38936] Saving new best policy, reward=26.499!
[2026-01-17 17:18:08,929][38991] Updated weights for policy 0, policy_version 3157 (0.0013)
[2026-01-17 17:18:09,493][38991] Updated weights for policy 0, policy_version 3167 (0.0013)
[2026-01-17 17:18:09,978][38991] Updated weights for policy 0, policy_version 3177 (0.0014)
[2026-01-17 17:18:10,567][38991] Updated weights for policy 0, policy_version 3187 (0.0013)
[2026-01-17 17:18:11,017][38991] Updated weights for policy 0, policy_version 3197 (0.0018)
[2026-01-17 17:18:11,594][38991] Updated weights for policy 0, policy_version 3207 (0.0012)
[2026-01-17 17:18:11,821][38936] Signal inference workers to stop experience collection... (1400 times)
[2026-01-17 17:18:11,835][38936] Signal inference workers to resume experience collection... (1400 times)
[2026-01-17 17:18:11,841][38991] InferenceWorker_p0-w0: stopping experience collection (1400 times)
[2026-01-17 17:18:11,857][38991] InferenceWorker_p0-w0: resuming experience collection (1400 times)
[2026-01-17 17:18:12,140][38991] Updated weights for policy 0, policy_version 3217 (0.0012)
[2026-01-17 17:18:12,545][38991] Updated weights for policy 0, policy_version 3227 (0.0015)
[2026-01-17 17:18:13,216][38991] Updated weights for policy 0, policy_version 3237 (0.0018)
[2026-01-17 17:18:13,702][38814] Fps is (10 sec: 76186.4, 60 sec: 74820.2, 300 sec: 59901.2). Total num frames: 13291520. Throughput: 0: 18697.1. Samples: 2315352. Policy #0 lag: (min: 0.0, avg: 13.5, max: 28.0)
[2026-01-17 17:18:13,702][38814] Avg episode reward: [(0, '22.919')]
[2026-01-17 17:18:13,765][38991] Updated weights for policy 0, policy_version 3247 (0.0015)
[2026-01-17 17:18:14,120][38991] Updated weights for policy 0, policy_version 3257 (0.0015)
[2026-01-17 17:18:14,815][38991] Updated weights for policy 0, policy_version 3267 (0.0014)
[2026-01-17 17:18:15,280][38991] Updated weights for policy 0, policy_version 3277 (0.0015)
[2026-01-17 17:18:15,384][38936] Signal inference workers to stop experience collection... (1450 times)
[2026-01-17 17:18:15,397][38936] Signal inference workers to resume experience collection... (1450 times)
[2026-01-17 17:18:15,417][38991] InferenceWorker_p0-w0: stopping experience collection (1450 times)
[2026-01-17 17:18:15,429][38991] InferenceWorker_p0-w0: resuming experience collection (1450 times)
[2026-01-17 17:18:15,776][38991] Updated weights for policy 0, policy_version 3287 (0.0012)
[2026-01-17 17:18:16,420][38991] Updated weights for policy 0, policy_version 3297 (0.0013)
[2026-01-17 17:18:16,882][38991] Updated weights for policy 0, policy_version 3307 (0.0013)
[2026-01-17 17:18:17,437][38991] Updated weights for policy 0, policy_version 3317 (0.0015)
[2026-01-17 17:18:17,990][38991] Updated weights for policy 0, policy_version 3327 (0.0012)
[2026-01-17 17:18:18,464][38991] Updated weights for policy 0, policy_version 3337 (0.0013)
[2026-01-17 17:18:18,702][38814] Fps is (10 sec: 77005.0, 60 sec: 75161.7, 300 sec: 60487.4). Total num frames: 13684736. Throughput: 0: 18883.8. Samples: 2430780. Policy #0 lag: (min: 1.0, avg: 12.7, max: 26.0)
[2026-01-17 17:18:18,702][38814] Avg episode reward: [(0, '24.718')]
[2026-01-17 17:18:19,073][38991] Updated weights for policy 0, policy_version 3347 (0.0012)
[2026-01-17 17:18:19,480][38936] Signal inference workers to stop experience collection... (1500 times)
[2026-01-17 17:18:19,494][38936] Signal inference workers to resume experience collection... (1500 times)
[2026-01-17 17:18:19,516][38991] InferenceWorker_p0-w0: stopping experience collection (1500 times)
[2026-01-17 17:18:19,536][38991] InferenceWorker_p0-w0: resuming experience collection (1500 times)
[2026-01-17 17:18:19,572][38991] Updated weights for policy 0, policy_version 3357 (0.0016)
[2026-01-17 17:18:20,055][38991] Updated weights for policy 0, policy_version 3367 (0.0013)
[2026-01-17 17:18:20,656][38991] Updated weights for policy 0, policy_version 3377 (0.0012)
[2026-01-17 17:18:21,144][38991] Updated weights for policy 0, policy_version 3387 (0.0012)
[2026-01-17 17:18:21,771][38991] Updated weights for policy 0, policy_version 3397 (0.0014)
[2026-01-17 17:18:22,304][38991] Updated weights for policy 0, policy_version 3407 (0.0012)
[2026-01-17 17:18:22,714][38991] Updated weights for policy 0, policy_version 3417 (0.0017)
[2026-01-17 17:18:23,115][38936] Signal inference workers to stop experience collection... (1550 times)
[2026-01-17 17:18:23,116][38936] Signal inference workers to resume experience collection... (1550 times)
[2026-01-17 17:18:23,137][38991] InferenceWorker_p0-w0: stopping experience collection (1550 times)
[2026-01-17 17:18:23,138][38991] InferenceWorker_p0-w0: resuming experience collection (1550 times)
[2026-01-17 17:18:23,292][38991] Updated weights for policy 0, policy_version 3427 (0.0014)
[2026-01-17 17:18:23,702][38814] Fps is (10 sec: 77413.3, 60 sec: 75229.6, 300 sec: 60963.3). Total num frames: 14065664. Throughput: 0: 18990.2. Samples: 2488140. Policy #0 lag: (min: 2.0, avg: 14.9, max: 31.0)
[2026-01-17 17:18:23,702][38814] Avg episode reward: [(0, '25.108')]
[2026-01-17 17:18:23,850][38991] Updated weights for policy 0, policy_version 3437 (0.0018)
[2026-01-17 17:18:24,394][38991] Updated weights for policy 0, policy_version 3447 (0.0012)
[2026-01-17 17:18:24,942][38991] Updated weights for policy 0, policy_version 3457 (0.0016)
[2026-01-17 17:18:25,457][38991] Updated weights for policy 0, policy_version 3467 (0.0015)
[2026-01-17 17:18:25,974][38991] Updated weights for policy 0, policy_version 3477 (0.0011)
[2026-01-17 17:18:26,429][38991] Updated weights for policy 0, policy_version 3487 (0.0015)
[2026-01-17 17:18:27,121][38936] Signal inference workers to stop experience collection... (1600 times)
[2026-01-17 17:18:27,138][38936] Signal inference workers to resume experience collection... (1600 times)
[2026-01-17 17:18:27,141][38991] Updated weights for policy 0, policy_version 3497 (0.0012)
[2026-01-17 17:18:27,157][38991] InferenceWorker_p0-w0: stopping experience collection (1600 times)
[2026-01-17 17:18:27,173][38991] InferenceWorker_p0-w0: resuming experience collection (1600 times)
[2026-01-17 17:18:27,583][38991] Updated weights for policy 0, policy_version 3507 (0.0012)
[2026-01-17 17:18:28,021][38991] Updated weights for policy 0, policy_version 3517 (0.0016)
[2026-01-17 17:18:28,702][38814] Fps is (10 sec: 75770.9, 60 sec: 75297.4, 300 sec: 61387.1). Total num frames: 14442496. Throughput: 0: 19041.9. Samples: 2603628. Policy #0 lag: (min: 2.0, avg: 15.0, max: 28.0)
[2026-01-17 17:18:28,703][38814] Avg episode reward: [(0, '25.160')]
[2026-01-17 17:18:28,743][38991] Updated weights for policy 0, policy_version 3527 (0.0011)
[2026-01-17 17:18:29,155][38991] Updated weights for policy 0, policy_version 3537 (0.0014)
[2026-01-17 17:18:29,640][38991] Updated weights for policy 0, policy_version 3547 (0.0015)
[2026-01-17 17:18:30,316][38991] Updated weights for policy 0, policy_version 3557 (0.0014)
[2026-01-17 17:18:30,671][38936] Signal inference workers to stop experience collection... (1650 times)
[2026-01-17 17:18:30,671][38936] Signal inference workers to resume experience collection... (1650 times)
[2026-01-17 17:18:30,697][38991] InferenceWorker_p0-w0: stopping experience collection (1650 times)
[2026-01-17 17:18:30,698][38991] InferenceWorker_p0-w0: resuming experience collection (1650 times)
[2026-01-17 17:18:30,699][38991] Updated weights for policy 0, policy_version 3567 (0.0014)
[2026-01-17 17:18:31,282][38991] Updated weights for policy 0, policy_version 3577 (0.0013)
[2026-01-17 17:18:31,853][38991] Updated weights for policy 0, policy_version 3587 (0.0011)
[2026-01-17 17:18:32,369][38991] Updated weights for policy 0, policy_version 3597 (0.0014)
[2026-01-17 17:18:32,911][38991] Updated weights for policy 0, policy_version 3607 (0.0013)
[2026-01-17 17:18:33,396][38991] Updated weights for policy 0, policy_version 3617 (0.0013)
[2026-01-17 17:18:33,702][38814] Fps is (10 sec: 76596.7, 60 sec: 75844.4, 300 sec: 61857.3). Total num frames: 14831616. Throughput: 0: 19057.6. Samples: 2718276. Policy #0 lag: (min: 2.0, avg: 13.6, max: 27.0)
[2026-01-17 17:18:33,702][38814] Avg episode reward: [(0, '26.438')]
[2026-01-17 17:18:33,992][38991] Updated weights for policy 0, policy_version 3627 (0.0013)
[2026-01-17 17:18:34,535][38991] Updated weights for policy 0, policy_version 3637 (0.0014)
[2026-01-17 17:18:35,020][38991] Updated weights for policy 0, policy_version 3647 (0.0014)
[2026-01-17 17:18:35,508][38936] Signal inference workers to stop experience collection... (1700 times)
[2026-01-17 17:18:35,509][38936] Signal inference workers to resume experience collection... (1700 times)
[2026-01-17 17:18:35,536][38991] InferenceWorker_p0-w0: stopping experience collection (1700 times)
[2026-01-17 17:18:35,537][38991] InferenceWorker_p0-w0: resuming experience collection (1700 times)
[2026-01-17 17:18:35,636][38991] Updated weights for policy 0, policy_version 3657 (0.0014)
[2026-01-17 17:18:36,114][38991] Updated weights for policy 0, policy_version 3667 (0.0018)
[2026-01-17 17:18:36,659][38991] Updated weights for policy 0, policy_version 3677 (0.0018)
[2026-01-17 17:18:37,230][38991] Updated weights for policy 0, policy_version 3687 (0.0013)
[2026-01-17 17:18:37,722][38991] Updated weights for policy 0, policy_version 3697 (0.0013)
[2026-01-17 17:18:38,209][38991] Updated weights for policy 0, policy_version 3707 (0.0014)
[2026-01-17 17:18:38,702][38814] Fps is (10 sec: 77419.1, 60 sec: 76254.2, 300 sec: 62278.3). Total num frames: 15216640. Throughput: 0: 19084.6. Samples: 2775924. Policy #0 lag: (min: 0.0, avg: 15.6, max: 31.0)
[2026-01-17 17:18:38,702][38814] Avg episode reward: [(0, '26.271')]
[2026-01-17 17:18:38,824][38991] Updated weights for policy 0, policy_version 3717 (0.0013)
[2026-01-17 17:18:39,288][38991] Updated weights for policy 0, policy_version 3727 (0.0015)
[2026-01-17 17:18:39,383][38936] Signal inference workers to stop experience collection... (1750 times)
[2026-01-17 17:18:39,389][38991] InferenceWorker_p0-w0: stopping experience collection (1750 times)
[2026-01-17 17:18:39,397][38936] Signal inference workers to resume experience collection... (1750 times)
[2026-01-17 17:18:39,407][38991] InferenceWorker_p0-w0: resuming experience collection (1750 times)
[2026-01-17 17:18:39,828][38991] Updated weights for policy 0, policy_version 3737 (0.0014)
[2026-01-17 17:18:40,439][38991] Updated weights for policy 0, policy_version 3747 (0.0012)
[2026-01-17 17:18:40,920][38991] Updated weights for policy 0, policy_version 3757 (0.0013)
[2026-01-17 17:18:41,387][38991] Updated weights for policy 0, policy_version 3767 (0.0012)
[2026-01-17 17:18:42,070][38991] Updated weights for policy 0, policy_version 3777 (0.0014)
[2026-01-17 17:18:42,516][38991] Updated weights for policy 0, policy_version 3787 (0.0012)
[2026-01-17 17:18:43,030][38991] Updated weights for policy 0, policy_version 3797 (0.0012)
[2026-01-17 17:18:43,665][38991] Updated weights for policy 0, policy_version 3807 (0.0014)
[2026-01-17 17:18:43,702][38814] Fps is (10 sec: 76184.5, 60 sec: 76185.5, 300 sec: 62632.2). Total num frames: 15593472. Throughput: 0: 19138.4. Samples: 2890344. Policy #0 lag: (min: 1.0, avg: 14.6, max: 30.0)
[2026-01-17 17:18:43,703][38814] Avg episode reward: [(0, '26.285')]
[2026-01-17 17:18:43,820][38936] Signal inference workers to stop experience collection... (1800 times)
[2026-01-17 17:18:43,835][38936] Signal inference workers to resume experience collection... (1800 times)
[2026-01-17 17:18:43,856][38991] InferenceWorker_p0-w0: stopping experience collection (1800 times)
[2026-01-17 17:18:43,857][38991] InferenceWorker_p0-w0: resuming experience collection (1800 times)
[2026-01-17 17:18:44,148][38991] Updated weights for policy 0, policy_version 3817 (0.0016)
[2026-01-17 17:18:44,637][38991] Updated weights for policy 0, policy_version 3827 (0.0010)
[2026-01-17 17:18:45,423][38991] Updated weights for policy 0, policy_version 3837 (0.0011)
[2026-01-17 17:18:45,798][38991] Updated weights for policy 0, policy_version 3847 (0.0017)
[2026-01-17 17:18:46,177][38991] Updated weights for policy 0, policy_version 3857 (0.0013)
[2026-01-17 17:18:46,975][38991] Updated weights for policy 0, policy_version 3867 (0.0014)
[2026-01-17 17:18:47,366][38991] Updated weights for policy 0, policy_version 3877 (0.0012)
[2026-01-17 17:18:47,435][38936] Signal inference workers to stop experience collection... (1850 times)
[2026-01-17 17:18:47,436][38936] Signal inference workers to resume experience collection... (1850 times)
[2026-01-17 17:18:47,456][38991] InferenceWorker_p0-w0: stopping experience collection (1850 times)
[2026-01-17 17:18:47,457][38991] InferenceWorker_p0-w0: resuming experience collection (1850 times)
[2026-01-17 17:18:47,857][38991] Updated weights for policy 0, policy_version 3887 (0.0012)
[2026-01-17 17:18:48,436][38991] Updated weights for policy 0, policy_version 3897 (0.0014)
[2026-01-17 17:18:48,702][38814] Fps is (10 sec: 76593.7, 60 sec: 76253.5, 300 sec: 63032.2). Total num frames: 15982592. Throughput: 0: 19176.5. Samples: 3005892. Policy #0 lag: (min: 2.0, avg: 17.6, max: 31.0)
[2026-01-17 17:18:48,702][38814] Avg episode reward: [(0, '27.742')]
[2026-01-17 17:18:48,705][38936] Saving new best policy, reward=27.742!
[2026-01-17 17:18:49,040][38991] Updated weights for policy 0, policy_version 3907 (0.0013)
[2026-01-17 17:18:49,424][38991] Updated weights for policy 0, policy_version 3917 (0.0014)
[2026-01-17 17:18:50,108][38991] Updated weights for policy 0, policy_version 3927 (0.0010)
[2026-01-17 17:18:50,616][38991] Updated weights for policy 0, policy_version 3937 (0.0013)
[2026-01-17 17:18:51,009][38991] Updated weights for policy 0, policy_version 3947 (0.0013)
[2026-01-17 17:18:51,690][38936] Signal inference workers to stop experience collection... (1900 times)
[2026-01-17 17:18:51,705][38936] Signal inference workers to resume experience collection... (1900 times)
[2026-01-17 17:18:51,708][38991] Updated weights for policy 0, policy_version 3957 (0.0012)
[2026-01-17 17:18:51,723][38991] InferenceWorker_p0-w0: stopping experience collection (1900 times)
[2026-01-17 17:18:51,724][38991] InferenceWorker_p0-w0: resuming experience collection (1900 times)
[2026-01-17 17:18:52,218][38991] Updated weights for policy 0, policy_version 3967 (0.0014)
[2026-01-17 17:18:52,590][38991] Updated weights for policy 0, policy_version 3977 (0.0014)
[2026-01-17 17:18:53,213][38991] Updated weights for policy 0, policy_version 3987 (0.0013)
[2026-01-17 17:18:53,702][38814] Fps is (10 sec: 76591.5, 60 sec: 76321.8, 300 sec: 63348.6). Total num frames: 16359424. Throughput: 0: 19172.0. Samples: 3063096. Policy #0 lag: (min: 1.0, avg: 12.1, max: 28.0)
[2026-01-17 17:18:53,703][38814] Avg episode reward: [(0, '28.264')]
[2026-01-17 17:18:53,749][38936] Saving new best policy, reward=28.264!
[2026-01-17 17:18:53,890][38991] Updated weights for policy 0, policy_version 3997 (0.0016)
[2026-01-17 17:18:54,294][38991] Updated weights for policy 0, policy_version 4007 (0.0011)
[2026-01-17 17:18:54,737][38991] Updated weights for policy 0, policy_version 4017 (0.0019)
[2026-01-17 17:18:55,437][38936] Signal inference workers to stop experience collection... (1950 times)
[2026-01-17 17:18:55,445][38936] Signal inference workers to resume experience collection... (1950 times)
[2026-01-17 17:18:55,449][38991] Updated weights for policy 0, policy_version 4027 (0.0012)
[2026-01-17 17:18:55,464][38991] InferenceWorker_p0-w0: stopping experience collection (1950 times)
[2026-01-17 17:18:55,464][38991] InferenceWorker_p0-w0: resuming experience collection (1950 times)
[2026-01-17 17:18:55,860][38991] Updated weights for policy 0, policy_version 4037 (0.0015)
[2026-01-17 17:18:56,362][38991] Updated weights for policy 0, policy_version 4047 (0.0011)
[2026-01-17 17:18:57,073][38991] Updated weights for policy 0, policy_version 4057 (0.0012)
[2026-01-17 17:18:57,467][38991] Updated weights for policy 0, policy_version 4067 (0.0016)
[2026-01-17 17:18:58,064][38991] Updated weights for policy 0, policy_version 4077 (0.0017)
[2026-01-17 17:18:58,625][38991] Updated weights for policy 0, policy_version 4087 (0.0015)
[2026-01-17 17:18:58,702][38814] Fps is (10 sec: 76594.5, 60 sec: 76526.6, 300 sec: 63710.8). Total num frames: 16748544. Throughput: 0: 19172.1. Samples: 3178104. Policy #0 lag: (min: 0.0, avg: 16.9, max: 30.0)
[2026-01-17 17:18:58,703][38814] Avg episode reward: [(0, '24.318')]
[2026-01-17 17:18:59,017][38991] Updated weights for policy 0, policy_version 4097 (0.0014)
[2026-01-17 17:18:59,078][38936] Signal inference workers to stop experience collection... (2000 times)
[2026-01-17 17:18:59,078][38936] Signal inference workers to resume experience collection... (2000 times)
[2026-01-17 17:18:59,108][38991] InferenceWorker_p0-w0: stopping experience collection (2000 times)
[2026-01-17 17:18:59,109][38991] InferenceWorker_p0-w0: resuming experience collection (2000 times)
[2026-01-17 17:18:59,688][38991] Updated weights for policy 0, policy_version 4107 (0.0015)
[2026-01-17 17:19:00,114][38991] Updated weights for policy 0, policy_version 4117 (0.0018)
[2026-01-17 17:19:00,694][38991] Updated weights for policy 0, policy_version 4127 (0.0014)
[2026-01-17 17:19:01,263][38991] Updated weights for policy 0, policy_version 4137 (0.0015)
[2026-01-17 17:19:01,681][38991] Updated weights for policy 0, policy_version 4147 (0.0014)
[2026-01-17 17:19:02,342][38991] Updated weights for policy 0, policy_version 4157 (0.0013)
[2026-01-17 17:19:02,758][38991] Updated weights for policy 0, policy_version 4167 (0.0015)
[2026-01-17 17:19:03,023][38936] Signal inference workers to stop experience collection... (2050 times)
[2026-01-17 17:19:03,033][38936] Signal inference workers to resume experience collection... (2050 times)
[2026-01-17 17:19:03,049][38991] InferenceWorker_p0-w0: stopping experience collection (2050 times)
[2026-01-17 17:19:03,050][38991] InferenceWorker_p0-w0: resuming experience collection (2050 times)
[2026-01-17 17:19:03,273][38991] Updated weights for policy 0, policy_version 4177 (0.0014)
[2026-01-17 17:19:03,702][38814] Fps is (10 sec: 76598.4, 60 sec: 76595.1, 300 sec: 63995.3). Total num frames: 17125376. Throughput: 0: 19171.1. Samples: 3293484. Policy #0 lag: (min: 0.0, avg: 16.1, max: 34.0)
[2026-01-17 17:19:03,704][38814] Avg episode reward: [(0, '26.623')]
[2026-01-17 17:19:03,977][38991] Updated weights for policy 0, policy_version 4187 (0.0016)
[2026-01-17 17:19:04,381][38991] Updated weights for policy 0, policy_version 4197 (0.0013)
[2026-01-17 17:19:04,918][38991] Updated weights for policy 0, policy_version 4207 (0.0016)
[2026-01-17 17:19:05,515][38991] Updated weights for policy 0, policy_version 4217 (0.0012)
[2026-01-17 17:19:06,001][38991] Updated weights for policy 0, policy_version 4227 (0.0012)
[2026-01-17 17:19:06,565][38991] Updated weights for policy 0, policy_version 4237 (0.0019)
[2026-01-17 17:19:06,995][38936] Signal inference workers to stop experience collection... (2100 times)
[2026-01-17 17:19:07,005][38936] Signal inference workers to resume experience collection... (2100 times)
[2026-01-17 17:19:07,030][38991] InferenceWorker_p0-w0: stopping experience collection (2100 times)
[2026-01-17 17:19:07,030][38991] InferenceWorker_p0-w0: resuming experience collection (2100 times)
[2026-01-17 17:19:07,113][38991] Updated weights for policy 0, policy_version 4247 (0.0015)
[2026-01-17 17:19:07,657][38991] Updated weights for policy 0, policy_version 4257 (0.0015)
[2026-01-17 17:19:08,156][38991] Updated weights for policy 0, policy_version 4267 (0.0014)
[2026-01-17 17:19:08,702][38814] Fps is (10 sec: 76185.9, 60 sec: 76594.8, 300 sec: 64305.2). Total num frames: 17510400. Throughput: 0: 19167.1. Samples: 3350664. Policy #0 lag: (min: 1.0, avg: 14.7, max: 28.0)
[2026-01-17 17:19:08,703][38814] Avg episode reward: [(0, '25.299')]
[2026-01-17 17:19:08,775][38991] Updated weights for policy 0, policy_version 4277 (0.0011)
[2026-01-17 17:19:09,332][38991] Updated weights for policy 0, policy_version 4287 (0.0020)
[2026-01-17 17:19:09,808][38991] Updated weights for policy 0, policy_version 4297 (0.0012)
[2026-01-17 17:19:10,498][38991] Updated weights for policy 0, policy_version 4307 (0.0012)
[2026-01-17 17:19:10,938][38991] Updated weights for policy 0, policy_version 4317 (0.0015)
[2026-01-17 17:19:11,200][38936] Signal inference workers to stop experience collection... (2150 times)
[2026-01-17 17:19:11,201][38936] Signal inference workers to resume experience collection... (2150 times)
[2026-01-17 17:19:11,225][38991] InferenceWorker_p0-w0: stopping experience collection (2150 times)
[2026-01-17 17:19:11,226][38991] InferenceWorker_p0-w0: resuming experience collection (2150 times)
[2026-01-17 17:19:11,388][38991] Updated weights for policy 0, policy_version 4327 (0.0015)
[2026-01-17 17:19:12,176][38991] Updated weights for policy 0, policy_version 4337 (0.0011)
[2026-01-17 17:19:12,611][38991] Updated weights for policy 0, policy_version 4347 (0.0013)
[2026-01-17 17:19:13,027][38991] Updated weights for policy 0, policy_version 4357 (0.0012)
[2026-01-17 17:19:13,702][38814] Fps is (10 sec: 75776.5, 60 sec: 76526.8, 300 sec: 64543.5). Total num frames: 17883136. Throughput: 0: 19111.2. Samples: 3463620. Policy #0 lag: (min: 4.0, avg: 17.7, max: 30.0)
[2026-01-17 17:19:13,702][38814] Avg episode reward: [(0, '23.431')]
[2026-01-17 17:19:13,735][38991] Updated weights for policy 0, policy_version 4367 (0.0011)
[2026-01-17 17:19:14,156][38991] Updated weights for policy 0, policy_version 4377 (0.0014)
[2026-01-17 17:19:14,848][38991] Updated weights for policy 0, policy_version 4387 (0.0015)
[2026-01-17 17:19:15,273][38936] Signal inference workers to stop experience collection... (2200 times)
[2026-01-17 17:19:15,295][38936] Signal inference workers to resume experience collection... (2200 times)
[2026-01-17 17:19:15,303][38991] InferenceWorker_p0-w0: stopping experience collection (2200 times)
[2026-01-17 17:19:15,305][38991] Updated weights for policy 0, policy_version 4397 (0.0014)
[2026-01-17 17:19:15,324][38991] InferenceWorker_p0-w0: resuming experience collection (2200 times)
[2026-01-17 17:19:15,739][38991] Updated weights for policy 0, policy_version 4407 (0.0011)
[2026-01-17 17:19:16,367][38991] Updated weights for policy 0, policy_version 4417 (0.0014)
[2026-01-17 17:19:16,860][38991] Updated weights for policy 0, policy_version 4427 (0.0012)
[2026-01-17 17:19:17,475][38991] Updated weights for policy 0, policy_version 4437 (0.0012)
[2026-01-17 17:19:17,931][38991] Updated weights for policy 0, policy_version 4447 (0.0013)
[2026-01-17 17:19:18,513][38991] Updated weights for policy 0, policy_version 4457 (0.0011)
[2026-01-17 17:19:18,702][38814] Fps is (10 sec: 75368.4, 60 sec: 76322.1, 300 sec: 64808.3). Total num frames: 18264064. Throughput: 0: 19074.1. Samples: 3576612. Policy #0 lag: (min: 1.0, avg: 13.0, max: 30.0)
[2026-01-17 17:19:18,702][38814] Avg episode reward: [(0, '26.805')]
[2026-01-17 17:19:19,180][38991] Updated weights for policy 0, policy_version 4467 (0.0015)
[2026-01-17 17:19:19,548][38936] Signal inference workers to stop experience collection... (2250 times)
[2026-01-17 17:19:19,563][38936] Signal inference workers to resume experience collection... (2250 times)
[2026-01-17 17:19:19,564][38991] Updated weights for policy 0, policy_version 4477 (0.0015)
[2026-01-17 17:19:19,578][38991] InferenceWorker_p0-w0: stopping experience collection (2250 times)
[2026-01-17 17:19:19,595][38991] InferenceWorker_p0-w0: resuming experience collection (2250 times)
[2026-01-17 17:19:20,281][38991] Updated weights for policy 0, policy_version 4487 (0.0011)
[2026-01-17 17:19:20,656][38991] Updated weights for policy 0, policy_version 4497 (0.0014)
[2026-01-17 17:19:21,160][38991] Updated weights for policy 0, policy_version 4507 (0.0013)
[2026-01-17 17:19:21,813][38991] Updated weights for policy 0, policy_version 4517 (0.0015)
[2026-01-17 17:19:22,299][38991] Updated weights for policy 0, policy_version 4527 (0.0014)
[2026-01-17 17:19:22,870][38991] Updated weights for policy 0, policy_version 4537 (0.0014)
[2026-01-17 17:19:23,456][38991] Updated weights for policy 0, policy_version 4547 (0.0013)
[2026-01-17 17:19:23,503][38936] Signal inference workers to stop experience collection... (2300 times)
[2026-01-17 17:19:23,527][38936] Signal inference workers to resume experience collection... (2300 times)
[2026-01-17 17:19:23,527][38991] InferenceWorker_p0-w0: stopping experience collection (2300 times)
[2026-01-17 17:19:23,545][38991] InferenceWorker_p0-w0: resuming experience collection (2300 times)
[2026-01-17 17:19:23,702][38814] Fps is (10 sec: 76595.0, 60 sec: 76390.4, 300 sec: 65079.4). Total num frames: 18649088. Throughput: 0: 19065.8. Samples: 3633888. Policy #0 lag: (min: 1.0, avg: 14.3, max: 29.0)
[2026-01-17 17:19:23,703][38814] Avg episode reward: [(0, '29.053')]
[2026-01-17 17:19:23,721][38936] Saving new best policy, reward=29.053!
[2026-01-17 17:19:23,896][38991] Updated weights for policy 0, policy_version 4557 (0.0017)
[2026-01-17 17:19:24,601][38991] Updated weights for policy 0, policy_version 4567 (0.0013)
[2026-01-17 17:19:25,050][38991] Updated weights for policy 0, policy_version 4577 (0.0013)
[2026-01-17 17:19:25,606][38991] Updated weights for policy 0, policy_version 4587 (0.0012)
[2026-01-17 17:19:26,173][38991] Updated weights for policy 0, policy_version 4597 (0.0015)
[2026-01-17 17:19:26,585][38991] Updated weights for policy 0, policy_version 4607 (0.0016)
[2026-01-17 17:19:27,245][38991] Updated weights for policy 0, policy_version 4617 (0.0019)
[2026-01-17 17:19:27,501][38936] Signal inference workers to stop experience collection... (2350 times)
[2026-01-17 17:19:27,520][38936] Signal inference workers to resume experience collection... (2350 times)
[2026-01-17 17:19:27,521][38991] InferenceWorker_p0-w0: stopping experience collection (2350 times)
[2026-01-17 17:19:27,544][38991] InferenceWorker_p0-w0: resuming experience collection (2350 times)
[2026-01-17 17:19:27,699][38991] Updated weights for policy 0, policy_version 4627 (0.0015)
[2026-01-17 17:19:28,227][38991] Updated weights for policy 0, policy_version 4637 (0.0013)
[2026-01-17 17:19:28,702][38814] Fps is (10 sec: 75776.0, 60 sec: 76322.9, 300 sec: 65285.4). Total num frames: 19021824. Throughput: 0: 19045.1. Samples: 3747372. Policy #0 lag: (min: 3.0, avg: 17.0, max: 33.0)
[2026-01-17 17:19:28,702][38814] Avg episode reward: [(0, '24.494')]
[2026-01-17 17:19:28,755][38936] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000004646_19030016.pth...
[2026-01-17 17:19:28,796][38936] Removing train_dir/default_experiment/checkpoint_p0/checkpoint_000000980_4014080.pth
[2026-01-17 17:19:28,824][38991] Updated weights for policy 0, policy_version 4647 (0.0014)
[2026-01-17 17:19:29,340][38991] Updated weights for policy 0, policy_version 4657 (0.0014)
[2026-01-17 17:19:29,969][38991] Updated weights for policy 0, policy_version 4667 (0.0013)
[2026-01-17 17:19:30,430][38991] Updated weights for policy 0, policy_version 4677 (0.0014)
[2026-01-17 17:19:30,954][38991] Updated weights for policy 0, policy_version 4687 (0.0012)
[2026-01-17 17:19:31,522][38991] Updated weights for policy 0, policy_version 4697 (0.0013)
[2026-01-17 17:19:31,698][38936] Signal inference workers to stop experience collection... (2400 times)
[2026-01-17 17:19:31,714][38936] Signal inference workers to resume experience collection... (2400 times)
[2026-01-17 17:19:31,720][38991] InferenceWorker_p0-w0: stopping experience collection (2400 times)
[2026-01-17 17:19:31,739][38991] InferenceWorker_p0-w0: resuming experience collection (2400 times)
[2026-01-17 17:19:32,041][38991] Updated weights for policy 0, policy_version 4707 (0.0013)
[2026-01-17 17:19:32,552][38991] Updated weights for policy 0, policy_version 4717 (0.0014)
[2026-01-17 17:19:33,157][38991] Updated weights for policy 0, policy_version 4727 (0.0015)
[2026-01-17 17:19:33,680][38991] Updated weights for policy 0, policy_version 4737 (0.0015)
[2026-01-17 17:19:33,702][38814] Fps is (10 sec: 75775.7, 60 sec: 76253.6, 300 sec: 65534.8). Total num frames: 19406848. Throughput: 0: 18998.4. Samples: 3860820. Policy #0 lag: (min: 1.0, avg: 13.6, max: 30.0)
[2026-01-17 17:19:33,703][38814] Avg episode reward: [(0, '27.886')]
[2026-01-17 17:19:34,237][38991] Updated weights for policy 0, policy_version 4747 (0.0013)
[2026-01-17 17:19:34,712][38991] Updated weights for policy 0, policy_version 4757 (0.0013)
[2026-01-17 17:19:35,298][38991] Updated weights for policy 0, policy_version 4767 (0.0012)
[2026-01-17 17:19:35,863][38991] Updated weights for policy 0, policy_version 4777 (0.0016)
[2026-01-17 17:19:36,006][38936] Signal inference workers to stop experience collection... (2450 times)
[2026-01-17 17:19:36,006][38936] Signal inference workers to resume experience collection... (2450 times)
[2026-01-17 17:19:36,020][38991] InferenceWorker_p0-w0: stopping experience collection (2450 times)
[2026-01-17 17:19:36,021][38991] InferenceWorker_p0-w0: resuming experience collection (2450 times)
[2026-01-17 17:19:36,329][38991] Updated weights for policy 0, policy_version 4787 (0.0016)
[2026-01-17 17:19:36,885][38991] Updated weights for policy 0, policy_version 4797 (0.0013)
[2026-01-17 17:19:37,428][38991] Updated weights for policy 0, policy_version 4807 (0.0012)
[2026-01-17 17:19:37,964][38991] Updated weights for policy 0, policy_version 4817 (0.0016)
[2026-01-17 17:19:38,619][38991] Updated weights for policy 0, policy_version 4827 (0.0017)
[2026-01-17 17:19:38,702][38814] Fps is (10 sec: 75773.2, 60 sec: 76048.6, 300 sec: 65722.6). Total num frames: 19779584. Throughput: 0: 19008.6. Samples: 3918480. Policy #0 lag: (min: 0.0, avg: 15.0, max: 31.0)
[2026-01-17 17:19:38,703][38814] Avg episode reward: [(0, '30.275')]
[2026-01-17 17:19:38,736][38936] Saving new best policy, reward=30.275!
[2026-01-17 17:19:39,105][38991] Updated weights for policy 0, policy_version 4837 (0.0012)
[2026-01-17 17:19:39,623][38991] Updated weights for policy 0, policy_version 4847 (0.0011)
[2026-01-17 17:19:40,214][38991] Updated weights for policy 0, policy_version 4857 (0.0014)
[2026-01-17 17:19:40,637][38936] Signal inference workers to stop experience collection... (2500 times)
[2026-01-17 17:19:40,652][38936] Signal inference workers to resume experience collection... (2500 times)
[2026-01-17 17:19:40,654][38991] InferenceWorker_p0-w0: stopping experience collection (2500 times)
[2026-01-17 17:19:40,675][38991] InferenceWorker_p0-w0: resuming experience collection (2500 times)
[2026-01-17 17:19:40,808][38991] Updated weights for policy 0, policy_version 4867 (0.0014)
[2026-01-17 17:19:41,201][38991] Updated weights for policy 0, policy_version 4877 (0.0013)
[2026-01-17 17:19:41,779][38991] Updated weights for policy 0, policy_version 4887 (0.0017)
[2026-01-17 17:19:42,444][38991] Updated weights for policy 0, policy_version 4897 (0.0014)
[2026-01-17 17:19:42,883][38991] Updated weights for policy 0, policy_version 4907 (0.0014)
[2026-01-17 17:19:43,381][38991] Updated weights for policy 0, policy_version 4917 (0.0014)
[2026-01-17 17:19:43,702][38814] Fps is (10 sec: 74136.9, 60 sec: 75912.3, 300 sec: 65886.1). Total num frames: 20148224. Throughput: 0: 18953.4. Samples: 4031004. Policy #0 lag: (min: 0.0, avg: 15.1, max: 29.0)
[2026-01-17 17:19:43,702][38814] Avg episode reward: [(0, '28.255')]
[2026-01-17 17:19:44,017][38991] Updated weights for policy 0, policy_version 4927 (0.0015)
[2026-01-17 17:19:44,148][38936] Signal inference workers to stop experience collection... (2550 times)
[2026-01-17 17:19:44,163][38936] Signal inference workers to resume experience collection... (2550 times)
[2026-01-17 17:19:44,178][38991] InferenceWorker_p0-w0: stopping experience collection (2550 times)
[2026-01-17 17:19:44,196][38991] InferenceWorker_p0-w0: resuming experience collection (2550 times)
[2026-01-17 17:19:44,426][38991] Updated weights for policy 0, policy_version 4937 (0.0014)
[2026-01-17 17:19:45,142][38991] Updated weights for policy 0, policy_version 4947 (0.0013)
[2026-01-17 17:19:45,587][38991] Updated weights for policy 0, policy_version 4957 (0.0014)
[2026-01-17 17:19:45,942][38991] Updated weights for policy 0, policy_version 4967 (0.0014)
[2026-01-17 17:19:46,718][38991] Updated weights for policy 0, policy_version 4977 (0.0012)
[2026-01-17 17:19:47,172][38991] Updated weights for policy 0, policy_version 4987 (0.0012)
[2026-01-17 17:19:47,717][38991] Updated weights for policy 0, policy_version 4997 (0.0014)
[2026-01-17 17:19:47,968][38936] Signal inference workers to stop experience collection... (2600 times)
[2026-01-17 17:19:47,971][38991] InferenceWorker_p0-w0: stopping experience collection (2600 times)
[2026-01-17 17:19:47,983][38936] Signal inference workers to resume experience collection... (2600 times)
[2026-01-17 17:19:47,986][38991] InferenceWorker_p0-w0: resuming experience collection (2600 times)
[2026-01-17 17:19:48,337][38991] Updated weights for policy 0, policy_version 5007 (0.0015)
[2026-01-17 17:19:48,702][38814] Fps is (10 sec: 76186.6, 60 sec: 75980.8, 300 sec: 66141.4). Total num frames: 20541440. Throughput: 0: 18919.7. Samples: 4144872. Policy #0 lag: (min: 0.0, avg: 15.7, max: 32.0)
[2026-01-17 17:19:48,703][38814] Avg episode reward: [(0, '26.653')]
[2026-01-17 17:19:48,759][38991] Updated weights for policy 0, policy_version 5017 (0.0013)
[2026-01-17 17:19:49,308][38991] Updated weights for policy 0, policy_version 5027 (0.0015)
[2026-01-17 17:19:49,905][38991] Updated weights for policy 0, policy_version 5037 (0.0013)
[2026-01-17 17:19:50,415][38991] Updated weights for policy 0, policy_version 5047 (0.0016)
[2026-01-17 17:19:50,941][38991] Updated weights for policy 0, policy_version 5057 (0.0012)
[2026-01-17 17:19:51,621][38991] Updated weights for policy 0, policy_version 5067 (0.0015)
[2026-01-17 17:19:51,823][38936] Signal inference workers to stop experience collection... (2650 times)
[2026-01-17 17:19:51,838][38936] Signal inference workers to resume experience collection... (2650 times)
[2026-01-17 17:19:51,854][38991] InferenceWorker_p0-w0: stopping experience collection (2650 times)
[2026-01-17 17:19:51,854][38991] InferenceWorker_p0-w0: resuming experience collection (2650 times)
[2026-01-17 17:19:52,032][38991] Updated weights for policy 0, policy_version 5077 (0.0016)
[2026-01-17 17:19:52,565][38991] Updated weights for policy 0, policy_version 5087 (0.0014)
[2026-01-17 17:19:53,265][38991] Updated weights for policy 0, policy_version 5097 (0.0016)
[2026-01-17 17:19:53,663][38991] Updated weights for policy 0, policy_version 5107 (0.0014)
[2026-01-17 17:19:53,702][38814] Fps is (10 sec: 77413.5, 60 sec: 76049.3, 300 sec: 66338.4). Total num frames: 20922368. Throughput: 0: 18939.7. Samples: 4202952. Policy #0 lag: (min: 0.0, avg: 11.7, max: 31.0)
[2026-01-17 17:19:53,703][38814] Avg episode reward: [(0, '26.299')]
[2026-01-17 17:19:54,229][38991] Updated weights for policy 0, policy_version 5117 (0.0012)
[2026-01-17 17:19:54,895][38991] Updated weights for policy 0, policy_version 5127 (0.0013)
[2026-01-17 17:19:55,350][38991] Updated weights for policy 0, policy_version 5137 (0.0012)
[2026-01-17 17:19:55,908][38991] Updated weights for policy 0, policy_version 5147 (0.0014)
[2026-01-17 17:19:56,106][38936] Signal inference workers to stop experience collection... (2700 times)
[2026-01-17 17:19:56,117][38936] Signal inference workers to resume experience collection... (2700 times)
[2026-01-17 17:19:56,121][38991] InferenceWorker_p0-w0: stopping experience collection (2700 times)
[2026-01-17 17:19:56,134][38991] InferenceWorker_p0-w0: resuming experience collection (2700 times)
[2026-01-17 17:19:56,569][38991] Updated weights for policy 0, policy_version 5157 (0.0014)
[2026-01-17 17:19:56,938][38991] Updated weights for policy 0, policy_version 5167 (0.0013)
[2026-01-17 17:19:57,524][38991] Updated weights for policy 0, policy_version 5177 (0.0012)
[2026-01-17 17:19:58,071][38991] Updated weights for policy 0, policy_version 5187 (0.0013)
[2026-01-17 17:19:58,657][38991] Updated weights for policy 0, policy_version 5197 (0.0014)
[2026-01-17 17:19:58,702][38814] Fps is (10 sec: 74957.7, 60 sec: 75708.0, 300 sec: 66480.6). Total num frames: 21291008. Throughput: 0: 18898.4. Samples: 4314048. Policy #0 lag: (min: 1.0, avg: 12.7, max: 30.0)
[2026-01-17 17:19:58,702][38814] Avg episode reward: [(0, '25.048')]
[2026-01-17 17:19:59,162][38991] Updated weights for policy 0, policy_version 5207 (0.0012)
[2026-01-17 17:19:59,625][38991] Updated weights for policy 0, policy_version 5217 (0.0014)
[2026-01-17 17:20:00,379][38991] Updated weights for policy 0, policy_version 5227 (0.0013)
[2026-01-17 17:20:00,525][38936] Signal inference workers to stop experience collection... (2750 times)
[2026-01-17 17:20:00,536][38936] Signal inference workers to resume experience collection... (2750 times)
[2026-01-17 17:20:00,555][38991] InferenceWorker_p0-w0: stopping experience collection (2750 times)
[2026-01-17 17:20:00,577][38991] InferenceWorker_p0-w0: resuming experience collection (2750 times)
[2026-01-17 17:20:00,834][38991] Updated weights for policy 0, policy_version 5237 (0.0011)
[2026-01-17 17:20:01,381][38991] Updated weights for policy 0, policy_version 5247 (0.0011)
[2026-01-17 17:20:02,053][38991] Updated weights for policy 0, policy_version 5257 (0.0013)
[2026-01-17 17:20:02,538][38991] Updated weights for policy 0, policy_version 5267 (0.0015)
[2026-01-17 17:20:03,166][38991] Updated weights for policy 0, policy_version 5277 (0.0014)
[2026-01-17 17:20:03,702][38814] Fps is (10 sec: 72911.1, 60 sec: 75434.8, 300 sec: 66586.5). Total num frames: 21651456. Throughput: 0: 18815.7. Samples: 4423320. Policy #0 lag: (min: 0.0, avg: 16.9, max: 31.0)
[2026-01-17 17:20:03,702][38814] Avg episode reward: [(0, '27.511')]
[2026-01-17 17:20:03,733][38991] Updated weights for policy 0, policy_version 5287 (0.0013)
[2026-01-17 17:20:04,329][38991] Updated weights for policy 0, policy_version 5297 (0.0016)
[2026-01-17 17:20:04,875][38991] Updated weights for policy 0, policy_version 5307 (0.0016)
[2026-01-17 17:20:04,938][38936] Signal inference workers to stop experience collection... (2800 times)
[2026-01-17 17:20:04,953][38936] Signal inference workers to resume experience collection... (2800 times)
[2026-01-17 17:20:04,962][38991] InferenceWorker_p0-w0: stopping experience collection (2800 times)
[2026-01-17 17:20:04,981][38991] InferenceWorker_p0-w0: resuming experience collection (2800 times)
[2026-01-17 17:20:05,503][38991] Updated weights for policy 0, policy_version 5317 (0.0014)
[2026-01-17 17:20:06,092][38991] Updated weights for policy 0, policy_version 5327 (0.0014)
[2026-01-17 17:20:06,580][38991] Updated weights for policy 0, policy_version 5337 (0.0014)
[2026-01-17 17:20:07,361][38991] Updated weights for policy 0, policy_version 5347 (0.0015)
[2026-01-17 17:20:07,811][38991] Updated weights for policy 0, policy_version 5357 (0.0017)
[2026-01-17 17:20:08,280][38991] Updated weights for policy 0, policy_version 5367 (0.0017)
[2026-01-17 17:20:08,702][38814] Fps is (10 sec: 70860.6, 60 sec: 74820.4, 300 sec: 66642.9). Total num frames: 21999616. Throughput: 0: 18716.0. Samples: 4476108. Policy #0 lag: (min: 0.0, avg: 15.8, max: 27.0)
[2026-01-17 17:20:08,702][38814] Avg episode reward: [(0, '26.106')]
[2026-01-17 17:20:08,994][38991] Updated weights for policy 0, policy_version 5377 (0.0013)
[2026-01-17 17:20:09,399][38936] Signal inference workers to stop experience collection... (2850 times)
[2026-01-17 17:20:09,410][38936] Signal inference workers to resume experience collection... (2850 times)
[2026-01-17 17:20:09,432][38991] InferenceWorker_p0-w0: stopping experience collection (2850 times)
[2026-01-17 17:20:09,432][38991] InferenceWorker_p0-w0: resuming experience collection (2850 times)
[2026-01-17 17:20:09,491][38991] Updated weights for policy 0, policy_version 5387 (0.0014)
[2026-01-17 17:20:09,988][38991] Updated weights for policy 0, policy_version 5397 (0.0015)
[2026-01-17 17:20:10,644][38991] Updated weights for policy 0, policy_version 5407 (0.0020)
[2026-01-17 17:20:11,045][38991] Updated weights for policy 0, policy_version 5417 (0.0014)
[2026-01-17 17:20:11,630][38991] Updated weights for policy 0, policy_version 5427 (0.0013)
[2026-01-17 17:20:12,262][38991] Updated weights for policy 0, policy_version 5437 (0.0014)
[2026-01-17 17:20:12,781][38991] Updated weights for policy 0, policy_version 5447 (0.0011)
[2026-01-17 17:20:13,308][38991] Updated weights for policy 0, policy_version 5458 (0.0013)
[2026-01-17 17:20:13,381][38936] Signal inference workers to stop experience collection... (2900 times)
[2026-01-17 17:20:13,382][38936] Signal inference workers to resume experience collection... (2900 times)
[2026-01-17 17:20:13,399][38991] InferenceWorker_p0-w0: stopping experience collection (2900 times)
[2026-01-17 17:20:13,399][38991] InferenceWorker_p0-w0: resuming experience collection (2900 times)
[2026-01-17 17:20:13,702][38814] Fps is (10 sec: 72499.2, 60 sec: 74888.6, 300 sec: 66801.6). Total num frames: 22376448. Throughput: 0: 18665.3. Samples: 4587312. Policy #0 lag: (min: 0.0, avg: 15.2, max: 30.0)
[2026-01-17 17:20:13,702][38814] Avg episode reward: [(0, '25.474')]
[2026-01-17 17:20:13,951][38991] Updated weights for policy 0, policy_version 5468 (0.0012)
[2026-01-17 17:20:14,484][38991] Updated weights for policy 0, policy_version 5478 (0.0013)
[2026-01-17 17:20:15,046][38991] Updated weights for policy 0, policy_version 5488 (0.0016)
[2026-01-17 17:20:15,584][38991] Updated weights for policy 0, policy_version 5498 (0.0020)
[2026-01-17 17:20:16,125][38991] Updated weights for policy 0, policy_version 5508 (0.0015)
[2026-01-17 17:20:16,611][38991] Updated weights for policy 0, policy_version 5518 (0.0011)
[2026-01-17 17:20:17,278][38991] Updated weights for policy 0, policy_version 5528 (0.0015)
[2026-01-17 17:20:17,761][38991] Updated weights for policy 0, policy_version 5538 (0.0013)
[2026-01-17 17:20:17,886][38936] Signal inference workers to stop experience collection... (2950 times)
[2026-01-17 17:20:17,901][38936] Signal inference workers to resume experience collection... (2950 times)
[2026-01-17 17:20:17,912][38991] InferenceWorker_p0-w0: stopping experience collection (2950 times)
[2026-01-17 17:20:17,928][38991] InferenceWorker_p0-w0: resuming experience collection (2950 times)
[2026-01-17 17:20:18,274][38991] Updated weights for policy 0, policy_version 5548 (0.0014)
[2026-01-17 17:20:18,702][38814] Fps is (10 sec: 75776.4, 60 sec: 74888.4, 300 sec: 66969.3). Total num frames: 22757376. Throughput: 0: 18617.6. Samples: 4698612. Policy #0 lag: (min: 2.0, avg: 12.7, max: 27.0)
[2026-01-17 17:20:18,702][38814] Avg episode reward: [(0, '28.203')]
[2026-01-17 17:20:18,787][38991] Updated weights for policy 0, policy_version 5558 (0.0015)
[2026-01-17 17:20:19,420][38991] Updated weights for policy 0, policy_version 5568 (0.0014)
[2026-01-17 17:20:19,865][38991] Updated weights for policy 0, policy_version 5578 (0.0013)
[2026-01-17 17:20:20,465][38991] Updated weights for policy 0, policy_version 5588 (0.0012)
[2026-01-17 17:20:20,980][38991] Updated weights for policy 0, policy_version 5598 (0.0016)
[2026-01-17 17:20:21,504][38991] Updated weights for policy 0, policy_version 5608 (0.0015)
[2026-01-17 17:20:21,940][38936] Signal inference workers to stop experience collection... (3000 times)
[2026-01-17 17:20:21,948][38991] InferenceWorker_p0-w0: stopping experience collection (3000 times)
[2026-01-17 17:20:21,955][38936] Signal inference workers to resume experience collection... (3000 times)
[2026-01-17 17:20:21,964][38991] InferenceWorker_p0-w0: resuming experience collection (3000 times)
[2026-01-17 17:20:22,051][38991] Updated weights for policy 0, policy_version 5618 (0.0012)
[2026-01-17 17:20:22,654][38991] Updated weights for policy 0, policy_version 5628 (0.0013)
[2026-01-17 17:20:23,157][38991] Updated weights for policy 0, policy_version 5638 (0.0011)
[2026-01-17 17:20:23,638][38991] Updated weights for policy 0, policy_version 5648 (0.0013)
[2026-01-17 17:20:23,702][38814] Fps is (10 sec: 75773.4, 60 sec: 74751.6, 300 sec: 67116.6). Total num frames: 23134208. Throughput: 0: 18596.8. Samples: 4755336. Policy #0 lag: (min: 1.0, avg: 12.0, max: 26.0)
[2026-01-17 17:20:23,703][38814] Avg episode reward: [(0, '27.376')]
[2026-01-17 17:20:24,227][38991] Updated weights for policy 0, policy_version 5658 (0.0012)
[2026-01-17 17:20:24,753][38991] Updated weights for policy 0, policy_version 5668 (0.0011)
[2026-01-17 17:20:25,328][38991] Updated weights for policy 0, policy_version 5678 (0.0014)
[2026-01-17 17:20:25,904][38991] Updated weights for policy 0, policy_version 5688 (0.0015)
[2026-01-17 17:20:26,206][38936] Signal inference workers to stop experience collection... (3050 times)
[2026-01-17 17:20:26,220][38936] Signal inference workers to resume experience collection... (3050 times)
[2026-01-17 17:20:26,239][38991] InferenceWorker_p0-w0: stopping experience collection (3050 times)
[2026-01-17 17:20:26,255][38991] InferenceWorker_p0-w0: resuming experience collection (3050 times)
[2026-01-17 17:20:26,385][38991] Updated weights for policy 0, policy_version 5698 (0.0012)
[2026-01-17 17:20:27,021][38991] Updated weights for policy 0, policy_version 5708 (0.0015)
[2026-01-17 17:20:27,624][38991] Updated weights for policy 0, policy_version 5718 (0.0013)
[2026-01-17 17:20:28,078][38991] Updated weights for policy 0, policy_version 5728 (0.0013)
[2026-01-17 17:20:28,581][38991] Updated weights for policy 0, policy_version 5738 (0.0011)
[2026-01-17 17:20:28,702][38814] Fps is (10 sec: 74545.2, 60 sec: 74683.3, 300 sec: 67230.6). Total num frames: 23502848. Throughput: 0: 18619.7. Samples: 4868892. Policy #0 lag: (min: 2.0, avg: 18.7, max: 33.0)
[2026-01-17 17:20:28,703][38814] Avg episode reward: [(0, '25.761')]
[2026-01-17 17:20:29,164][38991] Updated weights for policy 0, policy_version 5748 (0.0018)
[2026-01-17 17:20:29,750][38991] Updated weights for policy 0, policy_version 5758 (0.0017)
[2026-01-17 17:20:29,884][38936] Signal inference workers to stop experience collection... (3100 times)
[2026-01-17 17:20:29,896][38991] InferenceWorker_p0-w0: stopping experience collection (3100 times)
[2026-01-17 17:20:29,895][38936] Signal inference workers to resume experience collection... (3100 times)
[2026-01-17 17:20:29,915][38991] InferenceWorker_p0-w0: resuming experience collection (3100 times)
[2026-01-17 17:20:30,369][38991] Updated weights for policy 0, policy_version 5768 (0.0014)
[2026-01-17 17:20:30,753][38991] Updated weights for policy 0, policy_version 5778 (0.0016)
[2026-01-17 17:20:31,450][38991] Updated weights for policy 0, policy_version 5788 (0.0012)
[2026-01-17 17:20:31,932][38991] Updated weights for policy 0, policy_version 5798 (0.0014)
[2026-01-17 17:20:32,426][38991] Updated weights for policy 0, policy_version 5808 (0.0015)
[2026-01-17 17:20:33,161][38991] Updated weights for policy 0, policy_version 5818 (0.0014)
[2026-01-17 17:20:33,545][38991] Updated weights for policy 0, policy_version 5828 (0.0013)
[2026-01-17 17:20:33,702][38814] Fps is (10 sec: 74959.6, 60 sec: 74615.6, 300 sec: 67382.5). Total num frames: 23883776. Throughput: 0: 18552.1. Samples: 4979712. Policy #0 lag: (min: 4.0, avg: 15.9, max: 34.0)
[2026-01-17 17:20:33,703][38814] Avg episode reward: [(0, '27.872')]
[2026-01-17 17:20:33,754][38936] Signal inference workers to stop experience collection... (3150 times)
[2026-01-17 17:20:33,770][38936] Signal inference workers to resume experience collection... (3150 times)
[2026-01-17 17:20:33,771][38991] InferenceWorker_p0-w0: stopping experience collection (3150 times)
[2026-01-17 17:20:33,788][38991] InferenceWorker_p0-w0: resuming experience collection (3150 times)
[2026-01-17 17:20:34,129][38991] Updated weights for policy 0, policy_version 5838 (0.0014)
[2026-01-17 17:20:34,737][38991] Updated weights for policy 0, policy_version 5848 (0.0014)
[2026-01-17 17:20:35,119][38991] Updated weights for policy 0, policy_version 5858 (0.0014)
[2026-01-17 17:20:35,786][38991] Updated weights for policy 0, policy_version 5868 (0.0012)
[2026-01-17 17:20:36,236][38991] Updated weights for policy 0, policy_version 5878 (0.0013)
[2026-01-17 17:20:36,926][38991] Updated weights for policy 0, policy_version 5888 (0.0015)
[2026-01-17 17:20:37,367][38991] Updated weights for policy 0, policy_version 5898 (0.0011)
[2026-01-17 17:20:37,888][38991] Updated weights for policy 0, policy_version 5908 (0.0014)
[2026-01-17 17:20:38,122][38936] Signal inference workers to stop experience collection... (3200 times)
[2026-01-17 17:20:38,124][38991] InferenceWorker_p0-w0: stopping experience collection (3200 times)
[2026-01-17 17:20:38,129][38936] Signal inference workers to resume experience collection... (3200 times)
[2026-01-17 17:20:38,140][38991] InferenceWorker_p0-w0: resuming experience collection (3200 times)
[2026-01-17 17:20:38,605][38991] Updated weights for policy 0, policy_version 5918 (0.0013)
[2026-01-17 17:20:38,702][38814] Fps is (10 sec: 74550.9, 60 sec: 74479.6, 300 sec: 68590.6). Total num frames: 24248320. Throughput: 0: 18509.3. Samples: 5035860. Policy #0 lag: (min: 4.0, avg: 15.9, max: 34.0)
[2026-01-17 17:20:38,702][38814] Avg episode reward: [(0, '29.119')]
[2026-01-17 17:20:39,045][38991] Updated weights for policy 0, policy_version 5928 (0.0015)
[2026-01-17 17:20:39,633][38991] Updated weights for policy 0, policy_version 5938 (0.0016)
[2026-01-17 17:20:40,189][38991] Updated weights for policy 0, policy_version 5948 (0.0014)
[2026-01-17 17:20:40,556][38991] Updated weights for policy 0, policy_version 5958 (0.0013)
[2026-01-17 17:20:41,231][38991] Updated weights for policy 0, policy_version 5968 (0.0016)
[2026-01-17 17:20:41,749][38936] Signal inference workers to stop experience collection... (3250 times)
[2026-01-17 17:20:41,767][38936] Signal inference workers to resume experience collection... (3250 times)
[2026-01-17 17:20:41,771][38991] InferenceWorker_p0-w0: stopping experience collection (3250 times)
[2026-01-17 17:20:41,790][38991] InferenceWorker_p0-w0: resuming experience collection (3250 times)
[2026-01-17 17:20:41,810][38991] Updated weights for policy 0, policy_version 5978 (0.0014)
[2026-01-17 17:20:42,166][38991] Updated weights for policy 0, policy_version 5988 (0.0014)
[2026-01-17 17:20:42,840][38991] Updated weights for policy 0, policy_version 5998 (0.0016)
[2026-01-17 17:20:43,309][38991] Updated weights for policy 0, policy_version 6008 (0.0016)
[2026-01-17 17:20:43,702][38814] Fps is (10 sec: 75363.1, 60 sec: 74820.0, 300 sec: 69909.6). Total num frames: 24637440. Throughput: 0: 18578.0. Samples: 5150064. Policy #0 lag: (min: 0.0, avg: 14.9, max: 31.0)
[2026-01-17 17:20:43,703][38814] Avg episode reward: [(0, '29.217')]
[2026-01-17 17:20:43,856][38991] Updated weights for policy 0, policy_version 6018 (0.0011)
[2026-01-17 17:20:44,490][38991] Updated weights for policy 0, policy_version 6028 (0.0016)
[2026-01-17 17:20:44,919][38991] Updated weights for policy 0, policy_version 6038 (0.0016)
[2026-01-17 17:20:45,585][38991] Updated weights for policy 0, policy_version 6048 (0.0012)
[2026-01-17 17:20:45,808][38936] Signal inference workers to stop experience collection... (3300 times)
[2026-01-17 17:20:45,825][38936] Signal inference workers to resume experience collection... (3300 times)
[2026-01-17 17:20:45,830][38991] InferenceWorker_p0-w0: stopping experience collection (3300 times)
[2026-01-17 17:20:45,846][38991] InferenceWorker_p0-w0: resuming experience collection (3300 times)
[2026-01-17 17:20:46,058][38991] Updated weights for policy 0, policy_version 6058 (0.0014)
[2026-01-17 17:20:46,646][38991] Updated weights for policy 0, policy_version 6068 (0.0015)
[2026-01-17 17:20:47,174][38991] Updated weights for policy 0, policy_version 6078 (0.0014)
[2026-01-17 17:20:47,634][38991] Updated weights for policy 0, policy_version 6088 (0.0015)
[2026-01-17 17:20:48,387][38991] Updated weights for policy 0, policy_version 6098 (0.0013)
[2026-01-17 17:20:48,702][38814] Fps is (10 sec: 74956.0, 60 sec: 74274.5, 300 sec: 71131.5). Total num frames: 24997888. Throughput: 0: 18644.6. Samples: 5262324. Policy #0 lag: (min: 1.0, avg: 16.2, max: 33.0)
[2026-01-17 17:20:48,702][38814] Avg episode reward: [(0, '26.904')]
[2026-01-17 17:20:48,868][38991] Updated weights for policy 0, policy_version 6108 (0.0015)
[2026-01-17 17:20:49,386][38991] Updated weights for policy 0, policy_version 6118 (0.0014)
[2026-01-17 17:20:49,948][38991] Updated weights for policy 0, policy_version 6128 (0.0013)
[2026-01-17 17:20:50,017][38936] Signal inference workers to stop experience collection... (3350 times)
[2026-01-17 17:20:50,027][38936] Signal inference workers to resume experience collection... (3350 times)
[2026-01-17 17:20:50,050][38991] InferenceWorker_p0-w0: stopping experience collection (3350 times)
[2026-01-17 17:20:50,051][38991] InferenceWorker_p0-w0: resuming experience collection (3350 times)
[2026-01-17 17:20:50,446][38991] Updated weights for policy 0, policy_version 6138 (0.0012)
[2026-01-17 17:20:51,079][38991] Updated weights for policy 0, policy_version 6148 (0.0014)
[2026-01-17 17:20:51,518][38991] Updated weights for policy 0, policy_version 6158 (0.0013)
[2026-01-17 17:20:52,222][38991] Updated weights for policy 0, policy_version 6168 (0.0012)
[2026-01-17 17:20:52,646][38991] Updated weights for policy 0, policy_version 6178 (0.0014)
[2026-01-17 17:20:53,236][38991] Updated weights for policy 0, policy_version 6188 (0.0016)
[2026-01-17 17:20:53,702][38814] Fps is (10 sec: 73321.6, 60 sec: 74138.0, 300 sec: 72395.0). Total num frames: 25370624. Throughput: 0: 18711.3. Samples: 5318112. Policy #0 lag: (min: 3.0, avg: 15.0, max: 31.0)
[2026-01-17 17:20:53,703][38814] Avg episode reward: [(0, '27.447')]
[2026-01-17 17:20:53,897][38991] Updated weights for policy 0, policy_version 6198 (0.0012)
[2026-01-17 17:20:54,191][38936] Signal inference workers to stop experience collection... (3400 times)
[2026-01-17 17:20:54,197][38991] InferenceWorker_p0-w0: stopping experience collection (3400 times)
[2026-01-17 17:20:54,207][38936] Signal inference workers to resume experience collection... (3400 times)
[2026-01-17 17:20:54,211][38991] InferenceWorker_p0-w0: resuming experience collection (3400 times)
[2026-01-17 17:20:54,285][38991] Updated weights for policy 0, policy_version 6208 (0.0011)
[2026-01-17 17:20:54,967][38991] Updated weights for policy 0, policy_version 6218 (0.0013)
[2026-01-17 17:20:55,399][38991] Updated weights for policy 0, policy_version 6228 (0.0013)
[2026-01-17 17:20:55,905][38991] Updated weights for policy 0, policy_version 6238 (0.0013)
[2026-01-17 17:20:56,630][38991] Updated weights for policy 0, policy_version 6248 (0.0011)
[2026-01-17 17:20:57,115][38991] Updated weights for policy 0, policy_version 6258 (0.0012)
[2026-01-17 17:20:57,525][38991] Updated weights for policy 0, policy_version 6268 (0.0014)
[2026-01-17 17:20:58,125][38991] Updated weights for policy 0, policy_version 6278 (0.0013)
[2026-01-17 17:20:58,151][38936] Signal inference workers to stop experience collection... (3450 times)
[2026-01-17 17:20:58,159][38991] InferenceWorker_p0-w0: stopping experience collection (3450 times)
[2026-01-17 17:20:58,167][38936] Signal inference workers to resume experience collection... (3450 times)
[2026-01-17 17:20:58,172][38991] InferenceWorker_p0-w0: resuming experience collection (3450 times)
[2026-01-17 17:20:58,688][38991] Updated weights for policy 0, policy_version 6288 (0.0013)
[2026-01-17 17:20:58,702][38814] Fps is (10 sec: 75775.9, 60 sec: 74410.8, 300 sec: 73700.2). Total num frames: 25755648. Throughput: 0: 18733.6. Samples: 5430324. Policy #0 lag: (min: 3.0, avg: 19.3, max: 37.0)
[2026-01-17 17:20:58,702][38814] Avg episode reward: [(0, '25.771')]
[2026-01-17 17:20:59,230][38991] Updated weights for policy 0, policy_version 6298 (0.0012)
[2026-01-17 17:20:59,664][38991] Updated weights for policy 0, policy_version 6308 (0.0012)
[2026-01-17 17:21:00,354][38991] Updated weights for policy 0, policy_version 6318 (0.0011)
[2026-01-17 17:21:00,810][38991] Updated weights for policy 0, policy_version 6328 (0.0017)
[2026-01-17 17:21:01,286][38991] Updated weights for policy 0, policy_version 6338 (0.0013)
[2026-01-17 17:21:01,874][38991] Updated weights for policy 0, policy_version 6348 (0.0013)
[2026-01-17 17:21:02,297][38936] Signal inference workers to stop experience collection... (3500 times)
[2026-01-17 17:21:02,298][38936] Signal inference workers to resume experience collection... (3500 times)
[2026-01-17 17:21:02,326][38991] InferenceWorker_p0-w0: stopping experience collection (3500 times)
[2026-01-17 17:21:02,327][38991] InferenceWorker_p0-w0: resuming experience collection (3500 times)
[2026-01-17 17:21:02,419][38991] Updated weights for policy 0, policy_version 6358 (0.0012)
[2026-01-17 17:21:02,919][38991] Updated weights for policy 0, policy_version 6368 (0.0016)
[2026-01-17 17:21:03,516][38991] Updated weights for policy 0, policy_version 6378 (0.0015)
[2026-01-17 17:21:03,702][38814] Fps is (10 sec: 76595.6, 60 sec: 74752.1, 300 sec: 74533.3). Total num frames: 26136576. Throughput: 0: 18799.3. Samples: 5544576. Policy #0 lag: (min: 0.0, avg: 14.5, max: 26.0)
[2026-01-17 17:21:03,703][38814] Avg episode reward: [(0, '24.599')]
[2026-01-17 17:21:04,056][38991] Updated weights for policy 0, policy_version 6388 (0.0014)
[2026-01-17 17:21:04,578][38991] Updated weights for policy 0, policy_version 6398 (0.0014)
[2026-01-17 17:21:05,134][38991] Updated weights for policy 0, policy_version 6408 (0.0012)
[2026-01-17 17:21:05,639][38991] Updated weights for policy 0, policy_version 6418 (0.0011)
[2026-01-17 17:21:06,154][38936] Signal inference workers to stop experience collection... (3550 times)
[2026-01-17 17:21:06,168][38936] Signal inference workers to resume experience collection... (3550 times)
[2026-01-17 17:21:06,175][38991] InferenceWorker_p0-w0: stopping experience collection (3550 times)
[2026-01-17 17:21:06,193][38991] InferenceWorker_p0-w0: resuming experience collection (3550 times)
[2026-01-17 17:21:06,195][38991] Updated weights for policy 0, policy_version 6428 (0.0011)
[2026-01-17 17:21:06,730][38991] Updated weights for policy 0, policy_version 6438 (0.0015)
[2026-01-17 17:21:07,231][38991] Updated weights for policy 0, policy_version 6448 (0.0016)
[2026-01-17 17:21:07,870][38991] Updated weights for policy 0, policy_version 6458 (0.0012)
[2026-01-17 17:21:08,293][38991] Updated weights for policy 0, policy_version 6468 (0.0015)
[2026-01-17 17:21:08,702][38814] Fps is (10 sec: 76184.9, 60 sec: 75298.2, 300 sec: 74963.7). Total num frames: 26517504. Throughput: 0: 18809.5. Samples: 5601756. Policy #0 lag: (min: 0.0, avg: 13.7, max: 30.0)
[2026-01-17 17:21:08,704][38814] Avg episode reward: [(0, '29.875')]
[2026-01-17 17:21:08,991][38991] Updated weights for policy 0, policy_version 6478 (0.0013)
[2026-01-17 17:21:09,530][38991] Updated weights for policy 0, policy_version 6488 (0.0013)
[2026-01-17 17:21:09,954][38991] Updated weights for policy 0, policy_version 6498 (0.0018)
[2026-01-17 17:21:10,660][38936] Signal inference workers to stop experience collection... (3600 times)
[2026-01-17 17:21:10,679][38936] Signal inference workers to resume experience collection... (3600 times)
[2026-01-17 17:21:10,686][38991] InferenceWorker_p0-w0: stopping experience collection (3600 times)
[2026-01-17 17:21:10,687][38991] Updated weights for policy 0, policy_version 6508 (0.0013)
[2026-01-17 17:21:10,705][38991] InferenceWorker_p0-w0: resuming experience collection (3600 times)
[2026-01-17 17:21:11,068][38991] Updated weights for policy 0, policy_version 6518 (0.0016)
[2026-01-17 17:21:11,551][38991] Updated weights for policy 0, policy_version 6528 (0.0014)
[2026-01-17 17:21:12,199][38991] Updated weights for policy 0, policy_version 6538 (0.0013)
[2026-01-17 17:21:12,678][38991] Updated weights for policy 0, policy_version 6548 (0.0012)
[2026-01-17 17:21:13,267][38991] Updated weights for policy 0, policy_version 6558 (0.0014)
[2026-01-17 17:21:13,702][38814] Fps is (10 sec: 75774.6, 60 sec: 75298.0, 300 sec: 75116.4). Total num frames: 26894336. Throughput: 0: 18788.9. Samples: 5714388. Policy #0 lag: (min: 0.0, avg: 16.3, max: 33.0)
[2026-01-17 17:21:13,703][38814] Avg episode reward: [(0, '29.932')]
[2026-01-17 17:21:13,793][38991] Updated weights for policy 0, policy_version 6568 (0.0013)
[2026-01-17 17:21:14,324][38991] Updated weights for policy 0, policy_version 6578 (0.0012)
[2026-01-17 17:21:14,829][38936] Signal inference workers to stop experience collection... (3650 times)
[2026-01-17 17:21:14,841][38936] Signal inference workers to resume experience collection... (3650 times)
[2026-01-17 17:21:14,861][38991] InferenceWorker_p0-w0: stopping experience collection (3650 times)
[2026-01-17 17:21:14,861][38991] InferenceWorker_p0-w0: resuming experience collection (3650 times)
[2026-01-17 17:21:14,884][38991] Updated weights for policy 0, policy_version 6588 (0.0016)
[2026-01-17 17:21:15,444][38991] Updated weights for policy 0, policy_version 6598 (0.0012)
[2026-01-17 17:21:16,076][38991] Updated weights for policy 0, policy_version 6608 (0.0013)
[2026-01-17 17:21:16,496][38991] Updated weights for policy 0, policy_version 6618 (0.0015)
[2026-01-17 17:21:17,079][38991] Updated weights for policy 0, policy_version 6628 (0.0013)
[2026-01-17 17:21:17,719][38991] Updated weights for policy 0, policy_version 6638 (0.0015)
[2026-01-17 17:21:18,104][38991] Updated weights for policy 0, policy_version 6648 (0.0016)
[2026-01-17 17:21:18,702][38814] Fps is (10 sec: 73727.9, 60 sec: 74956.8, 300 sec: 75102.6). Total num frames: 27254784. Throughput: 0: 18847.2. Samples: 5827836. Policy #0 lag: (min: 0.0, avg: 16.3, max: 33.0)
[2026-01-17 17:21:18,702][38814] Avg episode reward: [(0, '25.953')]
[2026-01-17 17:21:18,801][38936] Signal inference workers to stop experience collection... (3700 times)
[2026-01-17 17:21:18,816][38936] Signal inference workers to resume experience collection... (3700 times)
[2026-01-17 17:21:18,835][38991] InferenceWorker_p0-w0: stopping experience collection (3700 times)
[2026-01-17 17:21:18,837][38991] InferenceWorker_p0-w0: resuming experience collection (3700 times)
[2026-01-17 17:21:18,856][38991] Updated weights for policy 0, policy_version 6658 (0.0014)
[2026-01-17 17:21:19,274][38991] Updated weights for policy 0, policy_version 6668 (0.0015)
[2026-01-17 17:21:19,659][38991] Updated weights for policy 0, policy_version 6678 (0.0014)
[2026-01-17 17:21:20,444][38991] Updated weights for policy 0, policy_version 6688 (0.0012)
[2026-01-17 17:21:20,808][38991] Updated weights for policy 0, policy_version 6698 (0.0019)
[2026-01-17 17:21:21,585][38991] Updated weights for policy 0, policy_version 6708 (0.0013)
[2026-01-17 17:21:21,885][38936] Signal inference workers to stop experience collection... (3750 times)
[2026-01-17 17:21:21,900][38936] Signal inference workers to resume experience collection... (3750 times)
[2026-01-17 17:21:21,904][38991] InferenceWorker_p0-w0: stopping experience collection (3750 times)
[2026-01-17 17:21:21,924][38991] InferenceWorker_p0-w0: resuming experience collection (3750 times)
[2026-01-17 17:21:21,981][38991] Updated weights for policy 0, policy_version 6718 (0.0013)
[2026-01-17 17:21:22,449][38991] Updated weights for policy 0, policy_version 6728 (0.0016)
[2026-01-17 17:21:23,033][38991] Updated weights for policy 0, policy_version 6738 (0.0014)
[2026-01-17 17:21:23,557][38991] Updated weights for policy 0, policy_version 6748 (0.0012)
[2026-01-17 17:21:23,702][38814] Fps is (10 sec: 75778.0, 60 sec: 75298.8, 300 sec: 75213.7). Total num frames: 27652096. Throughput: 0: 18850.9. Samples: 5884152. Policy #0 lag: (min: 1.0, avg: 12.5, max: 29.0)
[2026-01-17 17:21:23,702][38814] Avg episode reward: [(0, '27.963')]
[2026-01-17 17:21:24,240][38991] Updated weights for policy 0, policy_version 6758 (0.0014)
[2026-01-17 17:21:24,631][38991] Updated weights for policy 0, policy_version 6768 (0.0014)
[2026-01-17 17:21:25,190][38991] Updated weights for policy 0, policy_version 6778 (0.0014)
[2026-01-17 17:21:25,750][38991] Updated weights for policy 0, policy_version 6788 (0.0013)
[2026-01-17 17:21:25,874][38936] Signal inference workers to stop experience collection... (3800 times)
[2026-01-17 17:21:25,883][38936] Signal inference workers to resume experience collection... (3800 times)
[2026-01-17 17:21:25,900][38991] InferenceWorker_p0-w0: stopping experience collection (3800 times)
[2026-01-17 17:21:25,915][38991] InferenceWorker_p0-w0: resuming experience collection (3800 times)
[2026-01-17 17:21:26,221][38991] Updated weights for policy 0, policy_version 6798 (0.0013)
[2026-01-17 17:21:26,900][38991] Updated weights for policy 0, policy_version 6808 (0.0015)
[2026-01-17 17:21:27,291][38991] Updated weights for policy 0, policy_version 6818 (0.0015)
[2026-01-17 17:21:27,786][38991] Updated weights for policy 0, policy_version 6828 (0.0016)
[2026-01-17 17:21:28,439][38991] Updated weights for policy 0, policy_version 6838 (0.0014)
[2026-01-17 17:21:28,702][38814] Fps is (10 sec: 77003.6, 60 sec: 75366.5, 300 sec: 75199.9). Total num frames: 28024832. Throughput: 0: 18845.7. Samples: 5998116. Policy #0 lag: (min: 0.0, avg: 16.1, max: 32.0)
[2026-01-17 17:21:28,702][38814] Avg episode reward: [(0, '26.341')]
[2026-01-17 17:21:28,707][38936] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000006842_28024832.pth...
[2026-01-17 17:21:28,799][38936] Removing train_dir/default_experiment/checkpoint_p0/checkpoint_000002425_9932800.pth
[2026-01-17 17:21:29,015][38991] Updated weights for policy 0, policy_version 6848 (0.0014)
[2026-01-17 17:21:29,493][38991] Updated weights for policy 0, policy_version 6858 (0.0016)
[2026-01-17 17:21:30,079][38936] Signal inference workers to stop experience collection... (3850 times)
[2026-01-17 17:21:30,081][38936] Signal inference workers to resume experience collection... (3850 times)
[2026-01-17 17:21:30,113][38991] InferenceWorker_p0-w0: stopping experience collection (3850 times)
[2026-01-17 17:21:30,113][38991] InferenceWorker_p0-w0: resuming experience collection (3850 times)
[2026-01-17 17:21:30,129][38991] Updated weights for policy 0, policy_version 6868 (0.0016)
[2026-01-17 17:21:30,559][38991] Updated weights for policy 0, policy_version 6878 (0.0013)
[2026-01-17 17:21:31,115][38991] Updated weights for policy 0, policy_version 6888 (0.0014)
[2026-01-17 17:21:31,640][38991] Updated weights for policy 0, policy_version 6898 (0.0015)
[2026-01-17 17:21:32,183][38991] Updated weights for policy 0, policy_version 6908 (0.0013)
[2026-01-17 17:21:32,822][38991] Updated weights for policy 0, policy_version 6918 (0.0018)
[2026-01-17 17:21:33,288][38991] Updated weights for policy 0, policy_version 6928 (0.0014)
[2026-01-17 17:21:33,702][38814] Fps is (10 sec: 75365.3, 60 sec: 75366.4, 300 sec: 75241.4). Total num frames: 28405760. Throughput: 0: 18866.4. Samples: 6111312. Policy #0 lag: (min: 2.0, avg: 13.4, max: 29.0)
[2026-01-17 17:21:33,702][38814] Avg episode reward: [(0, '27.956')]
[2026-01-17 17:21:33,834][38936] Signal inference workers to stop experience collection... (3900 times)
[2026-01-17 17:21:33,840][38991] InferenceWorker_p0-w0: stopping experience collection (3900 times)
[2026-01-17 17:21:33,849][38936] Signal inference workers to resume experience collection... (3900 times)
[2026-01-17 17:21:33,856][38991] InferenceWorker_p0-w0: resuming experience collection (3900 times)
[2026-01-17 17:21:33,876][38991] Updated weights for policy 0, policy_version 6938 (0.0013)
[2026-01-17 17:21:34,425][38991] Updated weights for policy 0, policy_version 6948 (0.0017)
[2026-01-17 17:21:34,890][38991] Updated weights for policy 0, policy_version 6958 (0.0011)
[2026-01-17 17:21:35,514][38991] Updated weights for policy 0, policy_version 6968 (0.0013)
[2026-01-17 17:21:36,003][38991] Updated weights for policy 0, policy_version 6978 (0.0012)
[2026-01-17 17:21:36,637][38991] Updated weights for policy 0, policy_version 6988 (0.0015)
[2026-01-17 17:21:37,093][38991] Updated weights for policy 0, policy_version 6998 (0.0014)
[2026-01-17 17:21:37,705][38936] Signal inference workers to stop experience collection... (3950 times)
[2026-01-17 17:21:37,721][38936] Signal inference workers to resume experience collection... (3950 times)
[2026-01-17 17:21:37,724][38991] Updated weights for policy 0, policy_version 7008 (0.0016)
[2026-01-17 17:21:37,743][38991] InferenceWorker_p0-w0: stopping experience collection (3950 times)
[2026-01-17 17:21:37,745][38991] InferenceWorker_p0-w0: resuming experience collection (3950 times)
[2026-01-17 17:21:38,245][38991] Updated weights for policy 0, policy_version 7018 (0.0017)
[2026-01-17 17:21:38,642][38991] Updated weights for policy 0, policy_version 7028 (0.0011)
[2026-01-17 17:21:38,702][38814] Fps is (10 sec: 76188.4, 60 sec: 75639.4, 300 sec: 75241.9). Total num frames: 28786688. Throughput: 0: 18884.6. Samples: 6167916. Policy #0 lag: (min: 4.0, avg: 16.2, max: 34.0)
[2026-01-17 17:21:38,702][38814] Avg episode reward: [(0, '28.919')]
[2026-01-17 17:21:39,329][38991] Updated weights for policy 0, policy_version 7038 (0.0016)
[2026-01-17 17:21:39,840][38991] Updated weights for policy 0, policy_version 7048 (0.0021)
[2026-01-17 17:21:40,490][38991] Updated weights for policy 0, policy_version 7058 (0.0012)
[2026-01-17 17:21:40,973][38991] Updated weights for policy 0, policy_version 7068 (0.0016)
[2026-01-17 17:21:41,385][38991] Updated weights for policy 0, policy_version 7078 (0.0013)
[2026-01-17 17:21:41,493][38936] Signal inference workers to stop experience collection... (4000 times)
[2026-01-17 17:21:41,500][38936] Signal inference workers to resume experience collection... (4000 times)
[2026-01-17 17:21:41,523][38991] InferenceWorker_p0-w0: stopping experience collection (4000 times)
[2026-01-17 17:21:41,523][38991] InferenceWorker_p0-w0: resuming experience collection (4000 times)
[2026-01-17 17:21:42,125][38991] Updated weights for policy 0, policy_version 7088 (0.0012)
[2026-01-17 17:21:42,584][38991] Updated weights for policy 0, policy_version 7098 (0.0013)
[2026-01-17 17:21:43,102][38991] Updated weights for policy 0, policy_version 7108 (0.0014)
[2026-01-17 17:21:43,702][38814] Fps is (10 sec: 74546.6, 60 sec: 75230.3, 300 sec: 75185.9). Total num frames: 29151232. Throughput: 0: 18887.1. Samples: 6280248. Policy #0 lag: (min: 1.0, avg: 16.4, max: 31.0)
[2026-01-17 17:21:43,703][38814] Avg episode reward: [(0, '27.871')]
[2026-01-17 17:21:43,739][38991] Updated weights for policy 0, policy_version 7118 (0.0015)
[2026-01-17 17:21:44,115][38991] Updated weights for policy 0, policy_version 7128 (0.0017)
[2026-01-17 17:21:44,791][38991] Updated weights for policy 0, policy_version 7138 (0.0013)
[2026-01-17 17:21:45,283][38991] Updated weights for policy 0, policy_version 7148 (0.0013)
[2026-01-17 17:21:46,037][38991] Updated weights for policy 0, policy_version 7158 (0.0016)
[2026-01-17 17:21:46,140][38936] Signal inference workers to stop experience collection... (4050 times)
[2026-01-17 17:21:46,163][38936] Signal inference workers to resume experience collection... (4050 times)
[2026-01-17 17:21:46,165][38991] InferenceWorker_p0-w0: stopping experience collection (4050 times)
[2026-01-17 17:21:46,183][38991] InferenceWorker_p0-w0: resuming experience collection (4050 times)
[2026-01-17 17:21:46,552][38991] Updated weights for policy 0, policy_version 7168 (0.0017)
[2026-01-17 17:21:47,083][38991] Updated weights for policy 0, policy_version 7178 (0.0013)
[2026-01-17 17:21:47,646][38991] Updated weights for policy 0, policy_version 7188 (0.0013)
[2026-01-17 17:21:48,097][38991] Updated weights for policy 0, policy_version 7198 (0.0013)
[2026-01-17 17:21:48,702][38814] Fps is (10 sec: 72908.5, 60 sec: 75298.2, 300 sec: 75144.3). Total num frames: 29515776. Throughput: 0: 18814.1. Samples: 6391212. Policy #0 lag: (min: 1.0, avg: 16.4, max: 31.0)
[2026-01-17 17:21:48,702][38814] Avg episode reward: [(0, '28.165')]
[2026-01-17 17:21:48,752][38991] Updated weights for policy 0, policy_version 7208 (0.0014)
[2026-01-17 17:21:49,257][38991] Updated weights for policy 0, policy_version 7218 (0.0013)
[2026-01-17 17:21:49,604][38936] Signal inference workers to stop experience collection... (4100 times)
[2026-01-17 17:21:49,622][38936] Signal inference workers to resume experience collection... (4100 times)
[2026-01-17 17:21:49,624][38991] InferenceWorker_p0-w0: stopping experience collection (4100 times)
[2026-01-17 17:21:49,644][38991] InferenceWorker_p0-w0: resuming experience collection (4100 times)
[2026-01-17 17:21:49,693][38991] Updated weights for policy 0, policy_version 7228 (0.0014)
[2026-01-17 17:21:50,269][38991] Updated weights for policy 0, policy_version 7238 (0.0015)
[2026-01-17 17:21:50,878][38991] Updated weights for policy 0, policy_version 7248 (0.0014)
[2026-01-17 17:21:51,341][38991] Updated weights for policy 0, policy_version 7258 (0.0017)
[2026-01-17 17:21:52,061][38991] Updated weights for policy 0, policy_version 7268 (0.0012)
[2026-01-17 17:21:52,549][38991] Updated weights for policy 0, policy_version 7278 (0.0013)
[2026-01-17 17:21:52,920][38991] Updated weights for policy 0, policy_version 7288 (0.0013)
[2026-01-17 17:21:53,499][38936] Signal inference workers to stop experience collection... (4150 times)
[2026-01-17 17:21:53,505][38936] Signal inference workers to resume experience collection... (4150 times)
[2026-01-17 17:21:53,525][38991] InferenceWorker_p0-w0: stopping experience collection (4150 times)
[2026-01-17 17:21:53,525][38991] InferenceWorker_p0-w0: resuming experience collection (4150 times)
[2026-01-17 17:21:53,577][38991] Updated weights for policy 0, policy_version 7298 (0.0014)
[2026-01-17 17:21:53,702][38814] Fps is (10 sec: 74545.3, 60 sec: 75434.2, 300 sec: 75241.4). Total num frames: 29896704. Throughput: 0: 18798.8. Samples: 6447708. Policy #0 lag: (min: 1.0, avg: 14.4, max: 32.0)
[2026-01-17 17:21:53,703][38814] Avg episode reward: [(0, '31.245')]
[2026-01-17 17:21:53,724][38936] Saving new best policy, reward=31.245!
[2026-01-17 17:21:54,084][38991] Updated weights for policy 0, policy_version 7308 (0.0012)
[2026-01-17 17:21:54,596][38991] Updated weights for policy 0, policy_version 7318 (0.0013)
[2026-01-17 17:21:55,329][38991] Updated weights for policy 0, policy_version 7328 (0.0014)
[2026-01-17 17:21:55,955][38991] Updated weights for policy 0, policy_version 7338 (0.0017)
[2026-01-17 17:21:56,369][38991] Updated weights for policy 0, policy_version 7348 (0.0012)
[2026-01-17 17:21:56,969][38991] Updated weights for policy 0, policy_version 7358 (0.0015)
[2026-01-17 17:21:57,089][38936] Signal inference workers to stop experience collection... (4200 times)
[2026-01-17 17:21:57,090][38991] InferenceWorker_p0-w0: stopping experience collection (4200 times)
[2026-01-17 17:21:57,097][38936] Signal inference workers to resume experience collection... (4200 times)
[2026-01-17 17:21:57,106][38991] InferenceWorker_p0-w0: resuming experience collection (4200 times)
[2026-01-17 17:21:57,613][38991] Updated weights for policy 0, policy_version 7368 (0.0011)
[2026-01-17 17:21:58,034][38991] Updated weights for policy 0, policy_version 7378 (0.0021)
[2026-01-17 17:21:58,688][38991] Updated weights for policy 0, policy_version 7388 (0.0012)
[2026-01-17 17:21:58,702][38814] Fps is (10 sec: 74544.8, 60 sec: 75093.0, 300 sec: 75255.3). Total num frames: 30261248. Throughput: 0: 18762.9. Samples: 6558720. Policy #0 lag: (min: 0.0, avg: 14.3, max: 30.0)
[2026-01-17 17:21:58,703][38814] Avg episode reward: [(0, '29.094')]
[2026-01-17 17:21:59,193][38991] Updated weights for policy 0, policy_version 7398 (0.0013)
[2026-01-17 17:21:59,713][38991] Updated weights for policy 0, policy_version 7408 (0.0015)
[2026-01-17 17:22:00,396][38991] Updated weights for policy 0, policy_version 7418 (0.0013)
[2026-01-17 17:22:00,814][38991] Updated weights for policy 0, policy_version 7428 (0.0015)
[2026-01-17 17:22:01,138][38936] Signal inference workers to stop experience collection... (4250 times)
[2026-01-17 17:22:01,138][38936] Signal inference workers to resume experience collection... (4250 times)
[2026-01-17 17:22:01,158][38991] InferenceWorker_p0-w0: stopping experience collection (4250 times)
[2026-01-17 17:22:01,159][38991] InferenceWorker_p0-w0: resuming experience collection (4250 times)
[2026-01-17 17:22:01,352][38991] Updated weights for policy 0, policy_version 7438 (0.0013)
[2026-01-17 17:22:01,971][38991] Updated weights for policy 0, policy_version 7448 (0.0012)
[2026-01-17 17:22:02,482][38991] Updated weights for policy 0, policy_version 7458 (0.0013)
[2026-01-17 17:22:03,131][38991] Updated weights for policy 0, policy_version 7470 (0.0012)
[2026-01-17 17:22:03,702][38814] Fps is (10 sec: 72910.4, 60 sec: 74820.0, 300 sec: 75283.7). Total num frames: 30625792. Throughput: 0: 18681.6. Samples: 6668508. Policy #0 lag: (min: 1.0, avg: 15.0, max: 33.0)
[2026-01-17 17:22:03,703][38814] Avg episode reward: [(0, '28.883')]
[2026-01-17 17:22:03,831][38991] Updated weights for policy 0, policy_version 7480 (0.0012)
[2026-01-17 17:22:04,259][38991] Updated weights for policy 0, policy_version 7490 (0.0012)
[2026-01-17 17:22:04,867][38991] Updated weights for policy 0, policy_version 7500 (0.0013)
[2026-01-17 17:22:05,466][38991] Updated weights for policy 0, policy_version 7510 (0.0014)
[2026-01-17 17:22:05,662][38936] Signal inference workers to stop experience collection... (4300 times)
[2026-01-17 17:22:05,676][38936] Signal inference workers to resume experience collection... (4300 times)
[2026-01-17 17:22:05,691][38991] InferenceWorker_p0-w0: stopping experience collection (4300 times)
[2026-01-17 17:22:05,707][38991] InferenceWorker_p0-w0: resuming experience collection (4300 times)
[2026-01-17 17:22:05,891][38991] Updated weights for policy 0, policy_version 7520 (0.0013)
[2026-01-17 17:22:06,630][38991] Updated weights for policy 0, policy_version 7530 (0.0015)
[2026-01-17 17:22:07,033][38991] Updated weights for policy 0, policy_version 7540 (0.0016)
[2026-01-17 17:22:07,593][38991] Updated weights for policy 0, policy_version 7550 (0.0013)
[2026-01-17 17:22:08,165][38991] Updated weights for policy 0, policy_version 7560 (0.0014)
[2026-01-17 17:22:08,645][38991] Updated weights for policy 0, policy_version 7570 (0.0015)
[2026-01-17 17:22:08,702][38814] Fps is (10 sec: 74958.2, 60 sec: 74888.6, 300 sec: 75283.0). Total num frames: 31010816. Throughput: 0: 18673.0. Samples: 6724440. Policy #0 lag: (min: 1.0, avg: 15.0, max: 33.0)
[2026-01-17 17:22:08,703][38814] Avg episode reward: [(0, '31.400')]
[2026-01-17 17:22:08,705][38936] Saving new best policy, reward=31.400!
[2026-01-17 17:22:09,340][38991] Updated weights for policy 0, policy_version 7580 (0.0015)
[2026-01-17 17:22:09,456][38936] Signal inference workers to stop experience collection... (4350 times)
[2026-01-17 17:22:09,469][38936] Signal inference workers to resume experience collection... (4350 times)
[2026-01-17 17:22:09,487][38991] InferenceWorker_p0-w0: stopping experience collection (4350 times)
[2026-01-17 17:22:09,508][38991] InferenceWorker_p0-w0: resuming experience collection (4350 times)
[2026-01-17 17:22:09,896][38991] Updated weights for policy 0, policy_version 7590 (0.0014)
[2026-01-17 17:22:10,586][38991] Updated weights for policy 0, policy_version 7600 (0.0013)
[2026-01-17 17:22:11,015][38991] Updated weights for policy 0, policy_version 7610 (0.0015)
[2026-01-17 17:22:11,557][38991] Updated weights for policy 0, policy_version 7620 (0.0014)
[2026-01-17 17:22:12,065][38991] Updated weights for policy 0, policy_version 7630 (0.0015)
[2026-01-17 17:22:12,790][38991] Updated weights for policy 0, policy_version 7640 (0.0015)
[2026-01-17 17:22:13,333][38991] Updated weights for policy 0, policy_version 7650 (0.0018)
[2026-01-17 17:22:13,464][38936] Signal inference workers to stop experience collection... (4400 times)
[2026-01-17 17:22:13,481][38936] Signal inference workers to resume experience collection... (4400 times)
[2026-01-17 17:22:13,486][38991] InferenceWorker_p0-w0: stopping experience collection (4400 times)
[2026-01-17 17:22:13,501][38991] InferenceWorker_p0-w0: resuming experience collection (4400 times)
[2026-01-17 17:22:13,702][38814] Fps is (10 sec: 74138.3, 60 sec: 74547.3, 300 sec: 75227.5). Total num frames: 31367168. Throughput: 0: 18551.3. Samples: 6832920. Policy #0 lag: (min: 0.0, avg: 13.9, max: 27.0)
[2026-01-17 17:22:13,702][38814] Avg episode reward: [(0, '28.569')]
[2026-01-17 17:22:13,894][38991] Updated weights for policy 0, policy_version 7660 (0.0011)
[2026-01-17 17:22:14,506][38991] Updated weights for policy 0, policy_version 7670 (0.0013)
[2026-01-17 17:22:15,027][38991] Updated weights for policy 0, policy_version 7680 (0.0016)
[2026-01-17 17:22:15,648][38991] Updated weights for policy 0, policy_version 7690 (0.0014)
[2026-01-17 17:22:16,218][38991] Updated weights for policy 0, policy_version 7700 (0.0014)
[2026-01-17 17:22:16,794][38991] Updated weights for policy 0, policy_version 7710 (0.0014)
[2026-01-17 17:22:17,343][38991] Updated weights for policy 0, policy_version 7720 (0.0012)
[2026-01-17 17:22:17,597][38936] Signal inference workers to stop experience collection... (4450 times)
[2026-01-17 17:22:17,623][38936] Signal inference workers to resume experience collection... (4450 times)
[2026-01-17 17:22:17,625][38991] InferenceWorker_p0-w0: stopping experience collection (4450 times)
[2026-01-17 17:22:17,639][38991] InferenceWorker_p0-w0: resuming experience collection (4450 times)
[2026-01-17 17:22:17,853][38991] Updated weights for policy 0, policy_version 7730 (0.0011)
[2026-01-17 17:22:18,489][38991] Updated weights for policy 0, policy_version 7740 (0.0012)
[2026-01-17 17:22:18,702][38814] Fps is (10 sec: 70861.4, 60 sec: 74410.8, 300 sec: 75144.2). Total num frames: 31719424. Throughput: 0: 18431.0. Samples: 6940704. Policy #0 lag: (min: 0.0, avg: 14.9, max: 29.0)
[2026-01-17 17:22:18,703][38814] Avg episode reward: [(0, '26.289')]
[2026-01-17 17:22:18,958][38991] Updated weights for policy 0, policy_version 7750 (0.0016)
[2026-01-17 17:22:19,492][38991] Updated weights for policy 0, policy_version 7760 (0.0015)
[2026-01-17 17:22:20,130][38991] Updated weights for policy 0, policy_version 7770 (0.0016)
[2026-01-17 17:22:20,525][38991] Updated weights for policy 0, policy_version 7780 (0.0013)
[2026-01-17 17:22:21,347][38991] Updated weights for policy 0, policy_version 7790 (0.0012)
[2026-01-17 17:22:21,712][38936] Signal inference workers to stop experience collection... (4500 times)
[2026-01-17 17:22:21,717][38991] InferenceWorker_p0-w0: stopping experience collection (4500 times)
[2026-01-17 17:22:21,732][38936] Signal inference workers to resume experience collection... (4500 times)
[2026-01-17 17:22:21,733][38991] InferenceWorker_p0-w0: resuming experience collection (4500 times)
[2026-01-17 17:22:21,736][38991] Updated weights for policy 0, policy_version 7800 (0.0015)
[2026-01-17 17:22:22,153][38991] Updated weights for policy 0, policy_version 7810 (0.0016)
[2026-01-17 17:22:22,827][38991] Updated weights for policy 0, policy_version 7820 (0.0013)
[2026-01-17 17:22:23,249][38991] Updated weights for policy 0, policy_version 7830 (0.0013)
[2026-01-17 17:22:23,702][38814] Fps is (10 sec: 72909.5, 60 sec: 74069.2, 300 sec: 75158.2). Total num frames: 32096256. Throughput: 0: 18425.0. Samples: 6997044. Policy #0 lag: (min: 1.0, avg: 17.7, max: 33.0)
[2026-01-17 17:22:23,702][38814] Avg episode reward: [(0, '27.396')]
[2026-01-17 17:22:23,880][38991] Updated weights for policy 0, policy_version 7840 (0.0013)
[2026-01-17 17:22:24,387][38991] Updated weights for policy 0, policy_version 7850 (0.0015)
[2026-01-17 17:22:24,899][38991] Updated weights for policy 0, policy_version 7860 (0.0016)
[2026-01-17 17:22:25,425][38991] Updated weights for policy 0, policy_version 7870 (0.0012)
[2026-01-17 17:22:25,940][38936] Signal inference workers to stop experience collection... (4550 times)
[2026-01-17 17:22:25,953][38936] Signal inference workers to resume experience collection... (4550 times)
[2026-01-17 17:22:25,955][38991] InferenceWorker_p0-w0: stopping experience collection (4550 times)
[2026-01-17 17:22:25,972][38991] InferenceWorker_p0-w0: resuming experience collection (4550 times)
[2026-01-17 17:22:26,023][38991] Updated weights for policy 0, policy_version 7880 (0.0011)
[2026-01-17 17:22:26,500][38991] Updated weights for policy 0, policy_version 7890 (0.0015)
[2026-01-17 17:22:26,984][38991] Updated weights for policy 0, policy_version 7900 (0.0013)
[2026-01-17 17:22:27,676][38991] Updated weights for policy 0, policy_version 7910 (0.0012)
[2026-01-17 17:22:28,060][38991] Updated weights for policy 0, policy_version 7920 (0.0014)
[2026-01-17 17:22:28,702][38814] Fps is (10 sec: 76186.0, 60 sec: 74274.5, 300 sec: 75255.3). Total num frames: 32481280. Throughput: 0: 18483.8. Samples: 7112016. Policy #0 lag: (min: 1.0, avg: 17.7, max: 33.0)
[2026-01-17 17:22:28,702][38814] Avg episode reward: [(0, '29.411')]
[2026-01-17 17:22:28,708][38991] Updated weights for policy 0, policy_version 7930 (0.0013)
[2026-01-17 17:22:29,256][38991] Updated weights for policy 0, policy_version 7940 (0.0012)
[2026-01-17 17:22:29,661][38991] Updated weights for policy 0, policy_version 7950 (0.0015)
[2026-01-17 17:22:29,770][38936] Signal inference workers to stop experience collection... (4600 times)
[2026-01-17 17:22:29,778][38936] Signal inference workers to resume experience collection... (4600 times)
[2026-01-17 17:22:29,785][38991] InferenceWorker_p0-w0: stopping experience collection (4600 times)
[2026-01-17 17:22:29,800][38991] InferenceWorker_p0-w0: resuming experience collection (4600 times)
[2026-01-17 17:22:30,379][38991] Updated weights for policy 0, policy_version 7960 (0.0018)
[2026-01-17 17:22:30,828][38991] Updated weights for policy 0, policy_version 7970 (0.0012)
[2026-01-17 17:22:31,265][38991] Updated weights for policy 0, policy_version 7980 (0.0014)
[2026-01-17 17:22:31,868][38991] Updated weights for policy 0, policy_version 7990 (0.0013)
[2026-01-17 17:22:32,422][38991] Updated weights for policy 0, policy_version 8000 (0.0015)
[2026-01-17 17:22:32,914][38991] Updated weights for policy 0, policy_version 8010 (0.0016)
[2026-01-17 17:22:33,487][38991] Updated weights for policy 0, policy_version 8020 (0.0012)
[2026-01-17 17:22:33,702][38814] Fps is (10 sec: 76183.7, 60 sec: 74205.7, 300 sec: 75310.9). Total num frames: 32858112. Throughput: 0: 18573.5. Samples: 7227024. Policy #0 lag: (min: 2.0, avg: 13.5, max: 30.0)
[2026-01-17 17:22:33,702][38814] Avg episode reward: [(0, '29.492')]
[2026-01-17 17:22:33,928][38936] Signal inference workers to stop experience collection... (4650 times)
[2026-01-17 17:22:33,946][38936] Signal inference workers to resume experience collection... (4650 times)
[2026-01-17 17:22:33,965][38991] InferenceWorker_p0-w0: stopping experience collection (4650 times)
[2026-01-17 17:22:33,980][38991] InferenceWorker_p0-w0: resuming experience collection (4650 times)
[2026-01-17 17:22:33,996][38991] Updated weights for policy 0, policy_version 8030 (0.0015)
[2026-01-17 17:22:34,461][38991] Updated weights for policy 0, policy_version 8040 (0.0015)
[2026-01-17 17:22:35,113][38991] Updated weights for policy 0, policy_version 8050 (0.0014)
[2026-01-17 17:22:35,622][38991] Updated weights for policy 0, policy_version 8060 (0.0012)
[2026-01-17 17:22:36,084][38991] Updated weights for policy 0, policy_version 8070 (0.0014)
[2026-01-17 17:22:36,656][38991] Updated weights for policy 0, policy_version 8080 (0.0013)
[2026-01-17 17:22:37,246][38991] Updated weights for policy 0, policy_version 8090 (0.0013)
[2026-01-17 17:22:37,674][38936] Signal inference workers to stop experience collection... (4700 times)
[2026-01-17 17:22:37,687][38936] Signal inference workers to resume experience collection... (4700 times)
[2026-01-17 17:22:37,692][38991] InferenceWorker_p0-w0: stopping experience collection (4700 times)
[2026-01-17 17:22:37,709][38991] InferenceWorker_p0-w0: resuming experience collection (4700 times)
[2026-01-17 17:22:37,729][38991] Updated weights for policy 0, policy_version 8100 (0.0015)
[2026-01-17 17:22:38,273][38991] Updated weights for policy 0, policy_version 8110 (0.0011)
[2026-01-17 17:22:38,702][38814] Fps is (10 sec: 76594.7, 60 sec: 74342.3, 300 sec: 75338.6). Total num frames: 33247232. Throughput: 0: 18576.2. Samples: 7283628. Policy #0 lag: (min: 2.0, avg: 16.3, max: 35.0)
[2026-01-17 17:22:38,703][38814] Avg episode reward: [(0, '31.075')]
[2026-01-17 17:22:38,867][38991] Updated weights for policy 0, policy_version 8120 (0.0013)
[2026-01-17 17:22:39,357][38991] Updated weights for policy 0, policy_version 8130 (0.0013)
[2026-01-17 17:22:39,946][38991] Updated weights for policy 0, policy_version 8140 (0.0013)
[2026-01-17 17:22:40,478][38991] Updated weights for policy 0, policy_version 8150 (0.0014)
[2026-01-17 17:22:40,991][38991] Updated weights for policy 0, policy_version 8160 (0.0012)
[2026-01-17 17:22:41,573][38991] Updated weights for policy 0, policy_version 8170 (0.0014)
[2026-01-17 17:22:42,215][38991] Updated weights for policy 0, policy_version 8180 (0.0013)
[2026-01-17 17:22:42,282][38936] Signal inference workers to stop experience collection... (4750 times)
[2026-01-17 17:22:42,297][38936] Signal inference workers to resume experience collection... (4750 times)
[2026-01-17 17:22:42,301][38991] InferenceWorker_p0-w0: stopping experience collection (4750 times)
[2026-01-17 17:22:42,316][38991] InferenceWorker_p0-w0: resuming experience collection (4750 times)
[2026-01-17 17:22:42,685][38991] Updated weights for policy 0, policy_version 8190 (0.0019)
[2026-01-17 17:22:43,218][38991] Updated weights for policy 0, policy_version 8200 (0.0012)
[2026-01-17 17:22:43,702][38814] Fps is (10 sec: 76596.1, 60 sec: 74547.2, 300 sec: 75310.8). Total num frames: 33624064. Throughput: 0: 18627.3. Samples: 7396944. Policy #0 lag: (min: 1.0, avg: 13.7, max: 30.0)
[2026-01-17 17:22:43,702][38814] Avg episode reward: [(0, '31.188')]
[2026-01-17 17:22:43,765][38991] Updated weights for policy 0, policy_version 8210 (0.0011)
[2026-01-17 17:22:44,314][38991] Updated weights for policy 0, policy_version 8220 (0.0013)
[2026-01-17 17:22:44,879][38991] Updated weights for policy 0, policy_version 8230 (0.0013)
[2026-01-17 17:22:45,309][38991] Updated weights for policy 0, policy_version 8240 (0.0016)
[2026-01-17 17:22:46,042][38991] Updated weights for policy 0, policy_version 8250 (0.0014)
[2026-01-17 17:22:46,288][38936] Signal inference workers to stop experience collection... (4800 times)
[2026-01-17 17:22:46,301][38936] Signal inference workers to resume experience collection... (4800 times)
[2026-01-17 17:22:46,306][38991] InferenceWorker_p0-w0: stopping experience collection (4800 times)
[2026-01-17 17:22:46,324][38991] InferenceWorker_p0-w0: resuming experience collection (4800 times)
[2026-01-17 17:22:46,419][38991] Updated weights for policy 0, policy_version 8260 (0.0017)
[2026-01-17 17:22:46,903][38991] Updated weights for policy 0, policy_version 8270 (0.0013)
[2026-01-17 17:22:47,572][38991] Updated weights for policy 0, policy_version 8280 (0.0013)
[2026-01-17 17:22:48,013][38991] Updated weights for policy 0, policy_version 8290 (0.0013)
[2026-01-17 17:22:48,621][38991] Updated weights for policy 0, policy_version 8300 (0.0012)
[2026-01-17 17:22:48,702][38814] Fps is (10 sec: 75776.7, 60 sec: 74820.3, 300 sec: 75338.7). Total num frames: 34004992. Throughput: 0: 18727.8. Samples: 7511256. Policy #0 lag: (min: 1.0, avg: 13.7, max: 30.0)
[2026-01-17 17:22:48,702][38814] Avg episode reward: [(0, '30.413')]
[2026-01-17 17:22:49,243][38991] Updated weights for policy 0, policy_version 8310 (0.0011)
[2026-01-17 17:22:49,615][38991] Updated weights for policy 0, policy_version 8320 (0.0013)
[2026-01-17 17:22:50,166][38991] Updated weights for policy 0, policy_version 8330 (0.0014)
[2026-01-17 17:22:50,736][38936] Signal inference workers to stop experience collection... (4850 times)
[2026-01-17 17:22:50,747][38936] Signal inference workers to resume experience collection... (4850 times)
[2026-01-17 17:22:50,766][38991] InferenceWorker_p0-w0: stopping experience collection (4850 times)
[2026-01-17 17:22:50,767][38991] InferenceWorker_p0-w0: resuming experience collection (4850 times)
[2026-01-17 17:22:50,832][38991] Updated weights for policy 0, policy_version 8340 (0.0012)
[2026-01-17 17:22:51,211][38991] Updated weights for policy 0, policy_version 8350 (0.0011)
[2026-01-17 17:22:51,876][38991] Updated weights for policy 0, policy_version 8360 (0.0016)
[2026-01-17 17:22:52,401][38991] Updated weights for policy 0, policy_version 8370 (0.0013)
[2026-01-17 17:22:52,829][38991] Updated weights for policy 0, policy_version 8380 (0.0014)
[2026-01-17 17:22:53,510][38991] Updated weights for policy 0, policy_version 8390 (0.0016)
[2026-01-17 17:22:53,702][38814] Fps is (10 sec: 76186.3, 60 sec: 74820.7, 300 sec: 75352.5). Total num frames: 34385920. Throughput: 0: 18757.6. Samples: 7568532. Policy #0 lag: (min: 2.0, avg: 12.3, max: 29.0)
[2026-01-17 17:22:53,702][38814] Avg episode reward: [(0, '30.699')]
[2026-01-17 17:22:53,962][38991] Updated weights for policy 0, policy_version 8400 (0.0016)
[2026-01-17 17:22:54,265][38936] Signal inference workers to stop experience collection... (4900 times)
[2026-01-17 17:22:54,280][38936] Signal inference workers to resume experience collection... (4900 times)
[2026-01-17 17:22:54,286][38991] InferenceWorker_p0-w0: stopping experience collection (4900 times)
[2026-01-17 17:22:54,303][38991] InferenceWorker_p0-w0: resuming experience collection (4900 times)
[2026-01-17 17:22:54,512][38991] Updated weights for policy 0, policy_version 8410 (0.0012)
[2026-01-17 17:22:55,026][38991] Updated weights for policy 0, policy_version 8420 (0.0012)
[2026-01-17 17:22:55,534][38991] Updated weights for policy 0, policy_version 8430 (0.0012)
[2026-01-17 17:22:56,181][38991] Updated weights for policy 0, policy_version 8440 (0.0014)
[2026-01-17 17:22:56,570][38991] Updated weights for policy 0, policy_version 8450 (0.0017)
[2026-01-17 17:22:57,192][38991] Updated weights for policy 0, policy_version 8460 (0.0013)
[2026-01-17 17:22:57,788][38991] Updated weights for policy 0, policy_version 8470 (0.0013)
[2026-01-17 17:22:58,168][38991] Updated weights for policy 0, policy_version 8480 (0.0012)
[2026-01-17 17:22:58,702][38814] Fps is (10 sec: 75365.0, 60 sec: 74957.0, 300 sec: 75352.5). Total num frames: 34758656. Throughput: 0: 18878.9. Samples: 7682472. Policy #0 lag: (min: 2.0, avg: 11.5, max: 24.0)
[2026-01-17 17:22:58,702][38814] Avg episode reward: [(0, '28.692')]
[2026-01-17 17:22:58,874][38991] Updated weights for policy 0, policy_version 8490 (0.0016)
[2026-01-17 17:22:58,932][38936] Signal inference workers to stop experience collection... (4950 times)
[2026-01-17 17:22:58,948][38936] Signal inference workers to resume experience collection... (4950 times)
[2026-01-17 17:22:58,951][38991] InferenceWorker_p0-w0: stopping experience collection (4950 times)
[2026-01-17 17:22:58,967][38991] InferenceWorker_p0-w0: resuming experience collection (4950 times)
[2026-01-17 17:22:59,459][38991] Updated weights for policy 0, policy_version 8500 (0.0011)
[2026-01-17 17:22:59,915][38991] Updated weights for policy 0, policy_version 8510 (0.0014)
[2026-01-17 17:23:00,502][38991] Updated weights for policy 0, policy_version 8520 (0.0012)
[2026-01-17 17:23:00,999][38991] Updated weights for policy 0, policy_version 8530 (0.0013)
[2026-01-17 17:23:01,493][38991] Updated weights for policy 0, policy_version 8540 (0.0015)
[2026-01-17 17:23:02,091][38991] Updated weights for policy 0, policy_version 8550 (0.0014)
[2026-01-17 17:23:02,687][38991] Updated weights for policy 0, policy_version 8560 (0.0013)
[2026-01-17 17:23:03,146][38991] Updated weights for policy 0, policy_version 8570 (0.0014)
[2026-01-17 17:23:03,430][38936] Signal inference workers to stop experience collection... (5000 times)
[2026-01-17 17:23:03,438][38936] Signal inference workers to resume experience collection... (5000 times)
[2026-01-17 17:23:03,462][38991] InferenceWorker_p0-w0: stopping experience collection (5000 times)
[2026-01-17 17:23:03,463][38991] InferenceWorker_p0-w0: resuming experience collection (5000 times)
[2026-01-17 17:23:03,702][38814] Fps is (10 sec: 74955.9, 60 sec: 75161.6, 300 sec: 75324.7). Total num frames: 35135488. Throughput: 0: 18994.6. Samples: 7795464. Policy #0 lag: (min: 1.0, avg: 13.1, max: 25.0)
[2026-01-17 17:23:03,703][38814] Avg episode reward: [(0, '28.086')]
[2026-01-17 17:23:03,763][38991] Updated weights for policy 0, policy_version 8580 (0.0015)
[2026-01-17 17:23:04,278][38991] Updated weights for policy 0, policy_version 8590 (0.0011)
[2026-01-17 17:23:04,734][38991] Updated weights for policy 0, policy_version 8600 (0.0015)
[2026-01-17 17:23:05,311][38991] Updated weights for policy 0, policy_version 8610 (0.0012)
[2026-01-17 17:23:05,903][38991] Updated weights for policy 0, policy_version 8620 (0.0014)
[2026-01-17 17:23:06,477][38991] Updated weights for policy 0, policy_version 8630 (0.0018)
[2026-01-17 17:23:06,911][38991] Updated weights for policy 0, policy_version 8640 (0.0014)
[2026-01-17 17:23:07,553][38991] Updated weights for policy 0, policy_version 8650 (0.0013)
[2026-01-17 17:23:07,696][38936] Signal inference workers to stop experience collection... (5050 times)
[2026-01-17 17:23:07,702][38936] Signal inference workers to resume experience collection... (5050 times)
[2026-01-17 17:23:07,710][38991] InferenceWorker_p0-w0: stopping experience collection (5050 times)
[2026-01-17 17:23:07,728][38991] InferenceWorker_p0-w0: resuming experience collection (5050 times)
[2026-01-17 17:23:08,085][38991] Updated weights for policy 0, policy_version 8660 (0.0013)
[2026-01-17 17:23:08,506][38991] Updated weights for policy 0, policy_version 8670 (0.0012)
[2026-01-17 17:23:08,557][38814] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 38814], exiting...
[2026-01-17 17:23:08,565][38814] Runner profile tree view:
main_loop: 455.5076
[2026-01-17 17:23:08,565][38936] Stopping Batcher_0...
[2026-01-17 17:23:08,567][38814] Collected {0: 35512320}, FPS: 69149.8
[2026-01-17 17:23:08,567][38936] Loop batcher_evt_loop terminating...
[2026-01-17 17:23:08,591][38936] Saving train_dir/default_experiment/checkpoint_p0/checkpoint_000008671_35516416.pth...
[2026-01-17 17:23:08,597][39629] EvtLoop [rollout_proc21_evt_loop, process=rollout_proc21] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance21'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:08,601][39629] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc21_evt_loop
[2026-01-17 17:23:08,606][39974] EvtLoop [rollout_proc28_evt_loop, process=rollout_proc28] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance28'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:08,608][39974] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc28_evt_loop
[2026-01-17 17:23:08,651][38936] Removing train_dir/default_experiment/checkpoint_p0/checkpoint_000004646_19030016.pth
[2026-01-17 17:23:08,653][38936] Stopping LearnerWorker_p0...
[2026-01-17 17:23:08,653][38936] Loop learner_proc0_evt_loop terminating...
[2026-01-17 17:23:08,666][38991] Weights refcount: 2 0
[2026-01-17 17:23:08,667][38991] Stopping InferenceWorker_p0-w0...
[2026-01-17 17:23:08,668][38991] Loop inference_proc0-0_evt_loop terminating...
[2026-01-17 17:23:08,686][39999] EvtLoop [rollout_proc29_evt_loop, process=rollout_proc29] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance29'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:08,689][39999] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc29_evt_loop
[2026-01-17 17:23:08,873][39028] EvtLoop [rollout_proc13_evt_loop, process=rollout_proc13] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance13'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:08,880][39028] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc13_evt_loop
[2026-01-17 17:23:08,988][38995] EvtLoop [rollout_proc3_evt_loop, process=rollout_proc3] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance3'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:08,990][38995] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc3_evt_loop
[2026-01-17 17:23:09,014][39510] EvtLoop [rollout_proc18_evt_loop, process=rollout_proc18] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance18'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,017][39510] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc18_evt_loop
[2026-01-17 17:23:09,059][39582] EvtLoop [rollout_proc20_evt_loop, process=rollout_proc20] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance20'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,061][39582] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc20_evt_loop
[2026-01-17 17:23:09,081][38990] EvtLoop [rollout_proc0_evt_loop, process=rollout_proc0] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance0'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,083][38990] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc0_evt_loop
[2026-01-17 17:23:09,124][38992] EvtLoop [rollout_proc1_evt_loop, process=rollout_proc1] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance1'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,127][38992] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc1_evt_loop
[2026-01-17 17:23:09,146][39795] EvtLoop [rollout_proc24_evt_loop, process=rollout_proc24] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance24'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,151][39795] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc24_evt_loop
[2026-01-17 17:23:09,181][40052] EvtLoop [rollout_proc30_evt_loop, process=rollout_proc30] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance30'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,185][40052] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc30_evt_loop
[2026-01-17 17:23:09,186][39826] EvtLoop [rollout_proc26_evt_loop, process=rollout_proc26] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance26'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,188][39826] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc26_evt_loop
[2026-01-17 17:23:09,186][38997] EvtLoop [rollout_proc5_evt_loop, process=rollout_proc5] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance5'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,188][38997] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc5_evt_loop
[2026-01-17 17:23:09,187][39768] EvtLoop [rollout_proc23_evt_loop, process=rollout_proc23] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance23'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,190][39768] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc23_evt_loop
[2026-01-17 17:23:09,190][40170] EvtLoop [rollout_proc31_evt_loop, process=rollout_proc31] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance31'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,193][40170] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc31_evt_loop
[2026-01-17 17:23:09,189][39030] EvtLoop [rollout_proc11_evt_loop, process=rollout_proc11] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance11'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,193][39030] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc11_evt_loop
[2026-01-17 17:23:09,199][40311] EvtLoop [rollout_proc34_evt_loop, process=rollout_proc34] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance34'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,200][40311] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc34_evt_loop
[2026-01-17 17:23:09,201][39023] EvtLoop [rollout_proc10_evt_loop, process=rollout_proc10] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance10'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,207][39023] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc10_evt_loop
[2026-01-17 17:23:09,203][39796] EvtLoop [rollout_proc25_evt_loop, process=rollout_proc25] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance25'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,208][39796] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc25_evt_loop
[2026-01-17 17:23:09,207][39024] EvtLoop [rollout_proc9_evt_loop, process=rollout_proc9] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance9'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,210][39024] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc9_evt_loop
[2026-01-17 17:23:09,219][38996] EvtLoop [rollout_proc4_evt_loop, process=rollout_proc4] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance4'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,227][38996] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc4_evt_loop
[2026-01-17 17:23:09,227][39416] EvtLoop [rollout_proc16_evt_loop, process=rollout_proc16] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance16'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,235][39416] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc16_evt_loop
[2026-01-17 17:23:09,236][39029] EvtLoop [rollout_proc14_evt_loop, process=rollout_proc14] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance14'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,242][39029] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc14_evt_loop
[2026-01-17 17:23:09,240][40217] EvtLoop [rollout_proc32_evt_loop, process=rollout_proc32] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance32'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,248][40217] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc32_evt_loop
[2026-01-17 17:23:09,392][39509] EvtLoop [rollout_proc17_evt_loop, process=rollout_proc17] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance17'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,394][39509] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc17_evt_loop
[2026-01-17 17:23:09,426][38994] EvtLoop [rollout_proc2_evt_loop, process=rollout_proc2] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance2'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,432][38994] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc2_evt_loop
[2026-01-17 17:23:09,451][38998] EvtLoop [rollout_proc6_evt_loop, process=rollout_proc6] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance6'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,454][38998] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc6_evt_loop
[2026-01-17 17:23:09,640][39027] EvtLoop [rollout_proc12_evt_loop, process=rollout_proc12] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance12'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,653][39027] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc12_evt_loop
[2026-01-17 17:23:09,895][39415] EvtLoop [rollout_proc15_evt_loop, process=rollout_proc15] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance15'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,906][39415] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc15_evt_loop
[2026-01-17 17:23:09,910][39653] EvtLoop [rollout_proc22_evt_loop, process=rollout_proc22] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance22'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,913][39653] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc22_evt_loop
[2026-01-17 17:23:09,941][38999] EvtLoop [rollout_proc7_evt_loop, process=rollout_proc7] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance7'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:09,944][38999] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc7_evt_loop
[2026-01-17 17:23:10,004][39936] EvtLoop [rollout_proc27_evt_loop, process=rollout_proc27] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance27'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:10,010][39936] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc27_evt_loop
[2026-01-17 17:23:10,008][40228] EvtLoop [rollout_proc33_evt_loop, process=rollout_proc33] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance33'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:10,012][40228] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc33_evt_loop
[2026-01-17 17:23:10,009][39026] EvtLoop [rollout_proc8_evt_loop, process=rollout_proc8] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance8'), args=(1, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:10,018][39026] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc8_evt_loop
[2026-01-17 17:23:10,023][39544] EvtLoop [rollout_proc19_evt_loop, process=rollout_proc19] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance19'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:10,029][39544] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc19_evt_loop
[2026-01-17 17:23:10,092][40335] EvtLoop [rollout_proc35_evt_loop, process=rollout_proc35] unhandled exception in slot='advance_rollouts' connected to emitter=Emitter(object_id='InferenceWorker_p0-w0', signal_name='advance35'), args=(0, 0)
Traceback (most recent call last):
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/signal_slot/signal_slot.py", line 355, in _process_signal
    slot_callable(*args)
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/rollout_worker.py", line 241, in advance_rollouts
    complete_rollouts, episodic_stats = runner.advance_rollouts(policy_id, self.timing)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/sampling/non_batched_sampling.py", line 634, in advance_rollouts
    new_obs, rewards, terminated, truncated, infos = e.step(actions)
                                                     ^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 129, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/algo/utils/make_env.py", line 115, in step
    obs, rew, terminated, truncated, info = self.env.step(action)
                                            ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/scenario_wrappers/gathering_reward_shaping.py", line 33, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 522, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sample_factory/envs/env_wrappers.py", line 86, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/gymnasium/core.py", line 461, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/wrappers/multiplayer_stats.py", line 54, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
                                               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/brennan/Desktop/HuggingFace_DRL_Course/HFDRL_UNIT8_p2/sf_torch24_py311/lib/python3.11/site-packages/sf_examples/vizdoom/doom/doom_gym.py", line 452, in step
    reward = self.game.make_action(actions_flattened, self.skip_frames)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
vizdoom.vizdoom.SignalException: Signal SIGINT received. ViZDoom instance has been closed.
[2026-01-17 17:23:10,103][40335] Unhandled exception Signal SIGINT received. ViZDoom instance has been closed. in evt loop rollout_proc35_evt_loop
